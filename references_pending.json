[
  {
    "id": "arxiv:2512.03501",
    "type": "report",
    "title": "SocraticAI: Transforming LLMs into Guided CS Tutors Through Scaffolded Interaction",
    "author": [
      {
        "family": "Sunil",
        "given": "Karthik"
      },
      {
        "family": "Thakkar",
        "given": "Aalok"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          3
        ]
      ]
    },
    "abstract": "We present SocraticAI, a scaffolded AI tutoring system that integrates large language models (LLMs) into undergraduate Computer Science education through structured constraints rather than prohibition. The system enforces well-formulated questions, reflective engagement, and daily usage limits while providing Socratic dialogue scaffolds. Unlike traditional AI bans, our approach cultivates responsible and strategic AI interaction skills through technical guardrails, including authentication, query validation, structured feedback, and RAG-based course grounding. Initial deployment demonstrates that students progress from vague help-seeking to sophisticated problem decomposition within 2-3 weeks, with over 75% producing substantive reflections and displaying emergent patterns of deliberate, strategic AI use.",
    "URL": "http://arxiv.org/abs/2512.03501v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.03671",
    "type": "report",
    "title": "Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context",
    "author": [
      {
        "family": "Savoldi",
        "given": "Beatrice"
      },
      {
        "family": "Attanasio",
        "given": "Giuseppe"
      },
      {
        "family": "Gorodetskaya",
        "given": "Olga"
      },
      {
        "family": "Manerba",
        "given": "Marta Marchiori"
      },
      {
        "family": "Bassignana",
        "given": "Elisa"
      },
      {
        "family": "Casola",
        "given": "Silvia"
      },
      {
        "family": "Negri",
        "given": "Matteo"
      },
      {
        "family": "Caselli",
        "given": "Tommaso"
      },
      {
        "family": "Bentivogli",
        "given": "Luisa"
      },
      {
        "family": "Ramponi",
        "given": "Alan"
      },
      {
        "family": "Muti",
        "given": "Arianna"
      },
      {
        "family": "Balbo",
        "given": "Nicoletta"
      },
      {
        "family": "Nozza",
        "given": "Debora"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          3
        ]
      ]
    },
    "abstract": "The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.",
    "URL": "http://arxiv.org/abs/2512.03671v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.03694",
    "type": "report",
    "title": "SRPG: Semantically Reconstructed Privacy Guard for Zero-Trust Privacy in Educational Multi-Agent Systems",
    "author": [
      {
        "family": "Guo",
        "given": "Shuang"
      },
      {
        "family": "Li",
        "given": "Zihui"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          3
        ]
      ]
    },
    "abstract": "Multi-Agent Systems (MAS) with large language models (LLMs) enable personalized education but risk leaking minors personally identifiable information (PII) via unstructured dialogue. Existing privacy methods struggle to balance security and utility: role-based access control fails on unstructured text, while naive masking destroys pedagogical context. We propose SRPG, a privacy guard for educational MAS, using a Dual-Stream Reconstruction Mechanism: a strict sanitization stream ensures zero PII leakage, and a context reconstruction stream (LLM driven) recovers mathematical logic. This decouples instructional content from private data, preserving teaching efficacy. Tests on MathDial show SRPG works across models; with GPT-4o, it achieves 0.0000 Attack Success Rate (ASR) (zero leakage) and 0.8267 Exact Match, far outperforming the zero trust Pure LLM baseline (0.2138). SRPG effectively protects minors privacy without sacrificing mathematical instructional quality.",
    "URL": "http://arxiv.org/abs/2512.03694v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.04256",
    "type": "report",
    "title": "On the Role and Impact of GenAI Tools in Software Engineering Education",
    "author": [
      {
        "family": "Qin",
        "given": "Qiaolin"
      },
      {
        "family": "Santos",
        "given": "Ronnie de Souza"
      },
      {
        "family": "Spinola",
        "given": "Rodrigo"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          3
        ]
      ]
    },
    "abstract": "Context. The rise of generative AI (GenAI) tools like ChatGPT and GitHub Copilot has transformed how software is learned and written. In software engineering (SE) education, these tools offer new opportunities for support, but also raise concerns about over-reliance, ethical use, and impacts on learning. Objective. This study investigates how undergraduate SE students use GenAI tools, focusing on the benefits, challenges, ethical concerns, and instructional expectations that shape their experiences. Method. We conducted a survey with 130 undergraduate students from two universities. The survey combined structured Likert-scale items and open-ended questions to investigate five dimensions: usage context, perceived benefits, challenges, ethical and instructional perceptions. Results. Students most often use GenAI for incremental learning and advanced implementation, reporting benefits such as brainstorming support and confidence-building. At the same time, they face challenges including unclear rationales and difficulty adapting outputs. Students highlight ethical concerns around fairness and misconduct, and call for clearer instructional guidance. Conclusion. GenAI is reshaping SE education in nuanced ways. Our findings underscore the need for scaffolding, ethical policies, and adaptive instructional strategies to ensure that GenAI supports equitable and effective learning.",
    "URL": "http://arxiv.org/abs/2512.04256v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.04359",
    "type": "report",
    "title": "Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning",
    "author": [
      {
        "family": "Cao",
        "given": "Hongye"
      },
      {
        "family": "Bai",
        "given": "Zhixin"
      },
      {
        "family": "Peng",
        "given": "Ziyue"
      },
      {
        "family": "Wang",
        "given": "Boyan"
      },
      {
        "family": "Yang",
        "given": "Tianpei"
      },
      {
        "family": "Huo",
        "given": "Jing"
      },
      {
        "family": "Zhang",
        "given": "Yuyao"
      },
      {
        "family": "Gao",
        "given": "Yang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.",
    "URL": "http://arxiv.org/abs/2512.04359v2",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.04555",
    "type": "report",
    "title": "ADAPT: Learning Task Mixtures for Budget-Constrained Instruction Tuning",
    "author": [
      {
        "family": "Kadasi",
        "given": "Pritam"
      },
      {
        "family": "Upperwal",
        "given": "Abhishek"
      },
      {
        "family": "SIngh",
        "given": "Mayank"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "We propose ADAPT, a meta-learning algorithm that \\emph{learns} task sampling proportions under an explicit token budget for multi-task instruction tuning. Instead of fixing task weights by hand, \\adapt{} maintains a continuous distribution over tasks and updates it via meta-gradients of a smooth worst-case validation objective, inducing an adaptive curriculum that allocates more tokens to useful tasks while avoiding collapse. We instantiate ADAPT on three $\\sim$1B-parameter open-weight LLMs (Gemma-3-1B, LLaMA-3.2-1B, Qwen-0.6B), training on 20 Natural Instructions task types under budgets of $1\\%$, $5\\%$, and $10\\%$ of the available supervised tokens, and compare against strong supervised fine-tuning baselines with uniform and size-proportional mixing. We conduct evaluations on 11 out-of-domain benchmarks spanning reasoning, reading comprehension, code generation, and instruction following, we find that ADAPT matches or slightly improves average downstream performance relative to the best static mixture, while using fewer effective training tokens and reallocating budget toward harder, benchmark-aligned tasks.",
    "URL": "http://arxiv.org/abs/2512.04555v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.04629",
    "type": "report",
    "title": "BioMedGPT-Mol: Multi-task Learning for Molecular Understanding and Generation",
    "author": [
      {
        "family": "Zuo",
        "given": "Chenyang"
      },
      {
        "family": "Fan",
        "given": "Siqi"
      },
      {
        "family": "Nie",
        "given": "Zaiqing"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "Molecules play a crucial role in biomedical research and discovery, particularly in the field of small molecule drug development. Given the rapid advancements in large language models, especially the recent emergence of reasoning models, it is natural to explore how a general-purpose language model can be efficiently adapted for molecular science applications. In this work, we introduce BioMedGPT-Mol, a molecular language model designed to support molecular understanding and generation tasks. By curating and unifying existing public instruction datasets, we have assembled a large-scale, comprehensive, and high-quality training dataset. The model is then fine-tuned through a meticulously designed multi-task learning framework. On a consolidated benchmark derived from LlaSMol, TOMG-Bench, and MuMOInstruct, BioMedGPT-Mol achieves remarkable performance. Our experimental results demonstrate that a general-purpose reasoning model can be effectively and efficiently post-trained into a professional molecular language model through a well-structured multi-task curriculum. Leveraging these capabilities, we further apply the model to multi-step retrosynthetic planning, achieving state-of-the-art performance on RetroBench and demonstrating its superior efficacy as an end-to-end retrosynthetic planner. We anticipate that our approach can be extended to other biomedical scientific domains.",
    "URL": "http://arxiv.org/abs/2512.04629v2",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.04630",
    "type": "report",
    "title": "Reflection-Satisfaction Tradeoff: Investigating Impact of Reflection on Student Engagement with AI-Generated Programming Hints",
    "author": [
      {
        "family": "Choi",
        "given": "Heeryung"
      },
      {
        "family": "Phung",
        "given": "Tung"
      },
      {
        "family": "Wu",
        "given": "Mengyan"
      },
      {
        "family": "Singla",
        "given": "Adish"
      },
      {
        "family": "Brooks",
        "given": "Christopher"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "Generative AI tools, such as AI-generated hints, are increasingly integrated into programming education to offer timely, personalized support. However, little is known about how to effectively leverage these hints while ensuring autonomous and meaningful learning. One promising approach involves pairing AI-generated hints with reflection prompts, asking students to review and analyze their learning, when they request hints. This study investigates the interplay between AI-generated hints and different designs of reflection prompts in an online introductory programming course. We conducted a two-trial field experiment. In Trial 1, students were randomly assigned to receive prompts either before or after receiving hints, or no prompt at all. Each prompt also targeted one of three SRL phases: planning, monitoring, and evaluation. In Trial 2, we examined two types of prompt guidance: directed (offering more explicit and structured guidance) and open (offering more general and less constrained guidance). Findings show that students in the before-hint (RQ1), planning (RQ2), and directed (RQ3) prompt groups produced higher-quality reflections but reported lower satisfaction with AI-generated hints than those in other conditions. Immediate performance did not differ across conditions. This negative relationship between reflection quality and hint satisfaction aligns with previous work on student mental effort and satisfaction. Our results highlight the need to reconsider how AI models are trained and evaluated for education, as prioritizing user satisfaction can undermine deeper learning.",
    "URL": "http://arxiv.org/abs/2512.04630v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.04834",
    "type": "report",
    "title": "Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case",
    "author": [
      {
        "family": "Kembu",
        "given": "Vignesh Kumar"
      },
      {
        "family": "Morandini",
        "given": "Pierandrea"
      },
      {
        "family": "Ranzini",
        "given": "Marta Bianca Maria"
      },
      {
        "family": "Nocera",
        "given": "Antonino"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.",
    "URL": "http://arxiv.org/abs/2512.04834v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.04869",
    "type": "report",
    "title": "Developing a General Personal Tutor for Education",
    "author": [
      {
        "family": "Aru",
        "given": "Jaan"
      },
      {
        "family": "Laak",
        "given": "Kristjan-Julius"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "The vision of a universal AI tutor has remained elusive, despite decades of effort. Could LLMs be the game-changer? We overview novel issues arising from developing a nationwide AI tutor. We highlight the practical questions that point to specific gaps in our scientific understanding of the learning process.",
    "DOI": "10.1016/j.tics.2025.09.010",
    "URL": "https://doi.org/10.1016/j.tics.2025.09.010",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.05024",
    "type": "report",
    "title": "Model-Free Assessment of Simulator Fidelity via Quantile Curves",
    "author": [
      {
        "family": "Iyengar",
        "given": "Garud"
      },
      {
        "family": "Lin",
        "given": "Yu-Shiou Willy"
      },
      {
        "family": "Wang",
        "given": "Kaizheng"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "Simulation of complex systems originated in manufacturing and queuing applications. It is now widely used for large-scale, ML-based systems in research, education, and consumer surveys. However, characterizing the discrepancy between simulators and ground truth remains challenging for increasingly complex, machine-learning-based systems. We propose a computationally tractable method to estimate the quantile function of the discrepancy between the simulated and ground-truth outcome distributions. Our approach focuses on output uncertainty and treats the simulator as a black box, imposing no modeling assumptions on its internals, and hence applies broadly across many parameter families, from Bernoulli and multinomial models to continuous, vector-valued settings. The resulting quantile curve supports confidence interval construction for unseen scenarios, risk-aware summaries of sim-to-real discrepancy (e.g., VaR/CVaR), and comparison of simulators' performance. We demonstrate our methodology in an application assessing LLM simulation fidelity on the WorldValueBench dataset spanning four LLMs.",
    "URL": "http://arxiv.org/abs/2512.05024v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.05167",
    "type": "report",
    "title": "Bridging Traditional Machine Learning and Large Language Models: A Two-Part Course Design for Modern AI Education",
    "author": [
      {
        "family": "Li",
        "given": "Fang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "This paper presents an innovative pedagogical approach for teaching artificial intelligence and data science that systematically bridges traditional machine learning techniques with modern Large Language Models (LLMs). We describe a course structured in two sequential and complementary parts: foundational machine learning concepts and contemporary LLM applications. This design enables students to develop a comprehensive understanding of AI evolution while building practical skills with both established and cutting-edge technologies. We detail the course architecture, implementation strategies, assessment methods, and learning outcomes from our summer course delivery spanning two seven-week terms. Our findings demonstrate that this integrated approach enhances student comprehension of the AI landscape and better prepares them for industry demands in the rapidly evolving field of artificial intelligence.",
    "URL": "http://arxiv.org/abs/2512.05167v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.05506",
    "type": "report",
    "title": "When Scaffolding Breaks: Investigating Student Interaction with LLM-Based Writing Support in Real-Time K-12 EFL Classrooms",
    "author": [
      {
        "family": "Myung",
        "given": "Junho"
      },
      {
        "family": "Lim",
        "given": "Hyunseung"
      },
      {
        "family": "Oh",
        "given": "Hana"
      },
      {
        "family": "Jin",
        "given": "Hyoungwook"
      },
      {
        "family": "Kang",
        "given": "Nayeon"
      },
      {
        "family": "Ahn",
        "given": "So-Yeon"
      },
      {
        "family": "Hong",
        "given": "Hwajung"
      },
      {
        "family": "Oh",
        "given": "Alice"
      },
      {
        "family": "Kim",
        "given": "Juho"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          5
        ]
      ]
    },
    "abstract": "Large language models (LLMs) are promising tools for scaffolding students' English writing skills, but their effectiveness in real-time K-12 classrooms remains underexplored. Addressing this gap, our study examines the benefits and limitations of using LLMs as real-time learning support, considering how classroom constraints, such as diverse proficiency levels and limited time, affect their effectiveness. We conducted a deployment study with 157 eighth-grade students in a South Korean middle school English class over six weeks. Our findings reveal that while scaffolding improved students' ability to compose grammatically correct sentences, this step-by-step approach demotivated lower-proficiency students and increased their system reliance. We also observed challenges to classroom dynamics, where extroverted students often dominated the teacher's attention, and the system's assistance made it difficult for teachers to identify struggling students. Based on these findings, we discuss design guidelines for integrating LLMs into real-time writing classes as inclusive educational tools.",
    "URL": "http://arxiv.org/abs/2512.05506v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.05671",
    "type": "report",
    "title": "MedTutor-R1: Socratic Personalized Medical Teaching with Multi-Agent Simulation",
    "author": [
      {
        "family": "He",
        "given": "Zhitao"
      },
      {
        "family": "Yang",
        "given": "Haolin"
      },
      {
        "family": "Qin",
        "given": "Zeyu"
      },
      {
        "family": "Fung",
        "given": "Yi R"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          5
        ]
      ]
    },
    "abstract": "The significant gap between rising demands for clinical training and the scarcity of expert instruction poses a major challenge to medical education. With powerful capabilities in personalized guidance, Large Language Models (LLMs) offer a promising solution to bridge this gap. However, current research focuses mainly on one-on-one knowledge instruction, overlooking collaborative reasoning, a key skill for students developed in teamwork like ward rounds. To this end, we develop ClinEdu, a multi-agent pedagogical simulator with personality-driven patients and diverse student cohorts, enabling controlled testing of complex pedagogical processes and scalable generation of teaching data. Based on ClinEdu, we construct ClinTeach, a large Socratic teaching dialogue dataset that captures the complexities of group instruction. We then train MedTutor-R1, the first multimodal Socratic tutor designed for one-to-many instruction in clinical medical education. MedTutor-R1 is first instruction-tuned on our ClinTeach dataset and then optimized with reinforcement learning, using rewards derived from a three-axis rubric, covering structural fidelity, analytical quality, and clinical safety, to refine its adaptive Socratic strategies. For authentic in-situ assessment, we use simulation-based interactive evaluation that redeploys the tutor back into ClinEdu. Experimental results demonstrate that our MedTutor-R1 outperforms the base model by over 20% in average pedagogical score and is comparable to o3, while also exhibiting high adaptability in handling a varying number of students. This promising performance underscores the effectiveness of our pedagogical simulator, ClinEdu.",
    "URL": "http://arxiv.org/abs/2512.05671v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.05967",
    "type": "report",
    "title": "Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms",
    "author": [
      {
        "family": "Granata",
        "given": "Francesco"
      },
      {
        "family": "Poggi",
        "given": "Francesco"
      },
      {
        "family": "Mongiov√¨",
        "given": "Misael"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          5
        ]
      ]
    },
    "abstract": "In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.",
    "URL": "http://arxiv.org/abs/2512.05967v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.06018",
    "type": "report",
    "title": "Uncovering Students' Inquiry Patterns in GenAI-Supported Clinical Practice: An Integration of Epistemic Network Analysis and Sequential Pattern Mining",
    "author": [
      {
        "family": "Wei",
        "given": "Jiameng"
      },
      {
        "family": "Dang",
        "given": "Dinh"
      },
      {
        "family": "Yang",
        "given": "Kaixun"
      },
      {
        "family": "Stokes",
        "given": "Emily"
      },
      {
        "family": "Mazeh",
        "given": "Amna"
      },
      {
        "family": "Lim",
        "given": "Angelina"
      },
      {
        "family": "Dai",
        "given": "David Wei"
      },
      {
        "family": "Moore",
        "given": "Joel"
      },
      {
        "family": "Fan",
        "given": "Yizhou"
      },
      {
        "family": "Gasevic",
        "given": "Danijela"
      },
      {
        "family": "Gasevic",
        "given": "Dragan"
      },
      {
        "family": "Chen",
        "given": "Guanliang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "Assessment of medication history-taking has traditionally relied on human observation, limiting scalability and detailed performance data. While Generative AI (GenAI) platforms enable extensive data collection and learning analytics provide powerful methods for analyzing educational traces, these approaches remain largely underexplored in pharmacy clinical training. This study addresses this gap by applying learning analytics to understand how students develop clinical communication competencies with GenAI-powered virtual patients -- a crucial endeavor given the diversity of student cohorts, varying language backgrounds, and the limited opportunities for individualized feedback in traditional training settings. We analyzed 323 students' interaction logs across Australian and Malaysian institutions, comprising 50,871 coded utterances from 1,487 student-GenAI dialogues. Combining Epistemic Network Analysis to model inquiry co-occurrences with Sequential Pattern Mining to capture temporal sequences, we found that high performers demonstrated strategic deployment of information recognition behaviors. Specifically, high performers centered inquiry on recognizing clinically relevant information, integrating rapport-building and structural organization, while low performers remained in routine question-verification loops. Demographic factors including first-language background, prior pharmacy work experience, and institutional context, also shaped distinct inquiry patterns. These findings reveal inquiry patterns that may indicate clinical reasoning development in GenAI-assisted contexts, providing methodological insights for health professions education assessment and informing adaptive GenAI system design that supports diverse learning pathways.",
    "URL": "http://arxiv.org/abs/2512.06018v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.06483",
    "type": "report",
    "title": "Classifying German Language Proficiency Levels Using Large Language Models",
    "author": [
      {
        "family": "Ahlers",
        "given": "Elias-Leander"
      },
      {
        "family": "Brunsmann",
        "given": "Witold"
      },
      {
        "family": "Schilling",
        "given": "Malte"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          6
        ]
      ]
    },
    "abstract": "Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.",
    "URL": "http://arxiv.org/abs/2512.06483v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.06617",
    "type": "report",
    "title": "Teaching large language models to see in radar: aspect-distributed prototypes for few-shot HRRP ATR",
    "author": [
      {
        "family": "Bi",
        "given": "De"
      },
      {
        "family": "Xu",
        "given": "Chengbai"
      },
      {
        "family": "Chen",
        "given": "Lingfeng"
      },
      {
        "family": "Hu",
        "given": "Panhe"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          7
        ]
      ]
    },
    "abstract": "High-resolution range profiles (HRRPs) play a critical role in automatic target recognition (ATR) due to their richinformationregarding target scattering centers (SCs), which encapsulate the geometric and electromagnetic characteristics of thetarget.Under few-shot circumstances, traditional learning-based methods often suffer from overfitting and struggle togeneralizeeffectively. The recently proposed HRRPLLM, which leverages the in-context learning (ICL) capabilities of largelanguagemodels (LLMs) for one-shot HRRP ATR, is limited in few-shot scenarios. This limitation arises because it primarilyutilizesthe distribution of SCs for recognition while neglecting the variance of the samples caused by aspect sensitivity. Thispaperproposes a straightforward yet effective Aspect-Distributed Prototype (ADP) strategy for LLM-based ATRunder few-shotconditions to enhance aspect robustness. Experiments conducted on both simulated and measured aircraft electromagneticdatasets demonstrate that the proposed method significantly outperforms current benchmarks.",
    "URL": "http://arxiv.org/abs/2512.06617v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.06999",
    "type": "report",
    "title": "Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model",
    "author": [
      {
        "family": "Wang",
        "given": "Zihao"
      },
      {
        "family": "Yuan",
        "given": "Ruibin"
      },
      {
        "family": "Geng",
        "given": "Ziqi"
      },
      {
        "family": "Li",
        "given": "Hengjia"
      },
      {
        "family": "Qu",
        "given": "Xingwei"
      },
      {
        "family": "Li",
        "given": "Xinyi"
      },
      {
        "family": "Chen",
        "given": "Songye"
      },
      {
        "family": "Fu",
        "given": "Haoying"
      },
      {
        "family": "Dannenberg",
        "given": "Roger B."
      },
      {
        "family": "Zhang",
        "given": "Kejun"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          7
        ]
      ]
    },
    "abstract": "Automated singing assessment is crucial for education and entertainment. However, existing systems face two fundamental limitations: reliance on reference tracks, which stifles creative expression, and the simplification of complex performances into non-diagnostic scores based solely on pitch and rhythm. We advocate for a shift from discriminative to descriptive evaluation, creating a complete ecosystem for reference-free, multi-dimensional assessment. First, we introduce Sing-MD, a large-scale dataset annotated by experts across four dimensions: breath control, timbre quality, emotional expression, and vocal technique. Our analysis reveals significant annotation inconsistencies among experts, challenging the validity of traditional accuracy-based metrics. Second, addressing the memory limitations of Multimodal Large Language Models (MLLMs) in analyzing full-length songs, we propose VocalVerse. This efficient hybrid architecture leverages a lightweight acoustic encoder to model global performance features and long-term dependencies. Third, to address automated metric shortcomings, we establish the H-TPR (Human-in-the-loop Tiered Perceptual Ranking) benchmark, which evaluates a model's ability to generate perceptually valid rankings rather than predicting noisy ground-truth scores.",
    "DOI": "10.1145/3746027.3758148",
    "URL": "https://doi.org/10.1145/3746027.3758148",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.07143",
    "type": "report",
    "title": "A Theoretical Framework of Student Agency in AI- Assisted Learning: A Grounded Theory Approach",
    "author": [
      {
        "family": "Dai",
        "given": "Yun"
      },
      {
        "family": "Lai",
        "given": "Sichen"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          8
        ]
      ]
    },
    "abstract": "Generative AI(GenAI) is a kind of AI model capable of producing human-like content in various modalities, including text, image, audio, video, and computer programming. Although GenAI offers great potential for education, its value often depends on students' ability to engage with it actively, responsibly, and critically - qualities central to student agency. Nevertheless, student agency has long been a complex and ambiguous concept in educational discourses, with few empirical studies clarifying its distinct nature and process in AI-assisted learning environments. To address this gap, the qualitative study presented in this article examines how higher education students exercise agency in AI-assisted learning and proposes a theoretical framework using a grounded theory approach. Guided by agentic engagement theory, this article analyzes the authentic experiences of 26 students using data from their GenAI conversation records and cognitive interviews that capture their thought processes and decision-making. The findings identify four key aspects of student agency: initiating and (re)directing, mindful adoption, external help-seeking, and reflective learning. Together, these aspects form an empirically developed framework that characterizes student agency in AI-assisted learning as a proactive, intentional, adaptive, reflective, and iterative process. Based on the empirical findings, theoretical and practical implications are discussed for researchers, educators, and policymakers.",
    "DOI": "10.1093/9780198945253.003.0009",
    "URL": "https://doi.org/10.1093/9780198945253.003.0009",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.07454",
    "type": "report",
    "title": "Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning",
    "author": [
      {
        "family": "Akhlaghi",
        "given": "Amir Mohammad"
      },
      {
        "family": "Shabani",
        "given": "Amirhossein"
      },
      {
        "family": "Abdolmaleki",
        "given": "Mostafa"
      },
      {
        "family": "Kheradpisheh",
        "given": "Saeed Reza"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          8
        ]
      ]
    },
    "abstract": "The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique \"warm-up\" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.",
    "URL": "http://arxiv.org/abs/2512.07454v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.08057",
    "type": "report",
    "title": "Large Language Models for Education and Research: An Empirical and User Survey-based Analysis",
    "author": [
      {
        "family": "Rahman",
        "given": "Md Mostafizer"
      },
      {
        "family": "Shiplu",
        "given": "Ariful Islam"
      },
      {
        "family": "Amin",
        "given": "Md Faizul Ibne"
      },
      {
        "family": "Watanobe",
        "given": "Yutaka"
      },
      {
        "family": "Peng",
        "given": "Lu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          8
        ]
      ]
    },
    "abstract": "Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency-focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.",
    "URL": "http://arxiv.org/abs/2512.08057v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.08545",
    "type": "report",
    "title": "Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks",
    "author": [
      {
        "family": "Kar",
        "given": "Indrajit"
      },
      {
        "family": "Kumar",
        "given": "Kalathur Chenchu Kishore"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          9
        ]
      ]
    },
    "abstract": "Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.",
    "URL": "http://arxiv.org/abs/2512.08545v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.08596",
    "type": "report",
    "title": "Examining Student Interactions with a Pedagogical AI-Assistant for Essay Writing and their Impact on Students Writing Quality",
    "author": [
      {
        "family": "Febriantoro",
        "given": "Wicaksono"
      },
      {
        "family": "Zhou",
        "given": "Qi"
      },
      {
        "family": "Suraworachet",
        "given": "Wannapon"
      },
      {
        "family": "Bulathwela",
        "given": "Sahan"
      },
      {
        "family": "Gauthier",
        "given": "Andrea"
      },
      {
        "family": "Millan",
        "given": "Eva"
      },
      {
        "family": "Cukurova",
        "given": "Mutlu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          9
        ]
      ]
    },
    "abstract": "The dynamic nature of interactions between students and GenAI, as well as their relationship to writing quality, remains underexplored. While most research has examined how general-purpose GenAI can support writing, fewer studies have investigated how students interact with pedagogically designed systems across different phases of the writing process. To address this gap, we evaluated a GenAI-driven essay-writing assistant (EWA) designed to support higher education students in argumentative writing. Drawing on 1,282 interaction logs from 32 undergraduates during a two-hour writing session, Sequential Pattern Mining and K-Means clustering were used to identify behavioral patterns. Two clusters emerged: Cluster 1 emphasized outline planning and essay structure, while Cluster 2 focused on content development. A Mann-Whitney U test revealed a moderate effect size (r = 0.36) in the essay Organization dimension, with Cluster 1 showing higher scores. Qualitative analysis indicated that students with better performance actively wrote and shared essay sections with EWA for feedback, rather than interacted passively by asking questions. These findings suggest implications for teaching and system design. Teachers can encourage active engagement, while future EWAs may integrate automatic labeling and monitoring to prompt students to move from questioning to writing, enabling fuller benefits from GenAI-supported learning.",
    "URL": "http://arxiv.org/abs/2512.08596v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.08713",
    "type": "report",
    "title": "Automatic Essay Scoring and Feedback Generation in Basque Language Learning",
    "author": [
      {
        "family": "Azurmendi",
        "given": "Ekhi"
      },
      {
        "family": "Arregi",
        "given": "Xabier"
      },
      {
        "family": "Lacalle",
        "given": "Oier Lopez de"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          9
        ]
      ]
    },
    "abstract": "This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.",
    "URL": "http://arxiv.org/abs/2512.08713v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.08998",
    "type": "report",
    "title": "DermETAS-SNA LLM: A Dermatology Focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM Assistant",
    "author": [
      {
        "family": "Oruganty",
        "given": "Nitya Phani Santosh"
      },
      {
        "family": "Murali",
        "given": "Keerthi Vemula"
      },
      {
        "family": "Ngan",
        "given": "Chun-Kit"
      },
      {
        "family": "Pinho",
        "given": "Paulo Bandeira"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          9
        ]
      ]
    },
    "abstract": "Our work introduces the DermETAS-SNA LLM Assistant that integrates Dermatology-focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM. The assistant dynamically learns skin-disease classifiers and provides medically informed descriptions to facilitate clinician-patient interpretation. Contributions include: (1) Developed an ETAS framework on the SKINCON dataset to optimize a Vision Transformer (ViT) tailored for dermatological feature representation and then fine-tuned binary classifiers for each of the 23 skin disease categories in the DermNet dataset to enhance classification performance; (2) Designed a StackNet architecture that integrates multiple fine-tuned binary ViT classifiers to enhance predictive robustness and mitigate class imbalance issues; (3) Implemented a RAG pipeline, termed Diagnostic Explanation and Retrieval Model for Dermatology, which harnesses the capabilities of the Google Gemini 2.5 Pro LLM architecture to generate personalized, contextually informed diagnostic descriptions and explanations for patients, leveraging a repository of verified dermatological materials; (4) Performed extensive experimental evaluations on 23 skin disease categories to demonstrate performance increase, achieving an overall F1-score of 56.30% that surpasses SkinGPT-4 (48.51%) by a considerable margin, representing a performance increase of 16.06%; (5) Conducted a domain-expert evaluation, with eight licensed medical doctors, of the clinical responses generated by our AI assistant for seven dermatological conditions. Our results show a 92% agreement rate with the assessments provided by our AI assistant (6) Created a proof-of-concept prototype that fully integrates our DermETAS-SNA LLM into our AI assistant to demonstrate its practical feasibility for real-world clinical and educational applications.",
    "URL": "http://arxiv.org/abs/2512.08998v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.10159",
    "type": "report",
    "title": "Enhancing Large Language Models for End-to-End Circuit Analysis Problem Solving",
    "author": [
      {
        "family": "Chen",
        "given": "Liangliang"
      },
      {
        "family": "Sun",
        "given": "Weiyu"
      },
      {
        "family": "Zhang",
        "given": "Ying"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          10
        ]
      ]
    },
    "abstract": "Large language models (LLMs) have shown strong performance in data-rich domains such as programming, but their reliability in engineering tasks remains limited. Circuit analysis -- requiring multimodal understanding and precise mathematical reasoning -- highlights these challenges. Although Gemini 2.5 Pro improves diagram interpretation and analog-circuit reasoning, it still struggles to consistently produce correct solutions when given both text and circuit diagrams. At the same time, engineering education needs scalable AI tools capable of generating accurate solutions for tasks such as automated homework feedback and question-answering. This paper presents an enhanced, end-to-end circuit problem solver built on Gemini 2.5 Pro. We first benchmark Gemini on a representative set of undergraduate circuit problems and identify two major failure modes: 1) circuit-recognition hallucinations, particularly incorrect source polarity detection, and 2) reasoning-process hallucinations, such as incorrect current directions. To address recognition errors, we integrate a fine-tuned YOLO detector and OpenCV processing to isolate voltage and current sources, enabling Gemini to re-identify source polarities from cropped images with near-perfect accuracy. To reduce reasoning errors, we introduce an ngspice-based verification loop in which Gemini generates a .cir file, ngspice simulates the circuit, and discrepancies trigger iterative regeneration with optional human-in-the-loop review. Across 83 problems, the proposed pipeline achieves a 97.59% success rate (81 correct solutions), substantially outperforming Gemini 2.5 Pro's original 79.52% accuracy. This system extends LLM capabilities for multimodal engineering problem-solving and supports the creation of high-quality educational datasets and AI-powered instructional tools.",
    "URL": "http://arxiv.org/abs/2512.10159v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.10441",
    "type": "report",
    "title": "Decoding Student Minds: Leveraging Conversational Agents for Psychological and Learning Analysis",
    "author": [
      {
        "family": "Chaabene",
        "given": "Nour El Houda Ben"
      },
      {
        "family": "Hammami",
        "given": "Hamza"
      },
      {
        "family": "Kahloul",
        "given": "Laid"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          11
        ]
      ]
    },
    "abstract": "This paper presents a psychologically-aware conversational agent designed to enhance both learning performance and emotional well-being in educational settings. The system combines Large Language Models (LLMs), a knowledge graph-enhanced BERT (KG-BERT), and a bidirectional Long Short-Term Memory (LSTM) with attention to classify students' cognitive and affective states in real time. Unlike prior chatbots limited to either tutoring or affective support, our approach leverages multimodal data-including textual semantics, prosodic speech features, and temporal behavioral trends-to infer engagement, stress, and conceptual understanding. A pilot study with university students demonstrated improved motivation, reduced stress, and moderate academic gains compared to baseline methods. These results underline the promise of integrating semantic reasoning, multimodal fusion, and temporal modeling to support adaptive, student-centered educational interventions.",
    "URL": "http://arxiv.org/abs/2512.10441v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.10487",
    "type": "report",
    "title": "LLM-Assisted AHP for Explainable Cyber Range Evaluation",
    "author": [
      {
        "family": "Kampourakis",
        "given": "Vyron"
      },
      {
        "family": "Kavallieratos",
        "given": "Georgios"
      },
      {
        "family": "Spathoulas",
        "given": "Georgios"
      },
      {
        "family": "Gkioulos",
        "given": "Vasileios"
      },
      {
        "family": "Katsikas",
        "given": "Sokratis"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          11
        ]
      ]
    },
    "abstract": "Cyber Ranges (CRs) have emerged as prominent platforms for cybersecurity training and education, especially for Critical Infrastructure (CI) sectors that face rising cyber threats. One way to address these threats is through hands-on exercises that bridge IT and OT domains to improve defensive readiness. However, consistently evaluating whether a CR platform is suitable and effective remains a challenge. This paper proposes an evaluation framework for CRs, emphasizing mission-critical settings by using a multi-criteria decision-making approach. We define a set of evaluation criteria that capture technical fidelity, training and assessment capabilities, scalability, usability, and other relevant factors. To weight and aggregate these criteria, we employ the Analytic Hierarchy Process (AHP), supported by a simulated panel of multidisciplinary experts implemented through a Large Language Model (LLM). This LLM-assisted expert reasoning enables consistent and reproducible pairwise comparisons across criteria without requiring direct expert convening. The framework's output equals quantitative scores that facilitate objective comparison of CR platforms and highlight areas for improvement. Overall, this work lays the foundation for a standardized and explainable evaluation methodology to guide both providers and end-users of CRs.",
    "URL": "http://arxiv.org/abs/2512.10487v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.10758",
    "type": "report",
    "title": "Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework",
    "author": [
      {
        "family": "Ding",
        "given": "Kaihua"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          11
        ]
      ]
    },
    "abstract": "The rapid adoption of generative AI has undermined traditional modular assessments in computing education, creating a disconnect between academic evaluation and industry practice. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and multi-year empirical validation.\n  We make three contributions. First, we establish two theoretical results: (1) assessments composed of interconnected problems, where outputs feed into subsequent stages, are more AI-resilient than modular assessments because current language models struggle with sustained multi-step reasoning and context; and (2) semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution patterns. These results challenge common policy and institutional guidance that promotes open-ended assessments as the primary safeguard for academic integrity.\n  Second, we validate these results using data from four university data science courses (N = 138). While students achieve near-perfect scores on AI-assisted modular homework, performance drops by roughly 30 percentage points on proctored exams, indicating substantial AI score inflation. Interconnected projects remain strongly correlated with modular assessments, suggesting they measure the same underlying skills while resisting AI misuse. Proctored exams show weaker alignment, implying they may assess test-taking ability rather than intended learning outcomes.\n  Third, we translate these findings into a practical assessment design framework. The proposed approach enables educators to create assessments that promote integrative thinking, reflect real-world AI-augmented workflows, and naturally resist trivial delegation to generative AI, thereby helping restore academic integrity.",
    "URL": "http://arxiv.org/abs/2512.10758v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.10777",
    "type": "report",
    "title": "Opportunities and Challenges in Harnessing Digital Technology for Effective Teaching and Learning",
    "author": [
      {
        "family": "Chen",
        "given": "Zhongzhou"
      },
      {
        "family": "Singh",
        "given": "Chandralekha"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          11
        ]
      ]
    },
    "abstract": "Most of today's educators are in no shortage of digital and online learning technologies available at their fingertips, ranging from Learning Management Systems such as Canvas, Blackboard, or Moodle, online meeting tools, online homework, and tutoring systems, exam proctoring platforms, computer simulations, and even virtual reality/augmented reality technologies. Furthermore, with the rapid development and wide availability of generative artificial intelligence (GenAI) services such as ChatGPT, we are just at the beginning of harnessing their potential to transform higher education. Yet, facing the large number of available options provided by cutting-edge technology, an imminent question on the mind of most educators is the following: how should I choose the technologies and integrate them into my teaching process so that they would best support student learning? We contemplate over these types of important and timely questions and share our reflections on evidence-based approaches to harnessing digital learning tools using a Self-regulated Engaged Learning Framework we have employed in our research in physics education that can be valuable for educators in other disciplines.",
    "DOI": "10.3390/higheredu4010006",
    "URL": "https://doi.org/10.3390/higheredu4010006",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.10785",
    "type": "report",
    "title": "Developing and Evaluating a Large Language Model-Based Automated Feedback System Grounded in Evidence-Centered Design for Supporting Physics Problem Solving",
    "author": [
      {
        "family": "Maus",
        "given": "Holger"
      },
      {
        "family": "Tschisgale",
        "given": "Paul"
      },
      {
        "family": "Kieser",
        "given": "Fabian"
      },
      {
        "family": "Petersen",
        "given": "Stefan"
      },
      {
        "family": "Wulff",
        "given": "Peter"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          11
        ]
      ]
    },
    "abstract": "Generative AI offers new opportunities for individualized and adaptive learning, particularly through large language model (LLM)-based feedback systems. While LLMs can produce effective feedback for relatively straightforward conceptual tasks, delivering high-quality feedback for tasks that require advanced domain expertise, such as physics problem solving, remains a substantial challenge. This study presents the design of an LLM-based feedback system for physics problem solving grounded in evidence-centered design (ECD) and evaluates its performance within the German Physics Olympiad. Participants assessed the usefulness and accuracy of the generated feedback, which was generally perceived as useful and highly accurate. However, an in-depth analysis revealed that the feedback contained factual errors in 20% of cases; errors that often went unnoticed by the students. We discuss the risks associated with uncritical reliance on LLM-based feedback systems and outline potential directions for generating more adaptive and reliable LLM-based feedback in the future.",
    "URL": "http://arxiv.org/abs/2512.10785v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.11882",
    "type": "report",
    "title": "An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education",
    "author": [
      {
        "family": "Happe",
        "given": "Lucia"
      },
      {
        "family": "Fuch√ü",
        "given": "Dominik"
      },
      {
        "family": "H√ºttner",
        "given": "Luca"
      },
      {
        "family": "Marquardt",
        "given": "Kai"
      },
      {
        "family": "Koziolek",
        "given": "Anne"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          8
        ]
      ]
    },
    "abstract": "The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn.",
    "URL": "http://arxiv.org/abs/2512.11882v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.11930",
    "type": "report",
    "title": "Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction",
    "author": [
      {
        "family": "Jiang",
        "given": "Mei"
      },
      {
        "family": "Shen",
        "given": "Haihai"
      },
      {
        "family": "Luo",
        "given": "Zhuo"
      },
      {
        "family": "Li",
        "given": "Bingdong"
      },
      {
        "family": "Hong",
        "given": "Wenjing"
      },
      {
        "family": "Tang",
        "given": "Ke"
      },
      {
        "family": "Zhou",
        "given": "Aimin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          12
        ]
      ]
    },
    "abstract": "Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent.",
    "URL": "http://arxiv.org/abs/2512.11930v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.11934",
    "type": "report",
    "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching",
    "author": [
      {
        "family": "Mazaherian",
        "given": "Adeleh"
      },
      {
        "family": "Nourbakhsh",
        "given": "Erfan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          12
        ]
      ]
    },
    "abstract": "The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent).",
    "URL": "http://arxiv.org/abs/2512.11934v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.12045",
    "type": "report",
    "title": "AI as a Teaching Partner: Early Lessons from Classroom Codesign with Secondary Teachers",
    "author": [
      {
        "family": "Liu",
        "given": "Alex"
      },
      {
        "family": "Esbenshade",
        "given": "Lief"
      },
      {
        "family": "Sarkar",
        "given": "Shawon"
      },
      {
        "family": "Tian",
        "given": "Zewei"
      },
      {
        "family": "Sun",
        "given": "Min"
      },
      {
        "family": "Zhang",
        "given": "Zachary"
      },
      {
        "family": "Han",
        "given": "Thomas"
      },
      {
        "family": "Lapicus",
        "given": "Yulia"
      },
      {
        "family": "He",
        "given": "Kevin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          12
        ]
      ]
    },
    "abstract": "This report presents a comprehensive account of the Colleague AI Classroom pilot, a collaborative design (co-design) study that brought generative AI technology directly into real classrooms. In this study, AI functioned as a third agent, an active participant that mediated feedback, supported inquiry, and extended teachers' instructional reach while preserving human judgment and teacher authority.\n  Over seven weeks in spring 2025, 21 in-service teachers from four Washington State public school districts and one independent school integrated four AI-powered features of the Colleague AI Classroom into their instruction: Teaching Aide, Assessment and AI Grading, AI Tutor, and Student Growth Insights. More than 600 students in grades 6-12 used the platform in class at the direction of their teachers, who designed and facilitated the AI activities.\n  During the Classroom pilot, teachers were co-design partners: they planned activities, implemented them with students, and provided weekly reflections on AI's role in classroom settings. The teachers' feedback guided iterative improvements for Colleague AI. The research team captured rich data through surveys, planning and reflection forms, group meetings, one-on-one interviews, and platform usage logs to understand where AI adds instructional value and where it requires refinement.",
    "URL": "http://arxiv.org/abs/2512.12045v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.12306",
    "type": "report",
    "title": "From Co-Design to Metacognitive Laziness: Evaluating Generative AI in Vocational Education",
    "author": [
      {
        "family": "Yunus",
        "given": "Amir"
      },
      {
        "family": "Gay",
        "given": "Peng Rend"
      },
      {
        "family": "Lee",
        "given": "Oon Teng"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          13
        ]
      ]
    },
    "abstract": "This study examines the development and deployment of a Generative AI proof-of-concept (POC) designed to support lecturers in a vocational education setting in Singapore. Employing a user-centred, mixed-methods design process, we co-developed an AI chatbot with lecturers to address recurring instructional challenges during exam preparation, specifically managing repetitive questions and scaling feedback delivery. The POC achieved its primary operational goals: lecturers reported streamlined workflows, reduced cognitive load, and observed improved student confidence in navigating course content. However, the deployment yielded unexpected insights into student learning behaviours. Despite enhanced teaching processes, performance data revealed no significant improvement in overall student assessment outcomes. Deep analysis of interaction logs identified concerning patterns, including self-efficacy-driven dependency, \"metacognitive laziness\" (cognitive offloading), and divergent usage strategies. While high-ability students leveraged the tool for strategic verification, low-ability students frequently used it to bypass cognitive effort, potentially exacerbating performance gaps. These findings suggest that Generative AI's educational influence extends beyond instructional efficiency to shape cognitive engagement, self-regulation, and learner equity. The study raises consequential design questions regarding how AI tools can be engineered to minimise dependency, scaffold metacognitive development, and calibrate support across varying ability levels. We conclude that while Generative AI can substantially enhance the teaching experience, achieving meaningful learning gains requires rigorous attention to learner behaviour and the equitable design of AI-supported environments.",
    "URL": "http://arxiv.org/abs/2512.12306v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.12503",
    "type": "report",
    "title": "KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs",
    "author": [
      {
        "family": "Ye",
        "given": "Mingrui"
      },
      {
        "family": "Zheng",
        "given": "Chanjin"
      },
      {
        "family": "Yu",
        "given": "Zengyi"
      },
      {
        "family": "Xiang",
        "given": "Chenyu"
      },
      {
        "family": "Zhao",
        "given": "Zhixue"
      },
      {
        "family": "Yuan",
        "given": "Zheng"
      },
      {
        "family": "Yannakoudakis",
        "given": "Helen"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          14
        ]
      ]
    },
    "abstract": "Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.",
    "URL": "http://arxiv.org/abs/2512.12503v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.12633",
    "type": "report",
    "title": "DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model",
    "author": [
      {
        "family": "Tao",
        "given": "Zhou"
      },
      {
        "family": "Wang",
        "given": "Shida"
      },
      {
        "family": "Hua",
        "given": "Yongxiang"
      },
      {
        "family": "Cao",
        "given": "Haoyu"
      },
      {
        "family": "Xu",
        "given": "Linli"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          14
        ]
      ]
    },
    "abstract": "Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.",
    "URL": "http://arxiv.org/abs/2512.12633v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.12770",
    "type": "report",
    "title": "Curi√≥-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining",
    "author": [
      {
        "family": "Almeida",
        "given": "Thales Sales"
      },
      {
        "family": "Nogueira",
        "given": "Rodrigo"
      },
      {
        "family": "Pedrini",
        "given": "H√©lio"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          14
        ]
      ]
    },
    "abstract": "Continued pretraining extends a language model's capabilities by further exposing it to additional data, often tailored to a specific linguistic or domain context. This strategy has emerged as an efficient alternative to full retraining when adapting general-purpose models to new settings. In this work, we investigate this paradigm through Curi√≥ 7B, a 7-billion-parameter model derived from LLaMA-2 and trained on 100 billion Portuguese tokens from the ClassiCC-PT corpus - the most extensive Portuguese-specific continued-pretraining effort above the three-billion-parameter scale to date. Beyond scale, we investigate whether quantity alone suffices or whether data quality plays a decisive role in linguistic adaptation. To this end, we introduce Curi√≥-Edu 7B, a variant trained exclusively on the educational and STEM-filtered subset of the same corpus, totaling just 10 billion tokens. Despite using only 10% of the data and 20% of the computation, Curi√≥-Edu 7B surpasses the full-corpus model in our evaluations, demonstrating that data selection can be fundamental even when adapting models with limited prior exposure to the target language. The developed models are available at https://huggingface.co/collections/ClassiCC-Corpus/curio-edu",
    "URL": "http://arxiv.org/abs/2512.12770v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.12775",
    "type": "report",
    "title": "Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions",
    "author": [
      {
        "family": "Araujo",
        "given": "Pedro Henrique Luz de"
      },
      {
        "family": "Hedderich",
        "given": "Michael A."
      },
      {
        "family": "Modarressi",
        "given": "Ali"
      },
      {
        "family": "Schuetze",
        "given": "Hinrich"
      },
      {
        "family": "Roth",
        "given": "Benjamin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          14
        ]
      ]
    },
    "abstract": "Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.",
    "URL": "http://arxiv.org/abs/2512.12775v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.13102",
    "type": "report",
    "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions",
    "author": [
      {
        "family": "Ambati",
        "given": "Rajeev Bhatt"
      },
      {
        "family": "Niu",
        "given": "Tianyi"
      },
      {
        "family": "Singh",
        "given": "Aashu"
      },
      {
        "family": "Mishra",
        "given": "Shlok"
      },
      {
        "family": "Srivastava",
        "given": "Shashank"
      },
      {
        "family": "Chaturvedi",
        "given": "Snigdha"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          15
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.",
    "URL": "http://arxiv.org/abs/2512.13102v2",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.13526",
    "type": "report",
    "title": "Async Control: Stress-testing Asynchronous Control Measures for LLM Agents",
    "author": [
      {
        "family": "Stickland",
        "given": "Asa Cooper"
      },
      {
        "family": "Michelfeit",
        "given": "Jan"
      },
      {
        "family": "Mani",
        "given": "Arathi"
      },
      {
        "family": "Griffin",
        "given": "Charlie"
      },
      {
        "family": "Matthews",
        "given": "Ollie"
      },
      {
        "family": "Korbak",
        "given": "Tomek"
      },
      {
        "family": "Inglis",
        "given": "Rogan"
      },
      {
        "family": "Makins",
        "given": "Oliver"
      },
      {
        "family": "Cooney",
        "given": "Alan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          15
        ]
      ]
    },
    "abstract": "LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.",
    "URL": "http://arxiv.org/abs/2512.13526v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.13658",
    "type": "report",
    "title": "Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance",
    "author": [
      {
        "family": "Molavi",
        "given": "Mohammadreza"
      },
      {
        "family": "Moein",
        "given": "Mohammad"
      },
      {
        "family": "Tavakoli",
        "given": "Mohammadreza"
      },
      {
        "family": "Faraji",
        "given": "Abdolali"
      },
      {
        "family": "Mol",
        "given": "Stefan T."
      },
      {
        "family": "Kismih√≥k",
        "given": "G√°bor"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          15
        ]
      ]
    },
    "abstract": "As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.",
    "URL": "http://arxiv.org/abs/2512.13658v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.13730",
    "type": "report",
    "title": "Exploring the Modular Integration of \"AI + Architecture\" Pedagogy in Undergraduate Design Education: A Case Study of Architectural Design III/IV Courses at Zhejiang University",
    "author": [
      {
        "family": "Jiaqi",
        "given": "Wang"
      },
      {
        "family": "Yi",
        "given": "Lan"
      },
      {
        "family": "Xiang",
        "given": "Chen"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          14
        ]
      ]
    },
    "abstract": "This study investigates AI integration in architectural education through a teaching experiment in Zhejiang University's 2024-25 grade three undergraduate design studio. Adopting a dual-module framework (20-hour AI training + embedded ethics discussions), the course introduced deep learning models, LLMs, AIGC, LoRA, and ComfyUI while maintaining the original curriculum structure, supported by dedicated technical instructors. Findings demonstrate the effectiveness of phased guidance, balanced technical-ethical approaches, and institutional support. The model improved students' digital skills and strategic cognition while addressing AI ethics, providing a replicable approach combining technical and critical learning in design education.",
    "URL": "http://arxiv.org/abs/2512.13730v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.13914",
    "type": "report",
    "title": "Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming",
    "author": [
      {
        "family": "Nanjundappa",
        "given": "Bhargav Chickmagalur"
      },
      {
        "family": "Maaheshwari",
        "given": "Spandan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          15
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context.\n  We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.",
    "URL": "http://arxiv.org/abs/2512.13914v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.13978",
    "type": "report",
    "title": "Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms",
    "author": [
      {
        "family": "Cao",
        "given": "Yang"
      },
      {
        "family": "Chen",
        "given": "Yubin"
      },
      {
        "family": "Guo",
        "given": "Xuyang"
      },
      {
        "family": "Song",
        "given": "Zhao"
      },
      {
        "family": "Yue",
        "given": "Song"
      },
      {
        "family": "Zhang",
        "given": "Jiahao"
      },
      {
        "family": "Zhao",
        "given": "Jiale"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          16
        ]
      ]
    },
    "abstract": "The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${√≥}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].\n  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.",
    "URL": "http://arxiv.org/abs/2512.13978v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.14220",
    "type": "report",
    "title": "Estimating problem difficulty without ground truth using Large Language Model comparisons",
    "author": [
      {
        "family": "Ballon",
        "given": "Marthe"
      },
      {
        "family": "Algaba",
        "given": "Andres"
      },
      {
        "family": "Verbeken",
        "given": "Brecht"
      },
      {
        "family": "Ginis",
        "given": "Vincent"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          16
        ]
      ]
    },
    "abstract": "Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \\geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\\%$ degradation in Pearson correlation for $10\\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.",
    "URL": "http://arxiv.org/abs/2512.14220v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.14673",
    "type": "report",
    "title": "Reconsidering Conversational Norms in LLM Chatbots for Sustainable AI",
    "author": [
      {
        "family": "Santos",
        "given": "Ronnie de Souza"
      },
      {
        "family": "Magalh√£es",
        "given": "Cleyton"
      },
      {
        "family": "Santos",
        "given": "Italo"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          16
        ]
      ]
    },
    "abstract": "LLM based chatbots have become central interfaces in technical, educational, and analytical domains, supporting tasks such as code reasoning, problem solving, and information exploration. As these systems scale, sustainability concerns have intensified, with most assessments focusing on model architecture, hardware efficiency, and deployment infrastructure. However, existing mitigation efforts largely overlook how user interaction practices themselves shape the energy profile of LLM based systems. In this vision paper, we argue that interaction level behavior appears to be an underexamined factor shaping the environmental impact of LLM based systems, and we present this issue across four dimensions. First, extended conversational patterns increase token production and raise the computational cost of inference. Second, expectations of instant responses limit opportunities for energy aware scheduling and workload consolidation. Third, everyday user habits contribute to cumulative operational demand in ways that are rarely quantified. Fourth, the accumulation of context affects memory requirements and reduces the efficiency of long running dialogues. Addressing these challenges requires rethinking how chatbot interactions are designed and conceptualized, and adopting perspectives that recognize sustainability as partly dependent on the conversational norms through which users engage with LLM based systems.",
    "URL": "http://arxiv.org/abs/2512.14673v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.14944",
    "type": "report",
    "title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning",
    "author": [
      {
        "family": "Jeddi",
        "given": "Ahmadreza"
      },
      {
        "family": "Karaimer",
        "given": "Hakki Can"
      },
      {
        "family": "Nguyen",
        "given": "Hue"
      },
      {
        "family": "Wang",
        "given": "Zhongling"
      },
      {
        "family": "Zhao",
        "given": "Ke"
      },
      {
        "family": "Rajabi",
        "given": "Javad"
      },
      {
        "family": "Zhang",
        "given": "Ran"
      },
      {
        "family": "Goyal",
        "given": "Raghav"
      },
      {
        "family": "Taati",
        "given": "Babak"
      },
      {
        "family": "Grzeszczuk",
        "given": "Radek"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          16
        ]
      ]
    },
    "abstract": "Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.",
    "URL": "http://arxiv.org/abs/2512.14944v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.15183",
    "type": "report",
    "title": "From NLG Evaluation to Modern Student Assessment in the Era of ChatGPT: The Great Misalignment Problem and Pedagogical Multi-Factor Assessment (P-MFA)",
    "author": [
      {
        "family": "H√§m√§l√§inen",
        "given": "Mika"
      },
      {
        "family": "Leivisk√§",
        "given": "Kimmo"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          17
        ]
      ]
    },
    "abstract": "This paper explores the growing epistemic parallel between NLG evaluation and grading of students in a Finnish University. We argue that both domains are experiencing a Great Misalignment Problem. As students increasingly use tools like ChatGPT to produce sophisticated outputs, traditional assessment methods that focus on final products rather than learning processes have lost their validity. To address this, we introduce the Pedagogical Multi-Factor Assessment (P-MFA) model, a process-based, multi-evidence framework inspired by the logic of multi-factor authentication.",
    "URL": "http://arxiv.org/abs/2512.15183v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.15298",
    "type": "report",
    "title": "ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I",
    "author": [
      {
        "family": "Ga",
        "given": "Seok-Hyun"
      },
      {
        "family": "Chang",
        "given": "Chun-Yen"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          17
        ]
      ]
    },
    "abstract": "The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that \"Perception Errors\" were dominant, highlighting a \"Perception-Cognition Gap\" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a \"Calculation-Conceptualization Discrepancy,\" successfully performing calculations while failing to apply the underlying scientific concepts, and \"Process Hallucination,\" where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing \"AI-resistant questions\" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.",
    "URL": "http://arxiv.org/abs/2512.15298v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.15397",
    "type": "report",
    "title": "ORACLE: Time-Dependent Recursive Summary Graphs for Foresight on News Data Using LLMs",
    "author": [
      {
        "family": "Kharlashkin",
        "given": "Lev"
      },
      {
        "family": "Morooka",
        "given": "Eiaki"
      },
      {
        "family": "Tereshchenko",
        "given": "Yehor"
      },
      {
        "family": "H√§m√§l√§inen",
        "given": "Mika"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          17
        ]
      ]
    },
    "abstract": "ORACLE turns daily news into week-over-week, decision-ready insights for one of the Finnish University of Applied Sciences. The platform crawls and versions news, applies University-specific relevance filtering, embeds content, classifies items into PESTEL dimensions and builds a concise Time-Dependent Recursive Summary Graph (TRSG): two clustering layers summarized by an LLM and recomputed weekly. A lightweight change detector highlights what is new, removed or changed, then groups differences into themes for PESTEL-aware analysis. We detail the pipeline, discuss concrete design choices that make the system stable in production and present a curriculum-intelligence use case with an evaluation plan.",
    "URL": "http://arxiv.org/abs/2512.15397v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.16036",
    "type": "report",
    "title": "Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education",
    "author": [
      {
        "family": "Woodbridge",
        "given": "Diane Myung-kyung"
      },
      {
        "family": "Seba",
        "given": "Allyson"
      },
      {
        "family": "Seba",
        "given": "Freddie"
      },
      {
        "family": "Schwartz",
        "given": "Aydin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          17
        ]
      ]
    },
    "abstract": "As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.",
    "URL": "http://arxiv.org/abs/2512.16036v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.16428",
    "type": "report",
    "title": "Preparing Future-Ready Learners: K12 Skills Shift and GenAI EdTech Innovation Direction",
    "author": [
      {
        "family": "Miao",
        "given": "Xin"
      },
      {
        "family": "Mishra",
        "given": "Pawan Kumar"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          18
        ]
      ]
    },
    "abstract": "Since Generative AI came out it has quickly embedded itself in our social fabric, triggering lots of discussions, predictions, and efforts from research, industry, government and capital market to experiment and embrace the technology. The question for the global K12 education is, what and how should our children learn in this fast changing world to be prepared for the changing labor market and live a happy and balanced life? Three key aspects will be discussed: 1) Skills; 2) Evaluation of Learning; 3) Strategic GenAI-powered EdTech innovation for long term educational impact.",
    "URL": "http://arxiv.org/abs/2512.16428v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.16649",
    "type": "report",
    "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
    "author": [
      {
        "family": "He",
        "given": "Bingxiang"
      },
      {
        "family": "Qu",
        "given": "Zekai"
      },
      {
        "family": "Liu",
        "given": "Zeyuan"
      },
      {
        "family": "Chen",
        "given": "Yinghao"
      },
      {
        "family": "Zuo",
        "given": "Yuxin"
      },
      {
        "family": "Qian",
        "given": "Cheng"
      },
      {
        "family": "Zhang",
        "given": "Kaiyan"
      },
      {
        "family": "Chen",
        "given": "Weize"
      },
      {
        "family": "Xiao",
        "given": "Chaojun"
      },
      {
        "family": "Cui",
        "given": "Ganqu"
      },
      {
        "family": "Ding",
        "given": "Ning"
      },
      {
        "family": "Liu",
        "given": "Zhiyuan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          18
        ]
      ]
    },
    "abstract": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \\textbf{Is this complexity necessary?} We present \\textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2$\\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.",
    "URL": "http://arxiv.org/abs/2512.16649v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.16701",
    "type": "report",
    "title": "Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences",
    "author": [
      {
        "family": "Adorni",
        "given": "Giovanni"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          18
        ]
      ]
    },
    "abstract": "Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\\-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.\n  We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.",
    "URL": "http://arxiv.org/abs/2512.16701v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.17060",
    "type": "report",
    "title": "On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues",
    "author": [
      {
        "family": "Zamojska",
        "given": "Monika"
      },
      {
        "family": "Chudziak",
        "given": "Jaros≈Çaw A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          18
        ]
      ]
    },
    "abstract": "LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.",
    "URL": "http://arxiv.org/abs/2512.17060v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.18306",
    "type": "report",
    "title": "Leveraging Peer, Self, and Teacher Assessments for Generative AI-Enhanced Feedback",
    "author": [
      {
        "family": "Becerra",
        "given": "Alvaro"
      },
      {
        "family": "Cobos",
        "given": "Ruth"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          20
        ]
      ]
    },
    "abstract": "Providing timely and meaningful feedback remains a persistent challenge in higher education, especially in large courses where teachers must balance formative depth with scalability. Recent advances in Generative Artificial Intelligence (GenAI) offer new opportunities to support feedback processes while maintaining human oversight. This paper presents an study conducted within the AICoFe (AI-based Collaborative Feedback) system, which integrates teacher, peer, and self-assessments of engineering students' oral presentations. Using a validated rubric, 46 evaluation sets were analyzed to examine agreement, correlation, and bias across evaluators. The analyses revealed consistent overall alignment among sources but also systematic variations in scoring behavior, reflecting distinct evaluative perspectives. These findings informed the proposal of an enhanced GenAI model within AICoFe system, designed to integrate human assessments through weighted input aggregation, bias detection, and context-aware feedback generation. The study contributes empirical evidence and design principles for developing GenAI-based feedback systems that combine data-based efficiency with pedagogical validity and transparency.",
    "URL": "http://arxiv.org/abs/2512.18306v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.19512",
    "type": "report",
    "title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
    "author": [
      {
        "family": "Song",
        "given": "Ziyang"
      },
      {
        "family": "Zang",
        "given": "Zelin"
      },
      {
        "family": "Chen",
        "given": "Zuyao"
      },
      {
        "family": "Liang",
        "given": "Xusheng"
      },
      {
        "family": "Yi",
        "given": "Dong"
      },
      {
        "family": "Wu",
        "given": "Jinlin"
      },
      {
        "family": "Liu",
        "given": "Hongbin"
      },
      {
        "family": "Luo",
        "given": "Jiebo"
      },
      {
        "family": "Lei",
        "given": "Zhen."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          22
        ]
      ]
    },
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1",
    "URL": "http://arxiv.org/abs/2512.19512v2",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.19682",
    "type": "report",
    "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
    "author": [
      {
        "family": "Guo",
        "given": "Jiacheng"
      },
      {
        "family": "Yang",
        "given": "Ling"
      },
      {
        "family": "Chen",
        "given": "Peter"
      },
      {
        "family": "Xiao",
        "given": "Qixin"
      },
      {
        "family": "Wang",
        "given": "Yinjie"
      },
      {
        "family": "Juan",
        "given": "Xinzhe"
      },
      {
        "family": "Qiu",
        "given": "Jiahao"
      },
      {
        "family": "Shen",
        "given": "Ke"
      },
      {
        "family": "Wang",
        "given": "Mengdi"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          22
        ]
      ]
    },
    "abstract": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $Œ±$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
    "URL": "http://arxiv.org/abs/2512.19682v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.19903",
    "type": "report",
    "title": "How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse",
    "author": [
      {
        "family": "Vanacore",
        "given": "Kirk"
      },
      {
        "family": "Kizilcec",
        "given": "Rene F."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          22
        ]
      ]
    },
    "abstract": "Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.",
    "URL": "http://arxiv.org/abs/2512.19903v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.20714",
    "type": "report",
    "title": "From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education",
    "author": [
      {
        "family": "Reihanian",
        "given": "Iman"
      },
      {
        "family": "Hou",
        "given": "Yunfei"
      },
      {
        "family": "Sun",
        "given": "Qingquan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          23
        ]
      ]
    },
    "abstract": "Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.",
    "DOI": "10.3390/ai7010006",
    "URL": "https://doi.org/10.3390/ai7010006",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.20732",
    "type": "report",
    "title": "FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs",
    "author": [
      {
        "family": "Mohammadzadeh",
        "given": "Saeed"
      },
      {
        "family": "Hamdi",
        "given": "Erfan"
      },
      {
        "family": "Shor",
        "given": "Joel"
      },
      {
        "family": "Lejeune",
        "given": "Emma"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          23
        ]
      ]
    },
    "abstract": "As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification. The discipline requires constructing explicit models of physical systems and reasoning about geometry, spatial relationships, and material behavior, connecting directly to emerging AI goals in physical reasoning and world modeling. We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code. FEM-Bench 2025 contains a suite of introductory but nontrivial tasks aligned with material from a first graduate course on computational mechanics. These tasks capture essential numerical and physical modeling challenges while representing only a small fraction of the complexity present in the discipline. Despite their simplicity, state-of-the-art LLMs do not reliably solve all of them. In a five attempt run, the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least once and 26/33 tasks all five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%. Other popular models showed broad performance variation. FEM-Bench establishes a structured foundation for evaluating AI-generated scientific code, and future iterations will incorporate increasingly sophisticated tasks to track progress as models evolve.",
    "URL": "http://arxiv.org/abs/2512.20732v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.20780",
    "type": "report",
    "title": "Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles",
    "author": [
      {
        "family": "Abdulsalam",
        "given": "Ramatu Oiza"
      },
      {
        "family": "Aroyehun",
        "given": "Segun"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          23
        ]
      ]
    },
    "abstract": "Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.",
    "URL": "http://arxiv.org/abs/2512.20780v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.21041",
    "type": "report",
    "title": "When LLMs fall short in Deductive Coding: Model Comparison and Human AI Collaboration Workflow Design",
    "author": [
      {
        "family": "Li",
        "given": "Zijian"
      },
      {
        "family": "Tang",
        "given": "Luzhen"
      },
      {
        "family": "Xia",
        "given": "Mengyu"
      },
      {
        "family": "Li",
        "given": "Xinyu"
      },
      {
        "family": "Chen",
        "given": "Naping"
      },
      {
        "family": "Ga≈°eviƒá",
        "given": "Dragan"
      },
      {
        "family": "Fan",
        "given": "Yizhou"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          24
        ]
      ]
    },
    "abstract": "With generative artificial intelligence driving the growth of dialogic data in education, automated coding is a promising direction for learning analytics to improve efficiency. This surge highlights the need to understand the nuances of student-AI interactions, especially those rare yet crucial. However, automated coding may struggle to capture these rare codes due to imbalanced data, while human coding remains time-consuming and labour-intensive. The current study examined the potential of large language models (LLMs) to approximate or replace humans in deductive, theory-driven coding, while also exploring how human-AI collaboration might support such coding tasks at scale. We compared the coding performance of small transformer classifiers (e.g., BERT) and LLMs in two datasets, with particular attention to imbalanced head-tail distributions in dialogue codes. Our results showed that LLMs did not outperform BERT-based models and exhibited systematic errors and biases in deductive coding tasks. We designed and evaluated a human-AI collaborative workflow that improved coding efficiency while maintaining coding reliability. Our findings reveal both the limitations of LLMs -- especially their difficulties with semantic similarity and theoretical interpretations and the indispensable role of human judgment -- while demonstrating the practical promise of human-AI collaborative workflows for coding.",
    "URL": "http://arxiv.org/abs/2512.21041v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.21055",
    "type": "report",
    "title": "Making AI Work: An Autoethnography of a Workaround in Higher Education",
    "author": [
      {
        "family": "Lee",
        "given": "Shang Chieh"
      },
      {
        "family": "Narayan",
        "given": "Bhuva"
      },
      {
        "family": "Shum",
        "given": "Simon Buckingham"
      },
      {
        "family": "Ng",
        "given": "Stella"
      },
      {
        "family": "Kocaballi",
        "given": "A. Baki"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          24
        ]
      ]
    },
    "abstract": "Research on the implementation of Generative Artificial Intelligence (GenAI) in higher education often focuses on strategic goals, overlooking the hidden, and often politically charged, labour required to make it functional. This paper provides an insider's account of the sociotechnical friction that arises when an institutional goal of empowering non-technical staff conflicts with the technical limitations of enterprise Large Language Models (LLMs). Through analytic autoethnography, this study examines a GenAI project pushed to an impasse, focusing on a workaround developed to navigate not only technical constraints but also the combined challenge of organisational territoriality and assertions of positional power. Drawing upon Alter's (2014) theory of workarounds, the analysis interprets \"articulation work\" as a form of \"invisible labour\". By engaging with the Information Systems (IS) domains of user innovation and technology-in-practice, this study argues that such user-driven workarounds should be understood not as deviations, but as integral acts of sociotechnical integration. This integration, however, highlights the central paradoxes of modern GenAI where such workarounds for \"unfinished\" systems can simultaneously create unofficial \"shadow\" systems and obscure the crucial, yet invisible, sociotechnical labour involved. The findings suggest that the invisible labour required to integrate GenAI within complex organisational politics is an important, rather than peripheral, component of how it becomes functional in practice.",
    "URL": "http://arxiv.org/abs/2512.21055v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.21238",
    "type": "report",
    "title": "Assessing the Software Security Comprehension of Large Language Models",
    "author": [
      {
        "family": "Siddiq",
        "given": "Mohammed Latif"
      },
      {
        "family": "Sekerak",
        "given": "Natalie"
      },
      {
        "family": "Karam",
        "given": "Antonio"
      },
      {
        "family": "Leal",
        "given": "Maria"
      },
      {
        "family": "Islam-Gomes",
        "given": "Arvin"
      },
      {
        "family": "Santos",
        "given": "Joanna C. S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          24
        ]
      ]
    },
    "abstract": "Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.",
    "URL": "http://arxiv.org/abs/2512.21238v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.21347",
    "type": "report",
    "title": "Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey",
    "author": [
      {
        "family": "Brito",
        "given": "V√≠tor Mateus de"
      },
      {
        "family": "Farias",
        "given": "Kleinner"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          19
        ]
      ]
    },
    "abstract": "The rapid advancement of Large Language Models (LLMs) is reshaping software engineering by profoundly influencing coding, documentation, and system maintenance practices. As these tools become deeply embedded in developers' daily workflows, understanding how they are used has become essential. This paper reports an empirical study of LLM adoption in software engineering, based on a survey of 46 industry professionals with diverse educational backgrounds and levels of experience. The results reveal positive perceptions of LLMs, particularly regarding faster resolution of technical questions, improved documentation support, and enhanced source code standardization. However, respondents also expressed concerns about cognitive dependence, security risks, and the potential erosion of technical autonomy. These findings underscore the need for critical and supervised use of LLM-based tools. By grounding the discussion in empirical evidence from industry practice, this study bridges the gap between academic discourse and real-world software development. The results provide actionable insights for developers and researchers seeking to adopt and evolve LLM-based technologies in a more effective, responsible, and secure manner, while also motivating future research on their cognitive, ethical, and organizational implications.",
    "URL": "http://arxiv.org/abs/2512.21347v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.21422",
    "type": "report",
    "title": "Teaching People LLM's Errors and Getting it Right",
    "author": [
      {
        "family": "Stringham",
        "given": "Nathan"
      },
      {
        "family": "Chaleshtori",
        "given": "Fateme Hashemi"
      },
      {
        "family": "Yan",
        "given": "Xinyuan"
      },
      {
        "family": "Xu",
        "given": "Zhichao"
      },
      {
        "family": "Wang",
        "given": "Bei"
      },
      {
        "family": "Marasoviƒá",
        "given": "Ana"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          24
        ]
      ]
    },
    "abstract": "People use large language models (LLMs) when they should not. This is partly because they see LLMs compose poems and answer intricate questions, so they understandably, but incorrectly, assume LLMs won't stumble on basic tasks like simple arithmetic. Prior work has tried to address this by clustering instance embeddings into regions where an LLM is likely to fail and automatically describing patterns in these regions. The found failure patterns are taught to users to mitigate their overreliance. Yet, this approach has not fully succeeded. In this analysis paper, we aim to understand why.\n  We first examine whether the negative result stems from the absence of failure patterns. We group instances in two datasets by their meta-labels and evaluate an LLM's predictions on these groups. We then define criteria to flag groups that are sizable and where the LLM is error-prone, and find meta-label groups that meet these criteria. Their meta-labels are the LLM's failure patterns that could be taught to users, so they do exist. We next test whether prompting and embedding-based approaches can surface these known failures. Without this, users cannot be taught about them to reduce their overreliance. We find mixed results across methods, which could explain the negative result. Finally, we revisit the final metric that measures teaching effectiveness. We propose to assess a user's ability to effectively use the given failure patterns to anticipate when an LLM is error-prone. A user study shows a positive effect from teaching with this metric, unlike the human-AI team accuracy. Our findings show that teaching failure patterns could be a viable approach to mitigating overreliance, but success depends on better automated failure-discovery methods and using metrics like ours.",
    "URL": "http://arxiv.org/abs/2512.21422v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.22404",
    "type": "report",
    "title": "Mining the Gold: Student-AI Chat Logs as Rich Sources for Automated Knowledge Gap Detection",
    "author": [
      {
        "family": "Fu",
        "given": "Quanzhi"
      },
      {
        "family": "Wu",
        "given": "Qiyu"
      },
      {
        "family": "Williams",
        "given": "Dan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          26
        ]
      ]
    },
    "abstract": "With the significant increase in enrollment in computing-related programs over the past 20 years, lecture sizes have grown correspondingly. In large lectures, instructors face challenges on identifying students' knowledge gaps timely, which is critical for effective teaching. Existing classroom response systems rely on instructor-initiated interactions, which limits their ability to capture the spontaneous knowledge gaps that naturally emerge during lectures. With the widespread adoption of LLMs among students, we recognize these student-AI dialogues as a valuable, student-centered data source for identifying knowledge gaps. In this idea paper, we propose QueryQuilt, a multi-agent LLM framework that automatically detects common knowledge gaps in large-scale lectures by analyzing students' chat logs with AI assistants. QueryQuilt consists of two key components: (1) a Dialogue Agent that responds to student questions while employing probing questions to reveal underlying knowledge gaps, and (2) a Knowledge Gap Identification Agent that systematically analyzes these dialogues to identify knowledge gaps across the student population. By generating frequency distributions of identified gaps, instructors can gain comprehensive insights into class-wide understanding. Our evaluation demonstrates promising results, with QueryQuilt achieving 100% accuracy in identifying knowledge gaps among simulated students and 95% completeness when tested on real student-AI dialogue data. These initial findings indicate the system's potential for facilitate teaching in authentic learning environments. We plan to deploy QueryQuilt in actual classroom settings for comprehensive evaluation, measuring its detection accuracy and impact on instruction.",
    "URL": "http://arxiv.org/abs/2512.22404v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.22496",
    "type": "report",
    "title": "Hierarchical Pedagogical Oversight: A Multi-Agent Adversarial Framework for Reliable AI Tutoring",
    "author": [
      {
        "family": "Sadhu",
        "given": "Saisab"
      },
      {
        "family": "Dhor",
        "given": "Ashim"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          27
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) are increasingly deployed as automated tutors to address educator shortages; however, they often fail at pedagogical reasoning, frequently validating incorrect student solutions (sycophancy) or providing overly direct answers that hinder learning. We introduce Hierarchical Pedagogical Oversight (HPO), a framework that adapts structured adversarial synthesis to educational assessment. Unlike cooperative multi-agent systems that often drift toward superficial consensus, HPO enforces a dialectical separation of concerns: specialist agents first distill dialogue context, which then grounds a moderated, five-act debate between opposing pedagogical critics. We evaluate this framework on the MRBench dataset of 1,214 middle-school mathematics dialogues. Our 8B-parameter model achieves a Macro F1 of 0.845, outperforming GPT-4o (0.812) by 3.3% while using 20 times fewer parameters. These results establish adversarial reasoning as a critical mechanism for deploying reliable, low-compute pedagogical oversight in resource-constrained environments.",
    "URL": "http://arxiv.org/abs/2512.22496v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.22508",
    "type": "report",
    "title": "Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals",
    "author": [
      {
        "family": "Susanto",
        "given": "Lucky"
      },
      {
        "family": "Pranawijayana",
        "given": "Anasta"
      },
      {
        "family": "Sukotjo",
        "given": "Cortino"
      },
      {
        "family": "Prasad",
        "given": "Soni"
      },
      {
        "family": "Wijaya",
        "given": "Derry"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          27
        ]
      ]
    },
    "abstract": "Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models' internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.",
    "URL": "http://arxiv.org/abs/2512.22508v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.23036",
    "type": "report",
    "title": "Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education",
    "author": [
      {
        "family": "Hooshyar",
        "given": "Danial"
      },
      {
        "family": "Yang",
        "given": "Yeongwook"
      },
      {
        "family": "≈†√≠≈ô",
        "given": "Gustav"
      },
      {
        "family": "K√§rkk√§inen",
        "given": "Tommi"
      },
      {
        "family": "H√§m√§l√§inen",
        "given": "Raija"
      },
      {
        "family": "Cukurova",
        "given": "Mutlu"
      },
      {
        "family": "Azevedo",
        "given": "Roger"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          28
        ]
      ]
    },
    "abstract": "The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\\% over the zero-shot baseline, it remains 6\\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.",
    "URL": "http://arxiv.org/abs/2512.23036v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.23053",
    "type": "report",
    "title": "LLteacher: A Tool for the Integration of Generative AI into Statistics Assignments",
    "author": [
      {
        "family": "Furfaro",
        "given": "Emanuela"
      },
      {
        "family": "Mosciatti",
        "given": "Simone"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          28
        ]
      ]
    },
    "abstract": "As generative AI becomes increasingly embedded in everyday life, the thoughtful and intentional integration of AI-based tools into statistics education has become essential. We address this need with a focus on homework assignments and we propose the use of LLMs as a companion to complete homework by developing an open-source tool named LLteacher. This LLM-based tool preserves learning processes and it guides students to engage with AI in ways that support their learning, while ensuring alignment with course content and equitable access. We illustrate LLteacher's design and functionality with examples from an undergraduate Statistical Computing course in R, showing how it supports two distinct pedagogical goals: recalling prior knowledge and discovering new concepts. While this is an initial version, LLteacher demonstrates one possible pathway for integrating generative AI into statistics courses, with strong potential for adaptation to other types of classes and assignments.",
    "URL": "http://arxiv.org/abs/2512.23053v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.23136",
    "type": "report",
    "title": "Understanding EFL Learners' Code-Switching and Teachers' Pedagogical Approaches in LLM-Supported Speaking Practice",
    "author": [
      {
        "family": "Park",
        "given": "Junyeong"
      },
      {
        "family": "Han",
        "given": "Jieun"
      },
      {
        "family": "Park",
        "given": "Yeon Su"
      },
      {
        "family": "Lee",
        "given": "Youngbin"
      },
      {
        "family": "Kim",
        "given": "Suin"
      },
      {
        "family": "Kim",
        "given": "Juho"
      },
      {
        "family": "Oh",
        "given": "Alice"
      },
      {
        "family": "Ahn",
        "given": "So-Yeon"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          29
        ]
      ]
    },
    "abstract": "For English as a Foreign Language (EFL) learners, code-switching (CSW), or alternating between their native language and the target language (English), can lower anxiety and ease communication barriers. Large language models (LLMs), with their multilingual abilities, offer new opportunities to support CSW in speaking practice. Yet, the pedagogical design of LLM-based tutors remains underexplored. To this end, we conducted a six-week study of LLM-mediated speaking practice with 20 Korean EFL learners, alongside a qualitative study with nine English teachers who designed and refined responses to learner CSW. Findings show that learners used CSW not only to bridge lexical gaps but also to express cultural and emotional nuance, prompting teachers to employ selective interventions and dynamic scaffolding strategies. We conclude with design implications for bilingual LLM-powered tutors that leverage teachers' expertise to transform CSW into meaningful learning opportunities.",
    "URL": "http://arxiv.org/abs/2512.23136v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.23587",
    "type": "report",
    "title": "Can AI Recognize Its Own Reflection? Self-Detection Performance of LLMs in Computing Education",
    "author": [
      {
        "family": "Burger",
        "given": "Christopher"
      },
      {
        "family": "Talley",
        "given": "Karmece"
      },
      {
        "family": "Trotter",
        "given": "Christina"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          29
        ]
      ]
    },
    "abstract": "The rapid advancement of Large Language Models (LLMs) presents a significant challenge to academic integrity within computing education. As educators seek reliable detection methods, this paper evaluates the capacity of three prominent LLMs (GPT-4, Claude, and Gemini) to identify AI-generated text in computing-specific contexts. We test their performance under both standard and 'deceptive' prompt conditions, where the models were instructed to evade detection. Our findings reveal a significant instability: while default AI-generated text was easily identified, all models struggled to correctly classify human-written work (with error rates up to 32%). Furthermore, the models were highly susceptible to deceptive prompts, with Gemini's output completely fooling GPT-4. Given that simple prompt alterations significantly degrade detection efficacy, our results demonstrate that these LLMs are currently too unreliable for making high-stakes academic misconduct judgments.",
    "URL": "http://arxiv.org/abs/2512.23587v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.23601",
    "type": "report",
    "title": "Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation",
    "author": [
      {
        "family": "Nguyen",
        "given": "Manh Hung"
      },
      {
        "family": "Singla",
        "given": "Adish"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          29
        ]
      ]
    },
    "abstract": "Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.",
    "URL": "http://arxiv.org/abs/2512.23601v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.23633",
    "type": "report",
    "title": "AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms",
    "author": [
      {
        "family": "Team",
        "given": "LearnLM"
      },
      {
        "literal": "Eedi"
      },
      {
        "literal": ":"
      },
      {
        "family": "Wang",
        "given": "Albert"
      },
      {
        "family": "Rysbek",
        "given": "Aliya"
      },
      {
        "family": "Huber",
        "given": "Andrea"
      },
      {
        "family": "Nambiar",
        "given": "Anjali"
      },
      {
        "family": "Kenolty",
        "given": "Anna"
      },
      {
        "family": "Caulfield",
        "given": "Ben"
      },
      {
        "family": "Lilley-Draper",
        "given": "Beth"
      },
      {
        "family": "Groot",
        "given": "Bibi"
      },
      {
        "family": "Veprek",
        "given": "Brian"
      },
      {
        "family": "Burdett",
        "given": "Chelsea"
      },
      {
        "family": "Willis",
        "given": "Claire"
      },
      {
        "family": "Barton",
        "given": "Craig"
      },
      {
        "family": "Smith",
        "given": "Digory"
      },
      {
        "family": "Mu",
        "given": "George"
      },
      {
        "family": "Walters",
        "given": "Harriet"
      },
      {
        "family": "Jurenka",
        "given": "Irina"
      },
      {
        "family": "Hulls",
        "given": "Iris"
      },
      {
        "family": "Stalley-Moores",
        "given": "James"
      },
      {
        "family": "Caton",
        "given": "Jonathan"
      },
      {
        "family": "Wilkowski",
        "given": "Julia"
      },
      {
        "family": "Alarakyia",
        "given": "Kaiz"
      },
      {
        "family": "McKee",
        "given": "Kevin R."
      },
      {
        "family": "McCafferty",
        "given": "Liam"
      },
      {
        "family": "Dalton",
        "given": "Lucy"
      },
      {
        "family": "Kunesch",
        "given": "Markus"
      },
      {
        "family": "Malubay",
        "given": "Pauline"
      },
      {
        "family": "Kidson",
        "given": "Rachel"
      },
      {
        "family": "Wells",
        "given": "Rich"
      },
      {
        "family": "Wheeler",
        "given": "Sam"
      },
      {
        "family": "Wiltberger",
        "given": "Sara"
      },
      {
        "family": "Mohamed",
        "given": "Shakir"
      },
      {
        "family": "Woodhead",
        "given": "Simon"
      },
      {
        "family": "Braz√£o",
        "given": "Vasco"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          29
        ]
      ]
    },
    "abstract": "One-to-one tutoring is widely considered the gold standard for personalized education, yet it remains prohibitively expensive to scale. To evaluate whether generative AI might help expand access to this resource, we conducted an exploratory randomized controlled trial (RCT) with $N = 165$ students across five UK secondary schools. We integrated LearnLM -- a generative AI model fine-tuned for pedagogy -- into chat-based tutoring sessions on the Eedi mathematics platform. In the RCT, expert tutors directly supervised LearnLM, with the remit to revise each message it drafted until they would be satisfied sending it themselves. LearnLM proved to be a reliable source of pedagogical instruction, with supervising tutors approving 76.4% of its drafted messages making zero or minimal edits (i.e., changing only one or two characters). This translated into effective tutoring support: students guided by LearnLM performed at least as well as students chatting with human tutors on each learning outcome we measured. In fact, students who received support from LearnLM were 5.5 percentage points more likely to solve novel problems on subsequent topics (with a success rate of 66.2%) than those who received tutoring from human tutors alone (rate of 60.7%). In interviews, tutors highlighted LearnLM's strength at drafting Socratic questions that encouraged deeper reflection from students, with multiple tutors even reporting that they learned new pedagogical practices from the model. Overall, our results suggest that pedagogically fine-tuned AI tutoring systems may play a promising role in delivering effective, individualized learning support at scale.",
    "URL": "http://arxiv.org/abs/2512.23633v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.23982",
    "type": "report",
    "title": "Coding With AI: From a Reflection on Industrial Practices to Future Computer Science and Software Engineering Education",
    "author": [
      {
        "family": "Chang",
        "given": "Hung-Fu"
      },
      {
        "family": "Shirazi",
        "given": "MohammadShokrolah"
      },
      {
        "family": "Cao",
        "given": "Lizhou"
      },
      {
        "family": "Mobasser",
        "given": "Supannika Koolmanojwong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          30
        ]
      ]
    },
    "abstract": "Recent advances in large language models (LLMs) have introduced new paradigms in software development, including vibe coding, AI-assisted coding, and agentic coding, fundamentally reshaping how software is designed, implemented, and maintained. Prior research has primarily examined AI-based coding at the individual level or in educational settings, leaving industrial practitioners' perspectives underexplored. This paper addresses this gap by investigating how LLM coding tools are used in professional practice, the associated concerns and risks, and the resulting transformations in development workflows, with particular attention to implications for computing education. We conducted a qualitative analysis of 57 curated YouTube videos published between late 2024 and 2025, capturing reflections and experiences shared by practitioners. Following a filtering and quality assessment process, the selected sources were analyzed to compare LLM-based and traditional programming, identify emerging risks, and characterize evolving workflows. Our findings reveal definitions of AI-based coding practices, notable productivity gains, and lowered barriers to entry. Practitioners also report a shift in development bottlenecks toward code review and concerns regarding code quality, maintainability, security vulnerabilities, ethical issues, erosion of foundational problem-solving skills, and insufficient preparation of entry-level engineers. Building on these insights, we discuss implications for computer science and software engineering education and argue for curricular shifts toward problem-solving, architectural thinking, code review, and early project-based learning that integrates LLM tools. This study offers an industry-grounded perspective on AI-based coding and provides guidance for aligning educational practices with rapidly evolving professional realities.",
    "URL": "http://arxiv.org/abs/2512.23982v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.24618",
    "type": "report",
    "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
    "author": [
      {
        "family": "Lu",
        "given": "Junru"
      },
      {
        "family": "Qin",
        "given": "Jiarui"
      },
      {
        "family": "Qiao",
        "given": "Lingfeng"
      },
      {
        "family": "Li",
        "given": "Yinghui"
      },
      {
        "family": "Dai",
        "given": "Xinyi"
      },
      {
        "family": "Ke",
        "given": "Bo"
      },
      {
        "family": "He",
        "given": "Jianfeng"
      },
      {
        "family": "Qiao",
        "given": "Ruizhi"
      },
      {
        "family": "Yin",
        "given": "Di"
      },
      {
        "family": "Sun",
        "given": "Xing"
      },
      {
        "family": "Wu",
        "given": "Yunsheng"
      },
      {
        "family": "Liu",
        "given": "Yinsong"
      },
      {
        "family": "Liu",
        "given": "Shuangyin"
      },
      {
        "family": "Tang",
        "given": "Mingkong"
      },
      {
        "family": "Lin",
        "given": "Haodong"
      },
      {
        "family": "Kuang",
        "given": "Jiayi"
      },
      {
        "family": "Meng",
        "given": "Fanxu"
      },
      {
        "family": "Tang",
        "given": "Xiaojuan"
      },
      {
        "family": "Xi",
        "given": "Yunjia"
      },
      {
        "family": "Huang",
        "given": "Junjie"
      },
      {
        "family": "Yang",
        "given": "Haotong"
      },
      {
        "family": "Shen",
        "given": "Zhenyi"
      },
      {
        "family": "Li",
        "given": "Yangning"
      },
      {
        "family": "Zhang",
        "given": "Qianwen"
      },
      {
        "family": "Yu",
        "given": "Yifei"
      },
      {
        "family": "An",
        "given": "Siyu"
      },
      {
        "family": "Dong",
        "given": "Junnan"
      },
      {
        "family": "Wang",
        "given": "Qiufeng"
      },
      {
        "family": "Wang",
        "given": "Jie"
      },
      {
        "family": "Chen",
        "given": "Keyu"
      },
      {
        "family": "Wen",
        "given": "Wei"
      },
      {
        "family": "Guo",
        "given": "Taian"
      },
      {
        "family": "Shen",
        "given": "Zhifeng"
      },
      {
        "family": "Yu",
        "given": "Daohai"
      },
      {
        "family": "Li",
        "given": "Jiahao"
      },
      {
        "family": "Li",
        "given": "Ke"
      },
      {
        "family": "Li",
        "given": "Zongyi"
      },
      {
        "family": "Tan",
        "given": "Xiaoyu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          31
        ]
      ]
    },
    "abstract": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.",
    "URL": "http://arxiv.org/abs/2512.24618v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.24969",
    "type": "report",
    "title": "Large language models and the entropy of English",
    "author": [
      {
        "family": "Scheibner",
        "given": "Colin"
      },
      {
        "family": "Smith",
        "given": "Lindsay M."
      },
      {
        "family": "Bialek",
        "given": "William"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          31
        ]
      ]
    },
    "abstract": "We use large language models (LLMs) to uncover long-ranged structure in English texts from a variety of sources. The conditional entropy or code length in many cases continues to decrease with context length at least to $N\\sim 10^4$ characters, implying that there are direct dependencies or interactions across these distances. A corollary is that there are small but significant correlations between characters at these separations, as we show from the data independent of models. The distribution of code lengths reveals an emergent certainty about an increasing fraction of characters at large $N$. Over the course of model training, we observe different dynamics at long and short context lengths, suggesting that long-ranged structure is learned only gradually. Our results constrain efforts to build statistical physics models of LLMs or language itself.",
    "URL": "http://arxiv.org/abs/2512.24969v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.00493",
    "type": "report",
    "title": "Measuring University Students Satisfaction with Traditional Search Engines and Generative AI Tools as Information Sources",
    "author": [
      {
        "family": "Lund",
        "given": "Brady D."
      },
      {
        "family": "Warren",
        "given": "Scott J."
      },
      {
        "family": "Teel",
        "given": "Zoe A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          1
        ]
      ]
    },
    "abstract": "This study examines university students levels of satisfaction with generative artificial intelligence (AI) tools and traditional search engines as academic information sources. An electronic survey was distributed to students at U.S. universities in late fall 2025, with 236 valid responses received. In addition to demographic information about respondents, frequency of use and levels of satisfaction with both generative AI and traditional search engines were measured. Principal components analysis identified distinct constructs of satisfaction for each information source, while k-means cluster analysis revealed two primary student groups: those highly satisfied with search engines but dissatisfied with AI, and those moderately to highly satisfied with both. Regression analysis showed that frequency of use strongly predicts satisfaction, with international and undergraduate students reporting significantly higher satisfaction with AI tools than domestic and graduate students. Students generally expressed higher levels of satisfaction with traditional search engines over generative AI tools. Those who did prefer AI tools appear to see them more as a complementary source of information rather than a replacement for other sources. These findings stress evolving patterns of student information seeking and use behavior and offer meaningful insights for evaluating and integrating both traditional and AI-driven information sources within higher education.",
    "URL": "http://arxiv.org/abs/2601.00493v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.00497",
    "type": "report",
    "title": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications",
    "author": [
      {
        "family": "Sorokin",
        "given": "Lev"
      },
      {
        "family": "Vasilev",
        "given": "Ivan"
      },
      {
        "family": "Friedl",
        "given": "Ken E."
      },
      {
        "family": "Stocco",
        "given": "Andrea"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          1
        ]
      ]
    },
    "abstract": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.",
    "URL": "http://arxiv.org/abs/2601.00497v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.00570",
    "type": "report",
    "title": "User Perceptions of an LLM-Based Chatbot for Cognitive Reappraisal of Stress: Feasibility Study",
    "author": [
      {
        "family": "Bhattacharjee",
        "given": "Ananya"
      },
      {
        "family": "Suh",
        "given": "Jina"
      },
      {
        "family": "Chandra",
        "given": "Mohit"
      },
      {
        "family": "Hernandez",
        "given": "Javier"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          2
        ]
      ]
    },
    "abstract": "Cognitive reappraisal is a well-studied emotion regulation strategy that helps individuals reinterpret stressful situations to reduce their impact. Many digital mental health tools struggle to support this process because rigid scripts fail to accommodate how users naturally describe stressors. This study examined the feasibility of an LLM-based single-session intervention (SSI) for workplace stress reappraisal. We assessed short-term changes in stress-related outcomes and examined design tensions during use. We conducted a feasibility study with 100 employees at a large technology company who completed a structured cognitive reappraisal session delivered by a GPT-4o-based chatbot. Pre-post measures included perceived stress intensity, stress mindset, perceived demand, and perceived resources. These outcomes were analyzed using paired Wilcoxon signed-rank tests with correction for multiple comparisons. We also examined sentiment and stress trajectories across conversation quartiles using two RoBERTa-based classifiers and an LLM-based stress rater. Open-ended responses were analyzed using thematic analysis. Results showed significant reductions in perceived stress intensity and significant improvements in stress mindset. Changes in perceived resources and perceived demand trended in expected directions but were not statistically significant. Automated analyses indicated consistent declines in negative sentiment and stress over the course of the interaction. Qualitative findings suggested that participants valued the structured prompts for organizing thoughts, gaining perspective, and feeling acknowledged. Participants also reported tensions around scriptedness, preferred interaction length, and reactions to AI-driven empathy. These findings highlight both the promise and the design constraints of integrating LLMs into DMH interventions for workplace settings.",
    "URL": "http://arxiv.org/abs/2601.00570v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.00730",
    "type": "report",
    "title": "Grading Handwritten Engineering Exams with Multimodal Large Language Models",
    "author": [
      {
        "family": "Per≈°",
        "given": "Janez"
      },
      {
        "family": "Muhoviƒç",
        "given": "Jon"
      },
      {
        "family": "Ko≈°ir",
        "given": "Andrej"
      },
      {
        "family": "Murovec",
        "given": "Bo≈°tjan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          2
        ]
      ]
    },
    "abstract": "Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\\approx$17% at $D_{\\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.",
    "URL": "http://arxiv.org/abs/2601.00730v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.00943",
    "type": "report",
    "title": "PhyEduVideo: A Benchmark for Evaluating Text-to-Video Models for Physics Education",
    "author": [
      {
        "family": "M",
        "given": "Megha Mariam K."
      },
      {
        "family": "Arun",
        "given": "Aditya"
      },
      {
        "family": "Laskar",
        "given": "Zakaria"
      },
      {
        "family": "Jawahar",
        "given": "C. V."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          2
        ]
      ]
    },
    "abstract": "Generative AI models, particularly Text-to-Video (T2V) systems, offer a promising avenue for transforming science education by automating the creation of engaging and intuitive visual explanations. In this work, we take a first step toward evaluating their potential in physics education by introducing a dedicated benchmark for explanatory video generation. The benchmark is designed to assess how well T2V models can convey core physics concepts through visual illustrations. Each physics concept in our benchmark is decomposed into granular teaching points, with each point accompanied by a carefully crafted prompt intended for visual explanation of the teaching point. T2V models are evaluated on their ability to generate accurate videos in response to these prompts. Our aim is to systematically explore the feasibility of using T2V models to generate high-quality, curriculum-aligned educational content-paving the way toward scalable, accessible, and personalized learning experiences powered by AI. Our evaluation reveals that current models produce visually coherent videos with smooth motion and minimal flickering, yet their conceptual accuracy is less reliable. Performance in areas such as mechanics, fluids, and optics is encouraging, but models struggle with electromagnetism and thermodynamics, where abstract interactions are harder to depict. These findings underscore the gap between visual quality and conceptual correctness in educational video generation. We hope this benchmark helps the community close that gap and move toward T2V systems that can deliver accurate, curriculum-aligned physics content at scale. The benchmark and accompanying codebase are publicly available at https://github.com/meghamariamkm/PhyEduVideo.",
    "URL": "http://arxiv.org/abs/2601.00943v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.01196",
    "type": "report",
    "title": "EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners",
    "author": [
      {
        "family": "Lu",
        "given": "Shenqi"
      },
      {
        "family": "Zhang",
        "given": "Liangwei"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          3
        ]
      ]
    },
    "abstract": "In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an important challenge in the rapid development of human-robot interaction and intelligent automation industries. This challenge hinders intuitive human control over complex robotic systems, limiting their educational and practical accessibility. To address this, we present the EduSim-LLM, an educational platform that integrates LLMs with robot simulation and constructs a language-drive control model that translates natural language instructions into executable robot behavior sequences in CoppeliaSim. We design two human-robot interaction models: direct control and autonomous control, conduct systematic simulations based on multiple language models, and evaluate multi-robot collaboration, motion planning, and manipulation capabilities. Experiential results show that LLMs can reliably convert natural language into structured robot actions; after applying prompt-engineering templates instruction-parsing accuracy improves significantly; as task complexity increases, overall accuracy rate exceeds 88.9% in the highest complexity tests.",
    "URL": "http://arxiv.org/abs/2601.01196v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.01366",
    "type": "report",
    "title": "KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models",
    "author": [
      {
        "family": "Liu",
        "given": "Zixian"
      },
      {
        "family": "Liu",
        "given": "Sihao"
      },
      {
        "family": "Zhao",
        "given": "Yuqi"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          4
        ]
      ]
    },
    "abstract": "With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.",
    "URL": "http://arxiv.org/abs/2601.01366v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.01684",
    "type": "report",
    "title": "LACONIC: Dense-Level Effectiveness for Scalable Sparse Retrieval via a Two-Phase Training Curriculum",
    "author": [
      {
        "family": "Xu",
        "given": "Zhichao"
      },
      {
        "family": "Zhuang",
        "given": "Shengyao"
      },
      {
        "family": "Zhang",
        "given": "Crystina"
      },
      {
        "family": "Ma",
        "given": "Xueguang"
      },
      {
        "family": "Tian",
        "given": "Yijun"
      },
      {
        "family": "Mehta",
        "given": "Maitrey"
      },
      {
        "family": "Lin",
        "given": "Jimmy"
      },
      {
        "family": "Srikumar",
        "given": "Vivek"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          4
        ]
      ]
    },
    "abstract": "While dense retrieval models have become the standard for state-of-the-art information retrieval, their deployment is often constrained by high memory requirements and reliance on GPU accelerators for vector similarity search. Learned sparse retrieval offers a compelling alternative by enabling efficient search via inverted indices, yet it has historically received less attention than dense approaches. In this report, we introduce LACONIC, a family of learned sparse retrievers based on the Llama-3 architecture (1B, 3B, and 8B). We propose a streamlined two-phase training curriculum consisting of (1) weakly supervised pre-finetuning to adapt causal LLMs for bidirectional contextualization and (2) high-signal finetuning using curated hard negatives. Our results demonstrate that LACONIC effectively bridges the performance gap with dense models: the 8B variant achieves a state-of-the-art 60.2 nDCG on the MTEB Retrieval benchmark, ranking 15th on the leaderboard as of January 1, 2026, while utilizing 71\\% less index memory than an equivalent dense model. By delivering high retrieval effectiveness on commodity CPU hardware with a fraction of the compute budget required by competing models, LACONIC provides a scalable and efficient solution for real-world search applications.",
    "URL": "http://arxiv.org/abs/2601.01684v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.02404",
    "type": "report",
    "title": "PCEval: A Benchmark for Evaluating Physical Computing Capabilities of Large Language Models",
    "author": [
      {
        "family": "Song",
        "given": "Inpyo"
      },
      {
        "family": "Jeon",
        "given": "Eunji"
      },
      {
        "family": "Lee",
        "given": "Jangwon"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          31
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including software development, education, and technical assistance. Among these, software development is one of the key areas where LLMs are increasingly adopted. However, when hardware constraints are considered-for instance, in physical computing, where software must interact with and control physical hardware -their effectiveness has not been fully explored. To address this gap, we introduce \\textsc{PCEval} (Physical Computing Evaluation), the first benchmark in physical computing that enables a fully automatic evaluation of the capabilities of LLM in both the logical and physical aspects of the projects, without requiring human assessment. Our evaluation framework assesses LLMs in generating circuits and producing compatible code across varying levels of project complexity. Through comprehensive testing of 13 leading models, \\textsc{PCEval} provides the first reproducible and automatically validated empirical assessment of LLMs' ability to reason about fundamental hardware implementation constraints within a simulation environment. Our findings reveal that while LLMs perform well in code generation and logical circuit design, they struggle significantly with physical breadboard layout creation, particularly in managing proper pin connections and avoiding circuit errors. \\textsc{PCEval} advances our understanding of AI assistance in hardware-dependent computing environments and establishes a foundation for developing more effective tools to support physical computing education.",
    "URL": "http://arxiv.org/abs/2601.02404v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.02554",
    "type": "report",
    "title": "AI-exposed jobs deteriorated before ChatGPT",
    "author": [
      {
        "family": "Frank",
        "given": "Morgan R."
      },
      {
        "family": "Sabet",
        "given": "Alireza Javadian"
      },
      {
        "family": "Simon",
        "given": "Lisa"
      },
      {
        "family": "Bana",
        "given": "Sarah H."
      },
      {
        "family": "Yu",
        "given": "Renzhe"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          5
        ]
      ]
    },
    "abstract": "Public debate links worsening job prospects for AI-exposed occupations to the release of ChatGPT in late 2022. Using monthly U.S. unemployment insurance records, we measure occupation- and location-specific unemployment risk and find that risk rose in AI-exposed occupations beginning in early 2022, months before ChatGPT. Analyzing millions of LinkedIn profiles, we show that graduate cohorts from 2021 onward entered AI-exposed jobs at lower rates than earlier cohorts, with gaps opening before late 2022. Finally, from millions of university syllabi, we find that graduates taking more AI-exposed curricula had higher first-job pay and shorter job searches after ChatGPT. Together, these results point to forces pre-dating generative AI and to the ongoing value of LLM-relevant education.",
    "URL": "http://arxiv.org/abs/2601.02554v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.02902",
    "type": "report",
    "title": "Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning",
    "author": [
      {
        "family": "Zhang",
        "given": "Xinglang"
      },
      {
        "family": "Zhang",
        "given": "Yunyao"
      },
      {
        "family": "Chen",
        "given": "ZeLiang"
      },
      {
        "family": "Yu",
        "given": "Junqing"
      },
      {
        "family": "Yang",
        "given": "Wei"
      },
      {
        "family": "Song",
        "given": "Zikai"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          6
        ]
      ]
    },
    "abstract": "Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.",
    "URL": "http://arxiv.org/abs/2601.02902v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.03285",
    "type": "report",
    "title": "Feedback Indices to Evaluate LLM Responses to Rebuttals for Multiple Choice Type Questions",
    "author": [
      {
        "family": "Dunlap",
        "given": "Justin C."
      },
      {
        "family": "Parent",
        "given": "Anne-Simone"
      },
      {
        "family": "Widenhorn",
        "given": "Ralf"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          2
        ]
      ]
    },
    "abstract": "We present a systematic framework of indices designed to characterize Large Language Model (LLM) responses when challenged with rebuttals during a chat. Assessing how LLMs respond to user dissent is crucial for understanding their reliability and behavior patterns, yet the complexity of human-LLM interactions makes systematic evaluation challenging. Our approach employs a fictitious-response rebuttal method that quantifies LLM behavior when presented with multiple-choice questions followed by deliberate challenges to their fictitious previous response. The indices are specifically designed to detect and measure what could be characterized as sycophantic behavior (excessive agreement with user challenges) or stubborn responses (rigid adherence to the fictitious response in the chat history) from LLMs. These metrics allow investigation of the relationships between sycophancy, stubbornness, and the model's actual mastery of the subject matter. We demonstrate the utility of these indices using two physics problems as test scenarios with various OpenAI models. The framework is intentionally generalizable to any multiple-choice format question, including on topics without universally accepted correct answers. Our results reveal measurable differences across OpenAI model generations, with trends indicating that newer models and those employing greater \"Reasoning Effort\" exhibit reduced sycophantic behavior. The FR pairing method combined with our proposed indices provides a practical, adaptable toolkit for systematically comparing LLM dialogue behaviors across different models and contexts.",
    "URL": "http://arxiv.org/abs/2601.03285v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.03432",
    "type": "report",
    "title": "CodeEval: A pedagogical approach for targeted evaluation of code-trained Large Language Models",
    "author": [
      {
        "family": "Brahman",
        "given": "Danny"
      },
      {
        "family": "Mahoor",
        "given": "Mohammad"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          6
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) are predominantly assessed based on their common sense reasoning, language comprehension, and logical reasoning abilities. While models trained in specialized domains like mathematics or coding have demonstrated remarkable advancements in logical reasoning, there remains a significant gap in evaluating their code generation capabilities. Existing benchmark datasets fall short in pinpointing specific strengths and weaknesses, impeding targeted enhancements in models' reasoning abilities to synthesize code. To bridge this gap, our paper introduces an innovative, pedagogical benchmarking method that mirrors the evaluation processes encountered in academic programming courses. We introduce CodeEval, a multi-dimensional benchmark dataset designed to rigorously evaluate LLMs across 24 distinct aspects of Python programming. The dataset covers three proficiency levels - beginner, intermediate, and advanced - and includes both class-based and function-based problem types with detailed problem specifications and comprehensive test suites. To facilitate widespread adoption, we also developed RunCodeEval, an open-source execution framework that provides researchers with a ready-to-use evaluation pipeline for CodeEval. RunCodeEval handles test execution, context setup, and metrics generation, enabling researchers to quickly obtain detailed insights into model strengths and weaknesses across complexity levels, problem types, and programming categories. This combination enables targeted evaluation and guides improvements in LLMs' programming proficiencies.",
    "URL": "http://arxiv.org/abs/2601.03432v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.03458",
    "type": "report",
    "title": "Automated Feedback Generation for Undergraduate Mathematics: Development and Evaluation of an AI Teaching Assistant",
    "author": [
      {
        "family": "Gohr",
        "given": "Aron"
      },
      {
        "family": "Lawn",
        "given": "Marie-Amelie"
      },
      {
        "family": "Gao",
        "given": "Kevin"
      },
      {
        "family": "Serjeant",
        "given": "Inigo"
      },
      {
        "family": "Heslip",
        "given": "Stephen"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          6
        ]
      ]
    },
    "abstract": "Intelligent tutoring systems have long enabled automated immediate feedback on student work when it is presented in a tightly structured format and when problems are very constrained, but reliably assessing free-form mathematical reasoning remains challenging.\n  We present a system that processes free-form natural language input, handles a wide range of edge cases, and comments competently not only on the technical correctness of submitted proofs, but also on style and presentation issues. We discuss the advantages and disadvantages of various approaches to the evaluation of such a system, and show that by the metrics we evaluate, the quality of the feedback generated is comparable to that produced by human experts when assessing early undergraduate homework. We stress-test our system with a small set of more advanced and unusual questions, and report both significant gaps and encouraging successes in that more challenging setting.\n  Our system uses large language models in a modular workflow. The workflow configuration is human-readable and editable without programming knowledge, and allows some intermediate steps to be precomputed or injected by the instructor.\n  A version of our tool is deployed on the Imperial mathematics homework platform Lambdafeedback. We report also on the integration of our tool into this platform.",
    "URL": "http://arxiv.org/abs/2601.03458v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.03645",
    "type": "report",
    "title": "LLM-MC-Affect: LLM-Based Monte Carlo Modeling of Affective Trajectories and Latent Ambiguity for Interpersonal Dynamic Insight",
    "author": [
      {
        "family": "Lin",
        "given": "Yu-Zheng"
      },
      {
        "family": "Shih",
        "given": "Bono Po-Jen"
      },
      {
        "family": "Encinas",
        "given": "John Paul Martin"
      },
      {
        "family": "Achom",
        "given": "Elizabeth Victoria Abraham"
      },
      {
        "family": "Patel",
        "given": "Karan Himanshu"
      },
      {
        "family": "Pacheco",
        "given": "Jesus Horacio"
      },
      {
        "family": "Shao",
        "given": "Sicong"
      },
      {
        "family": "Dass",
        "given": "Jyotikrishna"
      },
      {
        "family": "Salehi",
        "given": "Soheil"
      },
      {
        "family": "Satam",
        "given": "Pratik"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "Emotional coordination is a core property of human interaction that shapes how relational meaning is constructed in real time. While text-based affect inference has become increasingly feasible, prior approaches often treat sentiment as a deterministic point estimate for individual speakers, failing to capture the inherent subjectivity, latent ambiguity, and sequential coupling found in mutual exchanges. We introduce LLM-MC-Affect, a probabilistic framework that characterizes emotion not as a static label, but as a continuous latent probability distribution defined over an affective space. By leveraging stochastic LLM decoding and Monte Carlo estimation, the methodology approximates these distributions to derive high-fidelity sentiment trajectories that explicitly quantify both central affective tendencies and perceptual ambiguity. These trajectories enable a structured analysis of interpersonal coupling through sequential cross-correlation and slope-based indicators, identifying leading or lagging influences between interlocutors. To validate the interpretive capacity of this approach, we utilize teacher-student instructional dialogues as a representative case study, where our quantitative indicators successfully distill high-level interaction insights such as effective scaffolding. This work establishes a scalable and deployable pathway for understanding interpersonal dynamics, offering a generalizable solution that extends beyond education to broader social and behavioral research.",
    "URL": "http://arxiv.org/abs/2601.03645v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.03725",
    "type": "report",
    "title": "EDCO: Dynamic Curriculum Orchestration for Domain-specific Large Language Model Fine-tuning",
    "author": [
      {
        "family": "Pang",
        "given": "Jing-Cheng"
      },
      {
        "family": "Sun",
        "given": "Liu"
      },
      {
        "family": "Zhou",
        "given": "Chang"
      },
      {
        "family": "Tang",
        "given": "Xian"
      },
      {
        "family": "Ma",
        "given": "Haichuan"
      },
      {
        "family": "Jiang",
        "given": "Kun"
      },
      {
        "family": "Wang",
        "given": "Jianlong"
      },
      {
        "family": "Zhang",
        "given": "Kai"
      },
      {
        "family": "Wu",
        "given": "Sijie"
      },
      {
        "family": "Cai",
        "given": "Haoran"
      },
      {
        "family": "Wu",
        "given": "Chenwei"
      },
      {
        "family": "Li",
        "given": "Xubin"
      },
      {
        "family": "Chen",
        "given": "Xin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "Domain-specific large language models (LLMs), typically developed by fine-tuning a pre-trained general-purpose LLM on specialized datasets, represent a significant advancement in applied AI. A common strategy in LLM fine-tuning is curriculum learning, which pre-orders training samples based on metrics like difficulty to improve learning efficiency compared to a random sampling strategy. However, most existing methods for LLM fine-tuning rely on a static curriculum, designed prior to training, which lacks adaptability to the model's evolving needs during fine-tuning. To address this, we propose EDCO, a novel framework based on two key concepts: inference entropy and dynamic curriculum orchestration. Inspired by recent findings that maintaining high answer entropy benefits long-term reasoning gains, EDCO prioritizes samples with high inference entropy in a continuously adapted curriculum. EDCO integrates three core components: an efficient entropy estimator that uses prefix tokens to approximate full-sequence entropy, an entropy-based curriculum generator that selects data points with the highest inference entropy, and an LLM trainer that optimizes the model on the selected curriculum. Comprehensive experiments in communication, medicine and law domains, EDCO outperforms traditional curriculum strategies for fine-tuning Qwen3-4B and Llama3.2-3B models under supervised and reinforcement learning settings. Furthermore, the proposed efficient entropy estimation reduces computational time by 83.5% while maintaining high accuracy.",
    "URL": "http://arxiv.org/abs/2601.03725v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.03858",
    "type": "report",
    "title": "What Does Loss Optimization Actually Teach, If Anything? Knowledge Dynamics in Continual Pre-training of LLMs",
    "author": [
      {
        "family": "Mousavi",
        "given": "Seyed Mahed"
      },
      {
        "family": "Alghisi",
        "given": "Simone"
      },
      {
        "family": "Riccardi",
        "given": "Giuseppe"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "Continual Pre-Training (CPT) is widely used for acquiring and updating factual knowledge in LLMs. This practice treats loss as a proxy for knowledge learning, while offering no grounding into how it changes during training. We study CPT as a knowledge learning process rather than a solely optimization problem. We construct a controlled, distribution-matched benchmark of factual documents and interleave diagnostic probes directly into the CPT loop, enabling epoch-level measurement of knowledge acquisition dynamics and changes in Out-Of-Domain (OOD) general skills (e.g., math). We further analyze how CPT reshapes knowledge circuits during training. Across three instruction-tuned LLMs and multiple CPT strategies, optimization and learning systematically diverge as loss decreases monotonically while factual learning is unstable and non-monotonic. Acquired facts are rarely consolidated, learning is strongly conditioned on prior exposure, and OOD performance degrades from early epochs. Circuit analysis reveals rapid reconfiguration of knowledge pathways across epochs, providing an explanation for narrow acquisition windows and systematic forgetting. These results show that loss optimization is misaligned with learning progress in CPT and motivate evaluation of stopping criteria based on task-level learning dynamics.",
    "URL": "http://arxiv.org/abs/2601.03858v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.03880",
    "type": "report",
    "title": "Women Worry, Men Adopt: How Gendered Perceptions Shape the Use of Generative AI",
    "author": [
      {
        "family": "Stephany",
        "given": "Fabian"
      },
      {
        "family": "Duszynski",
        "given": "Jedrzej"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "Generative artificial intelligence (GenAI) is diffusing rapidly, yet its adoption is strikingly unequal. Using nationally representative UK survey data from 2023 to 2024, we show that women adopt GenAI substantially less often than men because they perceive its societal risks differently. We construct a composite index capturing concerns about mental health, privacy, climate impact, and labor market disruption. This index explains between 9 and 18 percent of the variation in GenAI adoption and ranks among the strongest predictors for women across all age groups, surpassing digital literacy and education for young women. Intersectional analyses show that the largest disparities arise among younger, digitally fluent individuals with high societal risk concerns, where gender gaps in personal use exceed 45 percentage points. Using a synthetic twin panel design, we show that increased optimism about AI's societal impact raises GenAI use among young women from 13 percent to 33 percent, substantially narrowing the gender divide. These findings indicate that gendered perceptions of AI's social and ethical consequences, rather than access or capability, are the primary drivers of unequal GenAI adoption, with implications for productivity, skill formation, and economic inequality in an AI enabled economy.",
    "URL": "http://arxiv.org/abs/2601.03880v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04025",
    "type": "report",
    "title": "Simulated Students in Tutoring Dialogues: Substance or Illusion?",
    "author": [
      {
        "family": "Scarlatos",
        "given": "Alexander"
      },
      {
        "family": "Lee",
        "given": "Jaewook"
      },
      {
        "family": "Woodhead",
        "given": "Simon"
      },
      {
        "family": "Lan",
        "given": "Andrew"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "Advances in large language models (LLMs) enable many new innovations in education. However, evaluating the effectiveness of new technology requires real students, which is time-consuming and hard to scale up. Therefore, many recent works on LLM-powered tutoring solutions have used simulated students for both training and evaluation, often via simple prompting. Surprisingly, little work has been done to ensure or even measure the quality of simulated students. In this work, we formally define the student simulation task, propose a set of evaluation metrics that span linguistic, behavioral, and cognitive aspects, and benchmark a wide range of student simulation methods on these metrics. We experiment on a real-world math tutoring dialogue dataset, where both automated and human evaluation results show that prompting strategies for student simulation perform poorly; supervised fine-tuning and preference optimization yield much better but still limited performance, motivating future work on this challenging task.",
    "URL": "http://arxiv.org/abs/2601.04025v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04219",
    "type": "report",
    "title": "AgentTutor: Empowering Personalized Learning with Multi-Turn Interactive Teaching in Intelligent Education Systems",
    "author": [
      {
        "family": "Liu",
        "given": "Yuxin"
      },
      {
        "family": "Song",
        "given": "Zeqing"
      },
      {
        "family": "Lou",
        "given": "Jiong"
      },
      {
        "family": "Wu",
        "given": "Chentao"
      },
      {
        "family": "Li",
        "given": "Jie"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          24
        ]
      ]
    },
    "abstract": "The rapid advancement of large-scale language models (LLMs) has shown their potential to transform intelligent education systems (IESs) through automated teaching and learning support applications. However, current IESs often rely on single-turn static question-answering, which fails to assess learners' cognitive levels, cannot adjust teaching strategies based on real-time feedback, and is limited to providing simple one-off responses. To address these issues, we introduce AgentTutor, a multi-turn interactive intelligent education system to empower personalized learning. It features an LLM-powered generative multi-agent system and a learner-specific personalized learning profile environment that dynamically optimizes and delivers teaching strategies based on learners' learning status, personalized goals, learning preferences, and multimodal study materials. It includes five key modules: curriculum decomposition, learner assessment, dynamic strategy, teaching reflection, and knowledge & experience memory. We conducted extensive experiments on multiple benchmark datasets, AgentTutor significantly enhances learners' performance while demonstrating strong effectiveness in multi-turn interactions and competitiveness in teaching quality among other baselines.",
    "URL": "http://arxiv.org/abs/2601.04219v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04225",
    "type": "report",
    "title": "Can Consumer Chatbots Reason? A Student-Led Field Experiment Embedded in an \"AI-for-All\" Undergraduate Course",
    "author": [
      {
        "family": "Shehu",
        "given": "Amarda"
      },
      {
        "family": "Ababu",
        "given": "Adonyas"
      },
      {
        "family": "Akbary",
        "given": "Asma"
      },
      {
        "family": "Allen",
        "given": "Griffin"
      },
      {
        "family": "Baig",
        "given": "Aroush"
      },
      {
        "family": "Battle",
        "given": "Tereana"
      },
      {
        "family": "Beall",
        "given": "Elias"
      },
      {
        "family": "Byrom",
        "given": "Christopher"
      },
      {
        "family": "Dean",
        "given": "Matt"
      },
      {
        "family": "Demarco",
        "given": "Kate"
      },
      {
        "family": "Douglass",
        "given": "Ethan"
      },
      {
        "family": "Granados",
        "given": "Luis"
      },
      {
        "family": "Hantush",
        "given": "Layla"
      },
      {
        "family": "Hay",
        "given": "Andy"
      },
      {
        "family": "Hay",
        "given": "Eleanor"
      },
      {
        "family": "Jackson",
        "given": "Caleb"
      },
      {
        "family": "Jang",
        "given": "Jaewon"
      },
      {
        "family": "Jones",
        "given": "Carter"
      },
      {
        "family": "Li",
        "given": "Quanyang"
      },
      {
        "family": "Lopez",
        "given": "Adrian"
      },
      {
        "family": "Massimo",
        "given": "Logan"
      },
      {
        "family": "McMullin",
        "given": "Garrett"
      },
      {
        "family": "Maldonado",
        "given": "Ariana Mendoza"
      },
      {
        "family": "Mirza",
        "given": "Eman"
      },
      {
        "family": "Muddasar",
        "given": "Hadiya"
      },
      {
        "family": "Nuwayhid",
        "given": "Sara"
      },
      {
        "family": "Pak",
        "given": "Brandon"
      },
      {
        "family": "Petty",
        "given": "Ashley"
      },
      {
        "family": "Rancourt",
        "given": "Dryden"
      },
      {
        "family": "Rodriguez",
        "given": "Lily"
      },
      {
        "family": "Rogers",
        "given": "Corbin"
      },
      {
        "family": "Schiek",
        "given": "Jacob"
      },
      {
        "family": "Seok",
        "given": "Taeseo"
      },
      {
        "family": "Sethi",
        "given": "Aarav"
      },
      {
        "family": "Vitela",
        "given": "Giovanni"
      },
      {
        "family": "Williams",
        "given": "Winston"
      },
      {
        "family": "Yetukuri",
        "given": "Jagan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          28
        ]
      ]
    },
    "abstract": "Claims about whether large language model (LLM) chatbots \"reason\" are typically debated using curated benchmarks and laboratory-style evaluation protocols. This paper offers a complementary perspective: a student-led field experiment embedded as a midterm project in UNIV 182 (AI4All) at George Mason University, a Mason Core course designed for undergraduates across disciplines with no expected prior STEM exposure. Student teams designed their own reasoning tasks, ran them on widely used consumer chatbots representative of current capabilities, and evaluated both (i) answer correctness and (ii) the validity of the chatbot's stated reasoning (for example, cases where an answer is correct but the explanation is not, or vice versa). Across eight teams that reported standardized scores, students contributed 80 original reasoning prompts spanning six categories: pattern completion, transformation rules, spatial/visual reasoning, quantitative reasoning, relational/logic reasoning, and analogical reasoning. These prompts yielded 320 model responses plus follow-up explanations. Aggregating team-level results, OpenAI GPT-5 and Claude 4.5 achieved the highest mean answer accuracy (86.2% and 83.8%), followed by Grok 4 (82.5%) and Perplexity (73.1%); explanation validity showed a similar ordering (81.2%, 80.0%, 77.5%, 66.2%). Qualitatively, teams converged on a consistent error signature: strong performance on short, structured math and pattern items but reduced reliability on spatial/visual reasoning and multi-step transformations, with frequent \"sound right but reason wrong\" explanations. The assignment's primary contribution is pedagogical: it operationalizes AI literacy as experimental practice (prompt design, measurement, rater disagreement, and interpretability/grounding) while producing a reusable, student-generated corpus of reasoning probes grounded in authentic end-user interaction.",
    "URL": "http://arxiv.org/abs/2601.04225v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04336",
    "type": "report",
    "title": "Pilot Study on Student Public Opinion Regarding GAI",
    "author": [
      {
        "family": "Lamberti",
        "given": "William Franz"
      },
      {
        "family": "Kim",
        "given": "Sunbin"
      },
      {
        "family": "Lawrence",
        "given": "Samantha Rose"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "The emergence of generative AI (GAI) has sparked diverse opinions regarding its appropriate use across various domains, including education. This pilot study investigates university students' perceptions of GAI in higher education classrooms, aiming to lay the groundwork for understanding these attitudes. With a participation rate of approximately 4.4%, the study highlights the challenges of engaging students in GAI-related research and underscores the need for larger sample sizes in future studies. By gaining insights into student perspectives, instructors can better prepare to integrate discussions of GAI into their classrooms, fostering informed and critical engagement with this transformative technology.",
    "URL": "http://arxiv.org/abs/2601.04336v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04574",
    "type": "report",
    "title": "FeedEval: Pedagogically Aligned Evaluation of LLM-Generated Essay Feedback",
    "author": [
      {
        "family": "Chu",
        "given": "Seongyeub"
      },
      {
        "family": "Kim",
        "given": "Jongwoo"
      },
      {
        "family": "Yi",
        "given": "Munyong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          8
        ]
      ]
    },
    "abstract": "Going beyond the prediction of numerical scores, recent research in automated essay scoring has increasingly emphasized the generation of high-quality feedback that provides justification and actionable guidance. To mitigate the high cost of expert annotation, prior work has commonly relied on LLM-generated feedback to train essay assessment models. However, such feedback is often incorporated without explicit quality validation, resulting in the propagation of noise in downstream applications. To address this limitation, we propose FeedEval, an LLM-based framework for evaluating LLM-generated essay feedback along three pedagogically grounded dimensions: specificity, helpfulness, and validity. FeedEval employs dimension-specialized LLM evaluators trained on datasets curated in this study to assess multiple feedback candidates and select high-quality feedback for downstream use. Experiments on the ASAP++ benchmark show that FeedEval closely aligns with human expert judgments and that essay scoring models trained with FeedEval-filtered high-quality feedback achieve superior scoring performance. Furthermore, revision experiments using small LLMs show that the high-quality feedback identified by FeedEval leads to more effective essay revisions. We will release our code and curated datasets upon accepted.",
    "URL": "http://arxiv.org/abs/2601.04574v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04654",
    "type": "report",
    "title": "LLMs-Integrated Automatic Hate Speech Recognition Using Controllable Text Generation Models",
    "author": [
      {
        "family": "Oshima",
        "given": "Ryutaro"
      },
      {
        "family": "Hosoda",
        "given": "Yuya"
      },
      {
        "family": "Iiguni",
        "given": "Youji"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          8
        ]
      ]
    },
    "abstract": "This paper proposes an automatic speech recognition (ASR) model for hate speech using large language models (LLMs). The proposed method integrates the encoder of the ASR model with the decoder of the LLMs, enabling simultaneous transcription and censorship tasks to prevent the exposure of harmful content. Instruction tuning of the LLM to mask hate-related words with specific tokens requires an annotated hate speech dataset, which is limited. We generate text samples using an LLM with the Chain-of-Thought (CoT) prompting technique guided by cultural context and examples and then convert them into speech samples using a text-to-speech (TTS) system. However, some of them contain non-hate speech samples with hate-related words, which degrades the censorship performance. This paper filters the samples which text classification models correctly label as hate content. By adjusting the threshold for the number of correct answer models, we can control the level of hate in the generated dataset, allowing us to train the LLMs through curriculum learning in a gradual manner. Experimental results show that the proposed method achieves a masking accuracy of 58.6\\% for hate-related words, surpassing previous baselines. We also confirm that the curriculum training contributes to the efficiency of both transcription and censorship tasks.",
    "URL": "http://arxiv.org/abs/2601.04654v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04919",
    "type": "report",
    "title": "What Students Ask, How a Generative AI Assistant Responds: Exploring Higher Education Students' Dialogues on Learning Analytics Feedback",
    "author": [
      {
        "family": "Uzun",
        "given": "Yildiz"
      },
      {
        "family": "Gauthier",
        "given": "Andrea"
      },
      {
        "family": "Cukurova",
        "given": "Mutlu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          8
        ]
      ]
    },
    "abstract": "Learning analytics dashboards (LADs) aim to support students' regulation of learning by translating complex data into feedback. Yet students, especially those with lower self-regulated learning (SRL) competence, often struggle to engage with and interpret analytics feedback. Conversational generative artificial intelligence (GenAI) assistants have shown potential to scaffold this process through real-time, personalised, dialogue-based support. Further advancing this potential, we explored authentic dialogues between students and GenAI assistant integrated into LAD during a 10-week semester. The analysis focused on questions students with different SRL levels posed, the relevance and quality of the assistant's answers, and how students perceived the assistant's role in their learning. Findings revealed distinct query patterns. While low SRL students sought clarification and reassurance, high SRL students queried technical aspects and requested personalised strategies. The assistant provided clear and reliable explanations but limited in personalisation, handling emotionally charged queries, and integrating multiple data points for tailored responses. Findings further extend that GenAI interventions can be especially valuable for low SRL students, offering scaffolding that supports engagement with feedback and narrows gaps with their higher SRL peers. At the same time, students' reflections underscored the importance of trust, need for greater adaptivity, context-awareness, and technical refinement in future systems.",
    "URL": "http://arxiv.org/abs/2601.04919v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04940",
    "type": "report",
    "title": "CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs",
    "author": [
      {
        "family": "Nijdam",
        "given": "Arthur"
      },
      {
        "family": "K√§hk√∂nen",
        "given": "Harri"
      },
      {
        "family": "Niemi",
        "given": "Valtteri"
      },
      {
        "family": "Wagner",
        "given": "Paul Stankovski"
      },
      {
        "family": "Ramezanian",
        "given": "Sara"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          8
        ]
      ]
    },
    "abstract": "The cybersecurity landscape is constantly evolving, driven by increased digitalization and new cybersecurity threats. Cybersecurity programs often fail to equip graduates with skills demanded by the workforce, particularly concerning recent developments in cybersecurity, as curriculum design is costly and labor-intensive. To address this misalignment, we present a novel Large Language Model (LLM)-based framework for automated design and analysis of cybersecurity curricula, called CurricuLLM. Our approach provides three key contributions: (1) automation of personalized curriculum design, (2) a data-driven pipeline aligned with industry demands, and (3) a comprehensive methodology for leveraging fine-tuned LLMs in curriculum development.\n  CurricuLLM utilizes a two-tier approach consisting of PreprocessLM, which standardizes input data, and ClassifyLM, which assigns course content to nine Knowledge Areas in cybersecurity. We systematically evaluated multiple Natural Language Processing (NLP) architectures and fine-tuning strategies, ultimately selecting the Bidirectional Encoder Representations from Transformers (BERT) model as ClassifyLM, fine-tuned on foundational cybersecurity concepts and workforce competencies.\n  We are the first to validate our method with human experts who analyzed real-world cybersecurity curricula and frameworks, motivating that CurricuLLM is an efficient solution to replace labor-intensive curriculum analysis. Moreover, once course content has been classified, it can be integrated with established cybersecurity role-based weights, enabling alignment of the educational program with specific job roles, workforce categories, or general market needs. This lays the foundation for personalized, workforce-aligned cybersecurity curricula that prepare students for the evolving demands in cybersecurity.",
    "URL": "http://arxiv.org/abs/2601.04940v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.05187",
    "type": "report",
    "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
    "author": [
      {
        "family": "Liang",
        "given": "Yanchang"
      },
      {
        "family": "Zhao",
        "given": "Xiaowei"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          8
        ]
      ]
    },
    "abstract": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
    "URL": "http://arxiv.org/abs/2601.05187v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.05414",
    "type": "report",
    "title": "Large Language Models Are Bad Dice Players: LLMs Struggle to Generate Random Numbers from Statistical Distributions",
    "author": [
      {
        "family": "Zhao",
        "given": "Minda"
      },
      {
        "family": "Du",
        "given": "Yilun"
      },
      {
        "family": "Wang",
        "given": "Mengyu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          8
        ]
      ]
    },
    "abstract": "As large language models (LLMs) transition from chat interfaces to integral components of stochastic pipelines across domains like educational assessment and synthetic data construction, the ability to faithfully sample from specified probability distributions has become a functional requirement rather than a theoretical curiosity. We present the first large-scale, statistically powered audit of native probabilistic sampling in frontier LLMs, benchmarking 11 models across 15 distributions. To disentangle failure modes, we employ a dual-protocol design: Batch Generation, where a model produces N=1000 samples within one response, and Independent Requests, comprising $N=1000$ stateless calls. We observe a sharp protocol asymmetry: batch generation achieves only modest statistical validity, with a 13% median pass rate, while independent requests collapse almost entirely, with 10 of 11 models passing none of the distributions. Beyond this asymmetry, we reveal that sampling fidelity degrades monotonically with distributional complexity and aggravates as the requested sampling horizon N increases. Finally, we demonstrate the propagation of these failures into downstream tasks: models fail to enforce uniform answer-position constraints in MCQ generation and systematically violate demographic targets in attribute-constrained text-to-image prompt synthesis. These findings indicate that current LLMs lack a functional internal sampler, necessitating the use of external tools for applications requiring statistical guarantees.",
    "URL": "http://arxiv.org/abs/2601.05414v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.05473",
    "type": "report",
    "title": "Towards Valid Student Simulation with Large Language Models",
    "author": [
      {
        "family": "Yuan",
        "given": "Zhihao"
      },
      {
        "family": "Xiao",
        "given": "Yunze"
      },
      {
        "family": "Li",
        "given": "Ming"
      },
      {
        "family": "Xuan",
        "given": "Weihao"
      },
      {
        "family": "Tong",
        "given": "Richard"
      },
      {
        "family": "Diab",
        "given": "Mona"
      },
      {
        "family": "Mitchell",
        "given": "Tom"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          9
        ]
      ]
    },
    "abstract": "This paper presents a conceptual and methodological framework for large language model (LLM) based student simulation in educational settings. The authors identify a core failure mode, termed the \"competence paradox\" in which broadly capable LLMs are asked to emulate partially knowledgeable learners, leading to unrealistic error patterns and learning dynamics. To address this, the paper reframes student simulation as a constrained generation problem governed by an explicit Epistemic State Specification (ESS), which defines what a simulated learner can access, how errors are structured, and how learner state evolves over time. The work further introduces a Goal-by-Environment framework to situate simulated student systems according to behavioral objectives and deployment contexts. Rather than proposing a new system or benchmark, the paper synthesizes prior literature, formalizes key design dimensions, and articulates open challenges related to validity, evaluation, and ethical risks. Overall, the paper argues for epistemic fidelity over surface realism as a prerequisite for using LLM-based simulated students as reliable scientific and pedagogical instruments.",
    "URL": "http://arxiv.org/abs/2601.05473v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.05485",
    "type": "report",
    "title": "Readability-Robust Code Summarization via Meta Curriculum Learning",
    "author": [
      {
        "family": "Zeng",
        "given": "Wenhao"
      },
      {
        "family": "Chai",
        "given": "Yitian"
      },
      {
        "family": "Zhou",
        "given": "Hao"
      },
      {
        "family": "Meng",
        "given": "Fandong"
      },
      {
        "family": "Zhou",
        "given": "Jie"
      },
      {
        "family": "Gu",
        "given": "Xiaodong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          9
        ]
      ]
    },
    "abstract": "Code summarization has emerged as a fundamental technique in the field of program comprehension. While code language models have shown significant advancements, the current models and benchmarks are confined to high-readability code, which contains sufficient semantic cues such as function and variable names. In the real world, however, code is often poorly structured or obfuscated, significantly degrading model performance. In this paper, we first empirically evaluate the robustness of state-of-the-art language models on poor-readability code for the task of code summarization, focusing on (1) their effectiveness, (2) the impact of prompt engineering, and (3) the robustness of different variants. Experimental results reveal that state-of-the-art models-including GPT-4o and DeepSeek-V3 experience a substantial performance drop when faced with poorly readable code, and that prompt engineering and reasoning-enhanced models offer limited improvements. Motivated by these findings, we propose RoFTCodeSum, a novel fine-tuning method that enhances the robustness of code summarization against poorly readable code. RoFTCodeSum marries the concepts of curriculum learning and meta-learning: based on the original dataset for fine-tuning, it creates curricular training sets, e.g., obfuscating function names and identifiers from the code, respectively, that have progressive difficulty in code comprehension. In each training step, the approach meta-updates the gradients using these progressively challenging datasets, thereby optimizing both accuracy and readability robustness simultaneously. Experimental results demonstrate that RoFTCodeSum exhibits increased robustness against semantic perturbation while enhancing performance on the original code.",
    "URL": "http://arxiv.org/abs/2601.05485v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.05858",
    "type": "report",
    "title": "CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning",
    "author": [
      {
        "family": "Dragomir",
        "given": "Alexandra"
      },
      {
        "family": "Brad",
        "given": "Florin"
      },
      {
        "family": "Ionescu",
        "given": "Radu Tudor"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          9
        ]
      ]
    },
    "abstract": "Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given during training. We address this topic by integrating curriculum learning into various state-of-the-art preference optimization algorithms to boost MT performance. We introduce a novel curriculum learning strategy with restarts (CLewR), which reiterates easy-to-hard curriculum multiple times during training to effectively mitigate the catastrophic forgetting of easy examples. We demonstrate consistent gains across several model families (Gemma2, Qwen2.5, Llama3.1) and preference optimization techniques. We publicly release our code at https://github.com/alexandra-dragomir/CLewR.",
    "URL": "http://arxiv.org/abs/2601.05858v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06092",
    "type": "report",
    "title": "Islamic Chatbots in the Age of Large Language Models",
    "author": [
      {
        "family": "Ahmad",
        "given": "Muhammad Aurangzeb"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          31
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) are rapidly transforming how communities access, interpret, and circulate knowledge, and religious communities are no exception. Chatbots powered by LLMs are beginning to reshape authority, pedagogy, and everyday religious practice in Muslim communities. We analyze the landscape of LLM powered Islamic chatbots and how they are transforming Islamic religious practices e.g., democratizing access to religious knowledge but also running the risk of erosion of authority. We discuss what kind of challenges do these systems raise for Muslim communities and explore recommendations for the responsible design of these systems.",
    "URL": "http://arxiv.org/abs/2601.06092v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06121",
    "type": "report",
    "title": "Prompt Engineering for Responsible Generative AI Use in African Education: A Report from a Three-Day Training Series",
    "author": [
      {
        "family": "Quarshie",
        "given": "Benjamin"
      },
      {
        "family": "Willemse",
        "given": "Vanessa"
      },
      {
        "family": "Nabang",
        "given": "Macharious"
      },
      {
        "family": "Akanzire",
        "given": "Bismark Nyaaba"
      },
      {
        "family": "Kyeremeh",
        "given": "Patrick"
      },
      {
        "family": "Maigari",
        "given": "Saeed"
      },
      {
        "family": "Adomina",
        "given": "Dorcas"
      },
      {
        "family": "Kwarteng",
        "given": "Ellen"
      },
      {
        "family": "Majialuwe",
        "given": "Eric Kojo"
      },
      {
        "family": "Gibbs",
        "given": "Craig"
      },
      {
        "family": "Kudaya",
        "given": "Jerry Etornam"
      },
      {
        "family": "Koma",
        "given": "Sechaba"
      },
      {
        "family": "Nyaaba",
        "given": "Matthew Nyaaba Matthew"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          4
        ]
      ]
    },
    "abstract": "Generative artificial intelligence (GenAI) tools are increasingly adopted in education, yet many educators lack structured guidance on responsible and context sensitive prompt engineering, particularly in African and other resource constrained settings. This case report documents a three day online professional development programme organised by Generative AI for Education and Research in Africa (GenAI-ERA), designed to strengthen educators and researchers capacity to apply prompt engineering ethically for academic writing, teaching, and research. The programme engaged 468 participants across multiple African countries, including university educators, postgraduate students, and researchers. The training followed a scaffolded progression from foundational prompt design to applied and ethical strategies, including persona guided interactions. Data sources comprised registration surveys, webinar interaction records, facilitator observations, and session transcripts, analysed using descriptive statistics and computationally supported qualitative techniques. Findings indicate that participants increasingly conceptualised prompt engineering as a form of AI literacy requiring ethical awareness, contextual sensitivity, and pedagogical judgement rather than technical skill alone. The case highlights persistent challenges related to access, locally relevant training materials, and institutional support. The report recommends sustained professional development and the integration of prompt literacy into curricula to support responsible GenAI use in African education systems.",
    "URL": "http://arxiv.org/abs/2601.06121v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06141",
    "type": "report",
    "title": "An LLM -Powered Assessment Retrieval-Augmented Generation (RAG) For Higher Education",
    "author": [
      {
        "family": "Barenji",
        "given": "Reza Vatankhah"
      },
      {
        "family": "Salimi",
        "given": "Nazila"
      },
      {
        "family": "Khoshgoftar",
        "given": "Sina"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          5
        ]
      ]
    },
    "abstract": "Providing timely, consistent, and high-quality feedback in large-scale higher education courses remains a persistent challenge, often constrained by instructor workload and resource limitations. This study presents an LLM-powered, agentic assessment system built on a Retrieval-Augmented Generation (RAG) architecture to address these challenges. The system integrates a large language model with a structured retrieval mechanism that accesses rubric criteria, exemplar essays, and instructor feedback to generate contextually grounded grades and formative comments. A mixed-methods evaluation was conducted using 701 student essays, combining quantitative analyses of inter-rater reliability, scoring alignment, and consistency with instructor assessments, alongside qualitative evaluation of feedback quality, pedagogical relevance, and student support. Results demonstrate that the RAG system can produce reliable, rubric-aligned feedback at scale, achieving 94--99% agreement with human evaluators, while also enhancing students' opportunities for self-regulated learning and engagement with assessment criteria. The discussion highlights both pedagogical limitations, including potential constraints on originality and feedback dialogue, and the transformative potential of RAG systems to augment instructors' capabilities, streamline assessment workflows, and support scalable, adaptive learning environments. This research contributes empirical evidence for the application of agentic AI in higher education, offering a scalable and pedagogically informed model for enhancing feedback accessibility, consistency, and quality.",
    "URL": "http://arxiv.org/abs/2601.06141v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06171",
    "type": "report",
    "title": "From Individual Prompts to Collective Intelligence: Mainstreaming Generative AI in the Classroom",
    "author": [
      {
        "family": "Qadir",
        "given": "Junaid"
      },
      {
        "family": "Khan",
        "given": "Muhammad Salman"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "Engineering classrooms are increasingly experimenting with generative AI (GenAI), but most uses remain confined to individual prompting and isolated assistance. This narrow framing risks reinforcing equity gaps and only rewarding the already privileged or motivated students. We argue instead for a shift toward collective intelligence (CI)-focused pedagogy, where GenAI acts as a catalyst for peer-to-peer learning. We implemented Generative CI (GCI) activities in two undergraduate engineering courses, engaging 140 students through thinking routines -- short, repeatable scaffolds developed by Harvard Project Zero to make thinking visible and support collaborative sense-making. Using routines such as Question Sorts and Peel the Fruit, combined with strategic AI consultation, we enabled students to externalize their reasoning, compare interpretations, and iteratively refine ideas. Our dual-pronged approach synthesizes literature from learning sciences, CI, embodied cognition, and philosophy of technology, while also empirically learning through student surveys and engagement observations. Results demonstrate that students value the combination of human collaboration with strategic AI support, recognizing risks of over-reliance while appreciating AI's role in expanding perspectives. Students identified that group work fosters deeper understanding and creative problem-solving than AI alone, with the timing of AI consultation significantly affecting learning outcomes. We offer practical implementation pathways for mainstreaming CI-focused pedagogy that cultivates deeper engagement, resilient problem-solving, and shared ownership of knowledge.",
    "URL": "http://arxiv.org/abs/2601.06171v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06172",
    "type": "report",
    "title": "The Psychology of Learning from Machines: Anthropomorphic AI and the Paradox of Automation in Education",
    "author": [
      {
        "family": "Qadir",
        "given": "Junaid"
      },
      {
        "family": "Mumtaz",
        "given": "Muhammad"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "As AI tutors enter classrooms at unprecedented speed, their deployment increasingly outpaces our grasp of the psychological and social consequences of such technology. Yet decades of research in automation psychology, human factors, and human-computer interaction provide crucial insights that remain underutilized in educational AI design. This work synthesizes four research traditions -- automation psychology, human factors engineering, HCI, and philosophy of technology -- to establish a comprehensive framework for understanding how learners psychologically relate to anthropomorphic AI tutors. We identify three persistent challenges intensified by Generative AI's conversational fluency. First, learners exhibit dual trust calibration failures -- automation bias (uncritical acceptance) and algorithm aversion (excessive rejection after errors) -- with an expertise paradox where novices overrely while experts underrely. Second, while anthropomorphic design enhances engagement, it can distract from learning and foster harmful emotional attachment. Third, automation ironies persist: systems meant to aid cognition introduce designer errors, degrade skills through disuse, and create monitoring burdens humans perform poorly. We ground this theoretical synthesis through comparative analysis of over 104,984 YouTube comments across AI-generated philosophical debates and human-created engineering tutorials, revealing domain-dependent trust patterns and strong anthropomorphic projection despite minimal cues. For engineering education, our synthesis mandates differentiated approaches: AI tutoring for technical foundations where automation bias is manageable through proper scaffolding, but human facilitation for design, ethics, and professional judgment where tacit knowledge transmission proves irreplaceable.",
    "URL": "http://arxiv.org/abs/2601.06172v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06225",
    "type": "report",
    "title": "Classroom AI: Large Language Models as Grade-Specific Teachers",
    "author": [
      {
        "family": "Oh",
        "given": "Jio"
      },
      {
        "family": "Whang",
        "given": "Steven Euijong"
      },
      {
        "family": "Evans",
        "given": "James"
      },
      {
        "family": "Wang",
        "given": "Jindong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          9
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) offer a promising solution to complement traditional teaching and address global teacher shortages that affect hundreds of millions of children, but they fail to provide grade-appropriate responses for students at different educational levels. We introduce a framework for finetuning LLMs to generate age-appropriate educational content across six grade levels, from lower elementary to adult education. Our framework successfully adapts explanations to match students' comprehension capacities without sacrificing factual correctness. This approach integrates seven established readability metrics through a clustering method and builds a comprehensive dataset for grade-specific content generation. Evaluations across multiple datasets with 208 human participants demonstrate substantial improvements in grade-level alignment, achieving a 35.64 percentage point increase compared to prompt-based methods while maintaining response accuracy. AI-assisted learning tailored to different grade levels has the potential to advance educational engagement and equity.",
    "URL": "http://arxiv.org/abs/2601.06225v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06394",
    "type": "report",
    "title": "Context Matters: Peer-Aware Student Behavioral Engagement Measurement via VLM Action Parsing and LLM Sequence Classification",
    "author": [
      {
        "family": "Abdelkawy",
        "given": "Ahmed"
      },
      {
        "family": "Elsayed",
        "given": "Ahmed"
      },
      {
        "family": "Ali",
        "given": "Asem"
      },
      {
        "family": "Farag",
        "given": "Aly"
      },
      {
        "family": "Tretter",
        "given": "Thomas"
      },
      {
        "family": "McIntyre",
        "given": "Michael"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          10
        ]
      ]
    },
    "abstract": "Understanding student behavior in the classroom is essential to improve both pedagogical quality and student engagement. Existing methods for predicting student engagement typically require substantial annotated data to model the diversity of student behaviors, yet privacy concerns often restrict researchers to their own proprietary datasets. Moreover, the classroom context, represented in peers' actions, is ignored. To address the aforementioned limitation, we propose a novel three-stage framework for video-based student engagement measurement. First, we explore the few-shot adaptation of the vision-language model for student action recognition, which is fine-tuned to distinguish among action categories with a few training samples. Second, to handle continuous and unpredictable student actions, we utilize the sliding temporal window technique to divide each student's 2-minute-long video into non-overlapping segments. Each segment is assigned an action category via the fine-tuned VLM model, generating a sequence of action predictions. Finally, we leverage the large language model to classify this entire sequence of actions, together with the classroom context, as belonging to an engaged or disengaged student. The experimental results demonstrate the effectiveness of the proposed approach in identifying student engagement.",
    "URL": "http://arxiv.org/abs/2601.06394v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06536",
    "type": "report",
    "title": "Expos√≠a: Academic Writing Assessment of Expos√©s and Peer Feedback",
    "author": [
      {
        "family": "Zyska",
        "given": "Dennis"
      },
      {
        "family": "Rozovskaya",
        "given": "Alla"
      },
      {
        "family": "Kuznetsov",
        "given": "Ilia"
      },
      {
        "family": "Gurevych",
        "given": "Iryna"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          10
        ]
      ]
    },
    "abstract": "We present Expos√≠a, the first public dataset that connects writing and feedback assessment in higher education, enabling research on educationally grounded approaches to academic writing evaluation. Expos√≠a includes student research project proposals and peer and instructor feedback consisting of comments and free-text reviews. The dataset was collected in the \"Introduction to Scientific Work\" course of the Computer Science undergraduate program that focuses on teaching academic writing skills and providing peer feedback on academic writing. Expos√≠a reflects the multi-stage nature of the academic writing process that includes drafting, providing and receiving feedback, and revising the writing based on the feedback received. Both the project proposals and peer feedback are accompanied by human assessment scores based on a fine-grained, pedagogically-grounded schema for writing and feedback assessment that we develop.\n  We use Expos√≠a to benchmark state-of-the-art open-source large language models (LLMs) for two tasks: automated scoring of (1) the proposals and (2) the student reviews. The strongest LLMs attain high agreement on scoring aspects that require little domain knowledge but degrade on dimensions evaluating content, in line with human agreement values. We find that LLMs align better with the human instructors giving high scores. Finally, we establish that a prompting strategy that scores multiple aspects of the writing together is the most effective, an important finding for classroom deployment.",
    "URL": "http://arxiv.org/abs/2601.06536v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06543",
    "type": "report",
    "title": "SimLLM: Fine-Tuning Code LLMs for SimPy-Based Queueing System Simulation",
    "author": [
      {
        "family": "Chen",
        "given": "Jun-Qi"
      },
      {
        "family": "Zhang",
        "given": "Kun"
      },
      {
        "family": "Zheng",
        "given": "Rui"
      },
      {
        "family": "Zhong",
        "given": "Ying"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          10
        ]
      ]
    },
    "abstract": "The Python package SimPy is widely used for modeling queueing systems due to its flexibility, simplicity, and smooth integration with modern data analysis and optimization frameworks. Recent advances in large language models (LLMs) have shown strong ability in generating clear and executable code, making them powerful and suitable tools for writing SimPy queueing simulation code. However, directly employing closed-source models like GPT-4o to generate such code may lead to high computational costs and raise data privacy concerns. To address this, we fine-tune two open-source LLMs, Qwen-Coder-7B and DeepSeek-Coder-6.7B, on curated SimPy queueing data, which enhances their code-generating performance in executability, output-format compliance, and instruction-code consistency. Particularly, we proposed a multi-stage fine-tuning framework comprising two stages of supervised fine-tuning (SFT) and one stage of direct preference optimization (DPO), progressively enhancing the model's ability in SimPy-based queueing simulation code generation. Extensive evaluations demonstrate that both fine-tuned models achieve substantial improvements in executability, output-format compliance, and instruct consistency. These results confirm that domain-specific fine-tuning can effectively transform compact open-source code models into reliable SimPy simulation generators which provide a practical alternative to closed-source LLMs for education, research, and operational decision support.",
    "URL": "http://arxiv.org/abs/2601.06543v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06586",
    "type": "report",
    "title": "Detecting LLM-Generated Text with Performance Guarantees",
    "author": [
      {
        "family": "Zhou",
        "given": "Hongyi"
      },
      {
        "family": "Zhu",
        "given": "Jin"
      },
      {
        "family": "Yang",
        "given": "Ying"
      },
      {
        "family": "Shi",
        "given": "Chengchun"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          10
        ]
      ]
    },
    "abstract": "Large language models (LLMs) such as GPT, Claude, Gemini, and Grok have been deeply integrated into our daily life. They now support a wide range of tasks -- from dialogue and email drafting to assisting with teaching and coding, serving as search engines, and much more. However, their ability to produce highly human-like text raises serious concerns, including the spread of fake news, the generation of misleading governmental reports, and academic misconduct. To address this practical problem, we train a classifier to determine whether a piece of text is authored by an LLM or a human. Our detector is deployed on an online CPU-based platform https://huggingface.co/spaces/stats-powered-ai/StatDetectLLM, and contains three novelties over existing detectors: (i) it does not rely on auxiliary information, such as watermarks or knowledge of the specific LLM used to generate the text; (ii) it more effectively distinguishes between human- and LLM-authored text; and (iii) it enables statistical inference, which is largely absent in the current literature. Empirically, our classifier achieves higher classification accuracy compared to existing detectors, while maintaining type-I error control, high statistical power, and computational efficiency.",
    "URL": "http://arxiv.org/abs/2601.06586v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06633",
    "type": "report",
    "title": "KASER: Knowledge-Aligned Student Error Simulator for Open-Ended Coding Tasks",
    "author": [
      {
        "family": "Duan",
        "given": "Zhangqi"
      },
      {
        "family": "Fernandez",
        "given": "Nigel"
      },
      {
        "family": "Lan",
        "given": "Andrew"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          10
        ]
      ]
    },
    "abstract": "Open-ended tasks, such as coding problems that are common in computer science education, provide detailed insights into student knowledge. However, training large language models (LLMs) to simulate and predict possible student errors in their responses to these problems can be challenging: they often suffer from mode collapse and fail to fully capture the diversity in syntax, style, and solution approach in student responses. In this work, we present KASER (Knowledge-Aligned Student Error Simulator), a novel approach that aligns errors with student knowledge. We propose a training method based on reinforcement learning using a hybrid reward that reflects three aspects of student code prediction: i) code similarity to the ground-truth, ii) error matching, and iii) code prediction diversity. On two real-world datasets, we perform two levels of evaluation and show that: At the per-student-problem pair level, our method outperforms baselines on code and error prediction; at the per-problem level, our method outperforms baselines on error coverage and simulated code diversity.",
    "URL": "http://arxiv.org/abs/2601.06633v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06767",
    "type": "report",
    "title": "GanitLLM: Difficulty-Aware Bengali Mathematical Reasoning through Curriculum-GRPO",
    "author": [
      {
        "family": "Dipta",
        "given": "Shubhashis Roy"
      },
      {
        "family": "Mahbub",
        "given": "Khairul"
      },
      {
        "family": "Najjar",
        "given": "Nadia"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          11
        ]
      ]
    },
    "abstract": "We present a Bengali mathematical reasoning model called GanitLLM (named after the Bangla word for mathematics, \"Ganit\"), together with a new difficulty-aware Bengali math corpus and a curriculum-based GRPO pipeline. Bengali is one of the world's most widely spoken languages, yet existing LLMs either reason in English and then translate, or simply fail on multi-step Bengali math, in part because reinforcement learning recipes are tuned for high-resource languages and collapse under reward sparsity in low-resource settings. To address this, we construct Ganit, a rigorously filtered and decontaminated Bengali math dataset with automatic difficulty tags derived from the pass@k of a strong evaluator model. Building on this dataset, we propose Curriculum-GRPO, which combines multi-stage training (SFT + GRPO) with difficulty-aware sampling and verifiable rewards for format, numerical correctness, and Bengali reasoning. On Bn-MGSM and Bn-MSVAMP, GanitLLM-4B improves over its Qwen3-4B base by +8 and +7 accuracy points, respectively, while increasing the percentage of Bengali reasoning tokens from 14% to over 88% and reducing average solution length from 943 to 193 words.",
    "URL": "http://arxiv.org/abs/2601.06767v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06774",
    "type": "report",
    "title": "ImmuniFraug: A Metacognitive Intervention Anti-Fraud Approach to Enhance Undergraduate Students' Cyber Fraud Awareness",
    "author": [
      {
        "family": "Yuan",
        "given": "Xiangzhe"
      },
      {
        "family": "Wang",
        "given": "Jiajun"
      },
      {
        "family": "Wang",
        "given": "Huanchen"
      },
      {
        "family": "Wan",
        "given": "Qian"
      },
      {
        "family": "Hu",
        "given": "Siying"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          11
        ]
      ]
    },
    "abstract": "Cyber fraud now constitutes over half of criminal cases in China, with undergraduate students experiencing a disproportionate rise in victimization. Traditional anti-fraud training remains predominantly passive, yielding limited engagement and retention. This paper introduces ImmuniFraug, a Large Language Model (LLM)-based metacognitive intervention that delivers immersive, multimodal fraud simulations integrating text, voice, and visual avatars across ten prevalent fraud types. Each scenario is designed to replicate real-world persuasion tactics and psychological pressure, while post-interaction debriefs provide grounded feedback in protection motivation theory and reflective prompts to reinforce learning. In a controlled study with 846 Chinese undergraduates, ImmuniFraug was compared to official text-based materials. Linear Mixed-Effects Modeling (LMEM) reveals that the interactive intervention significantly improved fraud awareness (p = 0.026), successfully providing incremental learning value even when controlling for participants' extensive prior exposure to anti-fraud education, alongside high narrative immersion (M = 56.95/77). Thematic analysis of interviews revealed key effectiveness factors: perceived realism, adaptive deception, enforced time pressure, emotional manipulation awareness, and enhanced self-efficacy. Findings demonstrate that by shifting the focus from passive knowledge acquisition to active metacognitive engagement, LLM-based simulations offer a scalable and ecologically valid new paradigm for anti-fraud training and fostering fraud resilience.",
    "URL": "http://arxiv.org/abs/2601.06774v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06780",
    "type": "report",
    "title": "Multi-Stage Evolutionary Model Merging with Meta Data Driven Curriculum Learning for Sentiment-Specialized Large Language Modeling",
    "author": [
      {
        "family": "Inoshita",
        "given": "Keito"
      },
      {
        "family": "Zhou",
        "given": "Xiaokang"
      },
      {
        "family": "Kawai",
        "given": "Akira"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          11
        ]
      ]
    },
    "abstract": "The emergence of large language models (LLMs) has significantly transformed natural language processing (NLP), enabling more generalized models to perform various tasks with minimal training. However, traditional sentiment analysis methods, which focus on individual tasks such as sentiment classification or aspect-based analysis, are not practical for real-world applications that usually require handling multiple tasks. While offering flexibility, LLMs in sentiment-specific tasks often fall short of the required accuracy. Techniques like fine-tuning and evolutionary model merging help integrate models into a unified framework, which can improve the learning performance while reducing computational costs. The use of task meta-data and curriculum learning to optimize learning processes remains underexplored, while sentiment analysis is a critical task in NLP that requires high accuracy and scalability across multiple subtasks. In this study, we propose a hybrid learning model called Multi-stage Evolutionary Model Merging with Meta data driven Curriculum Learning (MEM-MCL), to enhance the sentiment analysis in large language modeling. In particular, expert models are created through instruction tuning for specific sentiment tasks and then merged using evolutionary algorithms to form a unified model. The merging process is optimized with weak data to enhance performance across tasks. The curriculum learning is incorporated to provide a learning sequence based on task difficulty, improving knowledge extraction from LLMs. Experiment results demonstrate that the proposed MEM-MCL model outperforms conventional LLMs in a majority of sentiment analysis tasks, achieving superior results across various subtasks.",
    "URL": "http://arxiv.org/abs/2601.06780v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06979",
    "type": "report",
    "title": "MedTutor: A Retrieval-Augmented LLM System for Case-Based Medical Education",
    "author": [
      {
        "family": "Jang",
        "given": "Dongsuk"
      },
      {
        "family": "Shangguan",
        "given": "Ziyao"
      },
      {
        "family": "Tegtmeyer",
        "given": "Kyle"
      },
      {
        "family": "Gupta",
        "given": "Anurag"
      },
      {
        "family": "Czerminski",
        "given": "Jan"
      },
      {
        "family": "Chheang",
        "given": "Sophie"
      },
      {
        "family": "Cohan",
        "given": "Arman"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          11
        ]
      ]
    },
    "abstract": "The learning process for medical residents presents significant challenges, demanding both the ability to interpret complex case reports and the rapid acquisition of accurate medical knowledge from reliable sources. Residents typically study case reports and engage in discussions with peers and mentors, but finding relevant educational materials and evidence to support their learning from these cases is often time-consuming and challenging. To address this, we introduce MedTutor, a novel system designed to augment resident training by automatically generating evidence-based educational content and multiple-choice questions from clinical case reports. MedTutor leverages a Retrieval-Augmented Generation (RAG) pipeline that takes clinical case reports as input and produces targeted educational materials. The system's architecture features a hybrid retrieval mechanism that synergistically queries a local knowledge base of medical textbooks and academic literature (using PubMed, Semantic Scholar APIs) for the latest related research, ensuring the generated content is both foundationally sound and current. The retrieved evidence is filtered and ordered using a state-of-the-art reranking model and then an LLM generates the final long-form output describing the main educational content regarding the case-report. We conduct a rigorous evaluation of the system. First, three radiologists assessed the quality of outputs, finding them to be of high clinical and educational value. Second, we perform a large scale evaluation using an LLM-as-a Judge to understand if LLMs can be used to evaluate the output of the system. Our analysis using correlation between LLMs outputs and human expert judgments reveals a moderate alignment and highlights the continued necessity of expert oversight.",
    "DOI": "10.18653/v1/2025.emnlp-demos.24",
    "URL": "https://doi.org/10.18653/v1/2025.emnlp-demos.24",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.07354",
    "type": "report",
    "title": "Semantic Compression of LLM Instructions via Symbolic Metalanguages",
    "author": [
      {
        "family": "Gassen",
        "given": "Ernst van"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          12
        ]
      ]
    },
    "abstract": "We introduce MetaGlyph, a symbolic language for compressing prompts by encoding instructions as mathematical symbols rather than prose. Unlike systems requiring explicit decoding rules, MetaGlyph uses symbols like $\\in$ (membership) and $\\Rightarrow$ (implication) that models already understand from their training data. We test whether these symbols work as ''instruction shortcuts'' that models can interpret without additional teaching.\n  We evaluate eight models across two dimensions relevant to practitioners: scale (3B-1T parameters) and accessibility (open-source for local deployment vs. proprietary APIs). MetaGlyph achieves 62-81% token reduction across all task types. For API-based deployments, this translates directly to cost savings; for local deployments, it reduces latency and memory pressure.\n  Results vary by model. Gemini 2.5 Flash achieves 75% semantic equivalence between symbolic and prose instructions on selection tasks, with 49.9% membership operator fidelity. Kimi K2 reaches 98.1% fidelity for implication ($\\Rightarrow$) and achieves perfect (100%) accuracy on selection tasks with symbolic prompts. GPT-5.2 Chat shows the highest membership fidelity observed (91.3%), though with variable parse success across task types. Claude Haiku 4.5 achieves 100% parse success with 26% membership fidelity. Among mid-sized models, Qwen 2.5 7B shows 62% equivalence on extraction tasks. Mid-sized open-source models (7B-12B) show near-zero operator fidelity, suggesting a U-shaped relationship where sufficient scale overcomes instruction-tuning biases.",
    "URL": "http://arxiv.org/abs/2601.07354v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.08035",
    "type": "report",
    "title": "From Tool to Teacher: Rethinking Search Systems as Instructive Interfaces",
    "author": [
      {
        "family": "Elsweiler",
        "given": "David"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          12
        ]
      ]
    },
    "abstract": "Information access systems such as search engines and generative AI are central to how people seek, evaluate, and interpret information. Yet most systems are designed to optimise retrieval rather than to help users develop better search strategies or critical awareness. This paper introduces a pedagogical perspective on information access, conceptualising search and conversational systems as instructive interfaces that can teach, guide, and scaffold users' learning. We draw on seven didactic frameworks from education and behavioural science to analyse how existing and emerging system features, including query suggestions, source labels, and conversational or agentic AI, support or limit user learning. Using two illustrative search tasks, we demonstrate how different design choices promote skills such as critical evaluation, metacognitive reflection, and strategy transfer. The paper contributes a conceptual lens for evaluating the instructional value of information access systems and outlines design implications for technologies that foster more effective, reflective, and resilient information seekers.",
    "URL": "http://arxiv.org/abs/2601.08035v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.08402",
    "type": "report",
    "title": "PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors",
    "author": [
      {
        "family": "Rooein",
        "given": "Donya"
      },
      {
        "family": "Chowdhury",
        "given": "Sankalan Pal"
      },
      {
        "family": "Eremeeva",
        "given": "Mariia"
      },
      {
        "family": "Qin",
        "given": "Yuan"
      },
      {
        "family": "Nozza",
        "given": "Debora"
      },
      {
        "family": "Sachan",
        "given": "Mrinmaya"
      },
      {
        "family": "Hovy",
        "given": "Dirk"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          13
        ]
      ]
    },
    "abstract": "Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. Despite this, current LLM tutoring systems do not take into account student personality traits. To address this problem, we first construct a taxonomy that links pedagogical methods to personality profiles, based on pedagogical literature. We simulate student-teacher conversations and use our framework to let the LLM tutor adjust its strategy to the simulated student personality. We evaluate the scenario with human teachers and find that they consistently prefer our approach over two baselines. Our method also increases the use of less common, high-impact strategies such as role-playing, which human and LLM annotators prefer significantly. Our findings pave the way for developing more personalized and effective LLM use in educational applications.",
    "URL": "http://arxiv.org/abs/2601.08402v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.08697",
    "type": "report",
    "title": "Auditing Student-AI Collaboration: A Case Study of Online Graduate CS Students",
    "author": [
      {
        "family": "Dan",
        "given": "Nifu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          13
        ]
      ]
    },
    "abstract": "As generative AI becomes embedded in higher education, it increasingly shapes how students complete academic tasks. While these systems offer efficiency and support, concerns persist regarding over-automation, diminished student agency, and the potential for unreliable or hallucinated outputs. This study conducts a mixed-methods audit of student-AI collaboration preferences by examining the alignment between current AI capabilities and students' desired levels of automation in academic work. Using two sequential and complementary surveys, we capture students' perceived benefits, risks, and preferred boundaries when using AI. The first survey employs an existing task-based framework to assess preferences for and actual usage of AI across 12 academic tasks, alongside primary concerns and reasons for use. The second survey, informed by the first, explores how AI systems could be designed to address these concerns through open-ended questions. This study aims to identify gaps between existing AI affordances and students' normative expectations of collaboration, informing the development of more effective and trustworthy AI systems for education.",
    "URL": "http://arxiv.org/abs/2601.08697v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.08857",
    "type": "report",
    "title": "Revisiting Software Engineering Education in the Era of Large Language Models: A Curriculum Adaptation and Academic Integrity Framework",
    "author": [
      {
        "family": "Degerli",
        "given": "Mustafa"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          6
        ]
      ]
    },
    "abstract": "The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into professional workflows is increasingly reshaping software engineering practices. These tools have lowered the cost of code generation, explanation, and testing, while introducing new forms of automation into routine development tasks. In contrast, most of the software engineering and computer engineering curricula remain closely aligned with pedagogical models that equate manual syntax production with technical competence. This growing misalignment raises concerns regarding assessment validity, learning outcomes, and the development of foundational skills. Adopting a conceptual research approach, this paper proposes a theoretical framework for analyzing how generative AI alters core software engineering competencies and introduces a pedagogical design model for LLM-integrated education. Attention is given to computer engineering programs in Turkey, where centralized regulation, large class sizes, and exam-oriented assessment practices amplify these challenges. The framework delineates how problem analysis, design, implementation, and testing increasingly shift from construction toward critique, validation, and human-AI stewardship. In addition, the paper argues that traditional plagiarism-centric integrity mechanisms are becoming insufficient, motivating a transition toward a process transparency model. While this work provides a structured proposal for curriculum adaptation, it remains a theoretical contribution; the paper concludes by outlining the need for longitudinal empirical studies to evaluate these interventions and their long-term impacts on learning.",
    "URL": "http://arxiv.org/abs/2601.08857v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.08950",
    "type": "report",
    "title": "ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue",
    "author": [
      {
        "family": "Sharma",
        "given": "Mayank"
      },
      {
        "family": "Pea",
        "given": "Roy"
      },
      {
        "family": "Subramonyam",
        "given": "Hari"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          13
        ]
      ]
    },
    "abstract": "In educational applications, LLMs exhibit several fundamental pedagogical limitations, such as their tendency to reveal solutions rather than support dialogic learning. We introduce ConvoLearn (https://huggingface.co/datasets/masharma/convolearn ), a dataset grounded in knowledge building theory that operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. We construct a semi-synthetic dataset of 1250 tutor-student dialogues (20 turns each) in middle school Earth Science through controlled interactions between human teachers and a simulated student. Using QLoRA, we demonstrate that training on this dataset meaningfully shifts LLM behavior toward knowledge-building strategies. Human evaluation by 31 teachers shows our fine-tuned Mistral 7B (M = 4.10, SD = 1.03) significantly outperforms both its base version (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29) overall. This work establishes a potential framework to guide future development and evaluation of constructivist AI tutors.",
    "URL": "http://arxiv.org/abs/2601.08950v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.09120",
    "type": "report",
    "title": "Adaptive Multi-Stage Patent Claim Generation with Unified Quality Assessment",
    "author": [
      {
        "family": "Liang",
        "given": "Chen-Wei"
      },
      {
        "family": "Guo",
        "given": "Bin"
      },
      {
        "family": "Wei",
        "given": "Zhen-Yuan"
      },
      {
        "family": "Wang",
        "given": "Mu-Jiang-Shan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          14
        ]
      ]
    },
    "abstract": "Current patent claim generation systems face three fundamental limitations: poor cross-jurisdictional generalization, inadequate semantic relationship modeling between claims and prior art, and unreliable quality assessment. We introduce a novel three-stage framework that addresses these challenges through relationship-aware similarity analysis, domain-adaptive claim generation, and unified quality assessment. Our approach employs multi-head attention with eight specialized heads for explicit relationship modeling, integrates curriculum learning with dynamic LoRA adapter selection across five patent domains, and implements cross-attention mechanisms between evaluation aspects for comprehensive quality assessment. Extensive experiments on USPTO HUPD dataset, EPO patent collections, and Patent-CE benchmark demonstrate substantial improvements: 7.6-point ROUGE-L gain over GPT-4o, 8.3\\% BERTScore enhancement over Llama-3.1-8B, and 0.847 correlation with human experts compared to 0.623 for separate evaluation models. Our method maintains 89.4\\% cross-jurisdictional performance retention versus 76.2\\% for baselines, establishing a comprehensive solution for automated patent prosecution workflows.",
    "URL": "http://arxiv.org/abs/2601.09120v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.09182",
    "type": "report",
    "title": "Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback",
    "author": [
      {
        "family": "Yun",
        "given": "JungMin"
      },
      {
        "family": "Kwon",
        "given": "JuneHyoung"
      },
      {
        "family": "Kim",
        "given": "MiHyeon"
      },
      {
        "family": "Kim",
        "given": "YoungBin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          14
        ]
      ]
    },
    "abstract": "The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.",
    "URL": "http://arxiv.org/abs/2601.09182v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.09470",
    "type": "report",
    "title": "Personalized Multimodal Feedback Using Multiple External Representations: Strategy Profiles and Learning in High School Physics",
    "author": [
      {
        "family": "Revenga-Lozano",
        "given": "Natalia"
      },
      {
        "family": "Avila",
        "given": "Karina E."
      },
      {
        "family": "Steinert",
        "given": "Steffen"
      },
      {
        "family": "Schweinberger",
        "given": "Matthias"
      },
      {
        "family": "G√≥mez-P√©rez",
        "given": "Clara E."
      },
      {
        "family": "Kuhn",
        "given": "Jochen"
      },
      {
        "family": "K√ºchemann",
        "given": "Stefan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          14
        ]
      ]
    },
    "abstract": "Multiple external representations (MERs) and personalized feedback support physics learning, yet evidence on how personalized feedback can effectively integrate MERs remains limited. This question is particularly timely given the emergence of multimodal large language models. We conducted a 16-24 week observational study in high school physics (N=661) using a computer-based platform that provided verification and optional elaborated feedback in verbal, graphical and mathematical forms. Linear mixed-effects models and strategy-cluster analyses (ANCOVA-adjusted comparisons) tested associations between feedback use and post-test performance and moderation by representational competence. Elaborated multirepresentational feedback showed a small but consistent positive association with post-test scores independent of prior knowledge and confidence. Learners adopted distinct representation-selection strategies; among students with lower representational competence, using a diverse set of representations related to higher learning, whereas this advantage diminished as competence increased. These findings motivate adaptive feedback designs and inform intelligent tutoring systems capable of tailoring feedback elaboration and representational format to learner profiles, advancing personalized instruction in physics education.",
    "URL": "http://arxiv.org/abs/2601.09470v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.09953",
    "type": "report",
    "title": "Take Out Your Calculators: Estimating the Real Difficulty of Question Items with LLM Student Simulations",
    "author": [
      {
        "family": "Acquaye",
        "given": "Christabel"
      },
      {
        "family": "Huang",
        "given": "Yi Ting"
      },
      {
        "family": "Carpuat",
        "given": "Marine"
      },
      {
        "family": "Rudinger",
        "given": "Rachel"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          15
        ]
      ]
    },
    "abstract": "Standardized math assessments require expensive human pilot studies to establish the difficulty of test items. We investigate the predictive value of open-source large language models (LLMs) for evaluating the difficulty of multiple-choice math questions for real-world students. We show that, while LLMs are poor direct judges of problem difficulty, simulation-based approaches with LLMs yield promising results under the right conditions. Under the proposed approach, we simulate a \"classroom\" of 4th, 8th, or 12th grade students by prompting the LLM to role-play students of varying proficiency levels. We use the outcomes of these simulations to fit Item Response Theory (IRT) models, comparing learned difficulty parameters for items to their real-world difficulties, as determined by item-level statistics furnished by the National Assessment of Educational Progress (NAEP). We observe correlations as high as 0.75, 0.76, and 0.82 for grades 4, 8, and 12, respectively. In our simulations, we experiment with different \"classroom sizes,\" showing tradeoffs between computation size and accuracy. We find that role-plays with named students improves predictions (compared to student ids), and stratifying names across gender and race further improves predictions. Our results show that LLMs with relatively weaker mathematical abilities (Gemma) actually yield better real-world difficulty predictions than mathematically stronger models (Llama and Qwen), further underscoring the suitability of open-source models for the task.",
    "URL": "http://arxiv.org/abs/2601.09953v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.10093",
    "type": "report",
    "title": "Mark My Works Autograder for Programming Courses",
    "author": [
      {
        "family": "Qiu",
        "given": "Yiding"
      },
      {
        "family": "Azimi",
        "given": "Seyed Mahdi"
      },
      {
        "family": "Lensky",
        "given": "Artem"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          15
        ]
      ]
    },
    "abstract": "Large programming courses struggle to provide timely, detailed feedback on student code. We developed Mark My Works, a local autograding system that combines traditional unit testing with LLM-generated explanations. The system uses role-based prompts to analyze submissions, critique code quality, and generate pedagogical feedback while maintaining transparency in its reasoning process.\n  We piloted the system in a 191-student engineering course, comparing AI-generated assessments with human grading on 79 submissions. While AI scores showed no linear correlation with human scores (r = -0.177, p = 0.124), both systems exhibited similar left-skewed distributions, suggesting they recognize comparable quality hierarchies despite different scoring philosophies. The AI system demonstrated more conservative scoring (mean: 59.95 vs 80.53 human) but generated significantly more detailed technical feedback.",
    "URL": "http://arxiv.org/abs/2601.10093v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.10691",
    "type": "report",
    "title": "The Conversational Exam: A Scalable Assessment Design for the AI Era",
    "author": [
      {
        "family": "Barba",
        "given": "Lorena A."
      },
      {
        "family": "Stegner",
        "given": "Laura"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          15
        ]
      ]
    },
    "abstract": "Traditional assessment methods collapse when students use generative AI to complete work without genuine engagement, creating an illusion of competence where they believe they're learning but aren't. This paper presents the conversational exam -- a scalable oral examination format that restores assessment validity by having students code live while explaining their reasoning. Drawing on human-computer interaction principles, we examined 58 students in small groups across just two days, demonstrating that oral exams can scale to typical class sizes. The format combines authentic practice (students work with documentation and supervised AI access) with inherent validity (real-time performance cannot be faked). We provide detailed implementation guidance to help instructors adapt this approach, offering a practical path forward when many educators feel paralyzed between banning AI entirely or accepting that valid assessment is impossible.",
    "URL": "http://arxiv.org/abs/2601.10691v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.10852",
    "type": "report",
    "title": "Gamifying Cyber Governance: A Virtual Escape Room to Transform Cybersecurity Policy Education",
    "author": [
      {
        "family": "Hasan",
        "given": "Khondokar Fida"
      },
      {
        "family": "Hughes",
        "given": "William"
      },
      {
        "family": "Rahman",
        "given": "Adrita"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          15
        ]
      ]
    },
    "abstract": "Serious games are gaining popularity as effective teaching and learning tools, providing engaging, interactive, and practical experiences for students. Gamified learning experiences, such as virtual escape rooms, have emerged as powerful tools in bridging theory and practice, fostering deeper understanding and engagement among students. This paper presents the design, implementation, and evaluation of a virtual escape room tailored specifically for cybersecurity governance and policy education. Developed as a 3D immersive environment, the escape room simulates a virtual company scenario to facilitate risk-informed cyber policy development. It consists of three interactive zones, each offering distinct sets of scenario-based problems that target specific educational objectives. Through these zones, students analyze cybersecurity risks, match security frameworks, and draft appropriate policies, thereby developing critical thinking, decision-making skills, and practical cybersecurity competencies. The primary contribution of this work lies in its innovative integration of game-based learning and immersive technology to create robust, interactive learning materials that are also resilient to generative AI interventions, thereby maintaining academic integrity. Additionally, the escape room provides students with exposure to real-world cybersecurity scenarios in a virtual office environment that meets industry expectations. Results from a student survey indicated strong positive feedback, highlighting significant improvements in students engagement, practical understanding, and enthusiasm toward cybersecurity governance and policy concepts, underscoring the effectiveness and potential of gamification in cybersecurity education.",
    "URL": "http://arxiv.org/abs/2601.10852v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.10983",
    "type": "report",
    "title": "Evaluating 21st-Century Competencies in Postsecondary Curricula with Large Language Models: Performance Benchmarking and Reasoning-Based Prompting Strategies",
    "author": [
      {
        "family": "Xu",
        "given": "Zhen"
      },
      {
        "family": "Guan",
        "given": "Xin"
      },
      {
        "family": "Shi",
        "given": "Chenxi"
      },
      {
        "family": "Chen",
        "given": "Qinhao"
      },
      {
        "family": "Yu",
        "given": "Renzhe"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          16
        ]
      ]
    },
    "abstract": "The growing emphasis on 21st-century competencies in postsecondary education, intensified by the transformative impact of generative AI, underscores the need to evaluate how these competencies are embedded in curricula and how effectively academic programs align with evolving workforce and societal demands. Curricular Analytics, particularly recent generative AI-powered approaches, offer a promising data-driven pathway. However, analyzing 21st-century competencies requires pedagogical reasoning beyond surface-level information retrieval, and the capabilities of large language models in this context remain underexplored. In this study, we extend prior curricular analytics research by examining a broader range of curriculum documents, competency frameworks, and models. Using 7,600 manually annotated curriculum-competency alignment scores, we assess the informativeness of different curriculum sources, benchmark general-purpose LLMs for curriculum-to-competency mapping, and analyze error patterns. We further introduce a reasoning-based prompting strategy, Curricular CoT, to strengthen LLMs' pedagogical reasoning. Our results show that detailed instructional activity descriptions are the most informative type of curriculum document for competency analytics. Open-weight LLMs achieve accuracy comparable to proprietary models on coarse-grained tasks, demonstrating their scalability and cost-effectiveness for institutional use. However, no model reaches human-level precision in fine-grained pedagogical reasoning. Our proposed Curricular CoT yields modest improvements by reducing bias in instructional keyword inference and improving the detection of nuanced pedagogical evidence in long text. Together, these findings highlight the untapped potential of institutional curriculum documents and provide an empirical foundation for advancing AI-driven curricular analytics.",
    "URL": "http://arxiv.org/abs/2601.10983v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.10986",
    "type": "report",
    "title": "ZPD Detector: Data Selection via Capability-Difficulty Alignment for Large Language Models",
    "author": [
      {
        "family": "Yang",
        "given": "Bo"
      },
      {
        "family": "Chen",
        "given": "Yunkui"
      },
      {
        "family": "Feng",
        "given": "Lanfei"
      },
      {
        "family": "Zhang",
        "given": "Yu"
      },
      {
        "family": "Li",
        "given": "Shijian"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          16
        ]
      ]
    },
    "abstract": "As the cost of training large language models continues to increase and high-quality training data become increasingly scarce, selecting high-value samples or synthesizing effective training data under limited data budgets has emerged as a critical research problem. Most existing data selection methods rely on static criteria, such as difficulty, uncertainty, or heuristics, and fail to model the evolving relationship between the model and the data. Inspired by the educational theory of the Zone of Proximal Development (ZPD), we propose ZPD Detector, a data selection framework that adopts a bidirectional perspective between models and data by explicitly modeling the alignment between sample difficulty and the model's current capability. ZPD Detector integrates difficulty calibration, model capability estimation based on Item Response Theory (IRT), and a capability-difficulty matching score to dynamically identify the most informative samples at each learning stage, improving data utilization efficiency; moreover, this dynamic matching strategy provides new insights into training strategy design. All code and data will be released after our work be accepted to support reproducible researc",
    "URL": "http://arxiv.org/abs/2601.10986v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.11060",
    "type": "report",
    "title": "Children's Expectations, Engagement, and Evaluation of an LLM-enabled Spherical Visualization Platform in the Classroom",
    "author": [
      {
        "family": "F√§lton",
        "given": "Emelie"
      },
      {
        "family": "Str√∂mstedt",
        "given": "Isabelle"
      },
      {
        "family": "Brossier",
        "given": "Mathis"
      },
      {
        "family": "G√∂ransson",
        "given": "Andreas"
      },
      {
        "family": "Sch√∂nborn",
        "given": "Konrad"
      },
      {
        "family": "Loutfi",
        "given": "Amy"
      },
      {
        "family": "Sunden",
        "given": "Erik"
      },
      {
        "family": "Jawad",
        "given": "Mujtaba Fadhil"
      },
      {
        "family": "Suleiman",
        "given": "Yadgar"
      },
      {
        "family": "Bj√∂rklund",
        "given": "Johanna"
      },
      {
        "family": "Romero",
        "given": "Mario"
      },
      {
        "family": "Ynnerman",
        "given": "Anders"
      },
      {
        "family": "Besan√ßon",
        "given": "Lonni"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          16
        ]
      ]
    },
    "abstract": "We present our first stage results from deploying an LLM-augmented visualization software in a classroom setting to engage primary school children with earth-related datasets. Motivated by the growing interest in conversational AI as a means to support inquiry-based learning, we investigate children's expectations, engagement, and evaluation of a spoken LLM interface with a shared, immersive visualization system in a formal educational context. Our system integrates a speech-capable large language model with an interactive spherical display. It enables children to ask natural-language questions and receive coordinated verbal explanations and visual responses through the LLM-augmented visualization updating in real time based on spoken queries. We report on a classroom study with Swedish children aged 9-10, combining structured observation and small-group discussions to capture expectations prior to interaction, interaction patterns during facilitated sessions, and children's reflections on their encounter afterward. Our results provide empirical insights into children's initial encounters with an LLM-enabled visualization platform within a classroom setting and their expectations, interactions, and evaluations of the system. These findings inform the technology's potential for educational use and highlight important directions for future research.",
    "URL": "http://arxiv.org/abs/2601.11060v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.11643",
    "type": "report",
    "title": "Syllabic Agglutinative Tokenizations for Indonesian LLM: A Study from Gasing Literacy Learning System",
    "author": [
      {
        "family": "Situngkir",
        "given": "H."
      },
      {
        "family": "Lumbantobing",
        "given": "A. B."
      },
      {
        "family": "Surya",
        "given": "Y."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          14
        ]
      ]
    },
    "abstract": "This paper presents a novel syllable-based tokenization approach for Indonesian large language models, inspired by the Gasing Literacy Learning System's pedagogical methodology. Drawing on information-theoretic principles, we develop a tokenization framework that segments Indonesian text at syllable boundaries before applying byte-pair encoding, creating a vocabulary that aligns with the language's morphophonological structure. Our approach first identifies high-frequency syllables through rule-based segmentation, then constructs a compact vocabulary of 3,500 tokens that preserves meaningful linguistic units while maintaining coverage through character-level fallback. Empirical evaluation on Indonesian Wikipedia and folklore corpora from Indonesian Culture Digital Library (PDBI) demonstrates substantial improvements over conventional tokenization methods: the syllable-based approach achieves R√©nyi efficiency of 0.74 compared to 0.50-0.64 for pretrained multilingual tokenizers, while maintaining higher average token lengths (3.67 characters versus 2.72 for GPT-2) despite using a vocabulary an order of magnitude smaller. These gains emerge from the method's ability to internalize character-level dependencies within syllable units, reducing the computational burden on language models while respecting Indonesian's agglutinative morphology. We call the LLM built upon this principle, TOBA LLM (Tokenisasi Optimum Berbasis Aglutinasi), the convergence of human literacy pedagogy with computational optimization principles offers a promising paradigm for developing linguistically-informed tokenization strategies, particularly for morphologically rich and underrepresented languages in natural language processing.",
    "URL": "http://arxiv.org/abs/2601.11643v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.11650",
    "type": "report",
    "title": "Large Language Model Agent for User-friendly Chemical Process Simulations",
    "author": [
      {
        "family": "Liang",
        "given": "Jingkang"
      },
      {
        "family": "Groll",
        "given": "Niklas"
      },
      {
        "family": "Sin",
        "given": "G√ºrkan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          15
        ]
      ]
    },
    "abstract": "Modern process simulators enable detailed process design, simulation, and optimization; however, constructing and interpreting simulations is time-consuming and requires expert knowledge. This limits early exploration by inexperienced users. To address this, a large language model (LLM) agent is integrated with AVEVA Process Simulation (APS) via Model Context Protocol (MCP), allowing natural language interaction with rigorous process simulations. An MCP server toolset enables the LLM to communicate programmatically with APS using Python, allowing it to execute complex simulation tasks from plain-language instructions. Two water-methanol separation case studies assess the framework across different task complexities and interaction modes. The first shows the agent autonomously analyzing flowsheets, finding improvement opportunities, and iteratively optimizing, extracting data, and presenting results clearly. The framework benefits both educational purposes, by translating technical concepts and demonstrating workflows, and experienced practitioners by automating data extraction, speeding routine tasks, and supporting brainstorming. The second case study assesses autonomous flowsheet synthesis through both a step-by-step dialogue and a single prompt, demonstrating its potential for novices and experts alike. The step-by-step mode gives reliable, guided construction suitable for educational contexts; the single-prompt mode constructs fast baseline flowsheets for later refinement. While current limitations such as oversimplification, calculation errors, and technical hiccups mean expert oversight is still needed, the framework's capabilities in analysis, optimization, and guided construction suggest LLM-based agents can become valuable collaborators.",
    "URL": "http://arxiv.org/abs/2601.11650v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.11835",
    "type": "report",
    "title": "Changes in Coding Behavior and Performance Since the Introduction of LLMs",
    "author": [
      {
        "family": "Zhang",
        "given": "Yufan"
      },
      {
        "family": "Savelka",
        "given": "Jaromir"
      },
      {
        "family": "Goldstein",
        "given": "Seth Copen"
      },
      {
        "family": "Conway",
        "given": "Michael"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          16
        ]
      ]
    },
    "abstract": "The widespread availability of large language models (LLMs) has changed how students engage with coding and problem-solving. While these tools may increase student productivity, they also make it more difficult for instructors to assess students' learning and effort. In this quasi-longitudinal study, we analyze five years of student source code submissions in a graduate-level cloud computing course, focusing on an assignment that remained unchanged and examining students' behavior during the period spanning five semesters before the release of ChatGPT and five semesters after.\n  Student coding behavior has changed significantly since Fall 2022. The length of their final submissions increased. Between consecutive submissions, average edit distances increased while average score improvement decreased, suggesting that both student productivity and learning have decreased after ChatGPT's release. Additionally, there are statistically significant correlations between these behavioral changes and their overall performance. Although we cannot definitively attribute them to LLM misuse, they are consistent with our hypothesis that some students are over-reliant on LLMs, which is negatively affecting their learning outcomes. Our findings raise an alarm around the first generation of graduates in the age of LLMs, calling upon both educators and employers to reflect on their evaluation methods for genuine expertise and productivity.",
    "DOI": "10.1145/3785022.3785075",
    "URL": "https://doi.org/10.1145/3785022.3785075",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.11920",
    "type": "report",
    "title": "Enhancing LLM-Based Data Annotation with Error Decomposition",
    "author": [
      {
        "family": "Xu",
        "given": "Zhen"
      },
      {
        "family": "Khatri",
        "given": "Vedant"
      },
      {
        "family": "Dai",
        "given": "Yijun"
      },
      {
        "family": "Liu",
        "given": "Xiner"
      },
      {
        "family": "Li",
        "given": "Siyan"
      },
      {
        "family": "Zhang",
        "given": "Xuanming"
      },
      {
        "family": "Yu",
        "given": "Renzhe"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          17
        ]
      ]
    },
    "abstract": "Large language models offer a scalable alternative to human coding for data annotation tasks, enabling the scale-up of research across data-intensive domains. While LLMs are already achieving near-human accuracy on objective annotation tasks, their performance on subjective annotation tasks, such as those involving psychological constructs, is less consistent and more prone to errors. Standard evaluation practices typically collapse all annotation errors into a single alignment metric, but this simplified approach may obscure different kinds of errors that affect final analytical conclusions in different ways. Here, we propose a diagnostic evaluation paradigm that incorporates a human-in-the-loop step to separate task-inherent ambiguity from model-driven inaccuracies and assess annotation quality in terms of their potential downstream impacts. We refine this paradigm on ordinal annotation tasks, which are common in subjective annotation. The refined paradigm includes: (1) a diagnostic taxonomy that categorizes LLM annotation errors along two dimensions: source (model-specific vs. task-inherent) and type (boundary ambiguity vs. conceptual misidentification); (2) a lightweight human annotation test to estimate task-inherent ambiguity from LLM annotations; and (3) a computational method to decompose observed LLM annotation errors following our taxonomy. We validate this paradigm on four educational annotation tasks, demonstrating both its conceptual validity and practical utility. Theoretically, our work provides empirical evidence for why excessively high alignment is unrealistic in specific annotation tasks and why single alignment metrics inadequately reflect the quality of LLM annotations. In practice, our paradigm can be a low-cost diagnostic tool that assesses the suitability of a given task for LLM annotation and provides actionable insights for further technical optimization.",
    "URL": "http://arxiv.org/abs/2601.11920v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.12061",
    "type": "report",
    "title": "Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs Annotation: LLM-Assisted and Gold-Label-Free Evaluation",
    "author": [
      {
        "family": "Lee",
        "given": "Jinsook"
      },
      {
        "family": "Vanacore",
        "given": "Kirk"
      },
      {
        "family": "Zhou",
        "given": "Zhuqian"
      },
      {
        "family": "Ahtisham",
        "given": "Bakhtawar"
      },
      {
        "family": "Grutter",
        "given": "Jeanine"
      },
      {
        "family": "Kizilcec",
        "given": "Rene F."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          17
        ]
      ]
    },
    "abstract": "Dialogue Act (DA) annotation typically treats communicative or pedagogical intent as localized to individual utterances or turns. This leads annotators to agree on the underlying action while disagreeing on segment boundaries, reducing apparent reliability. We propose codebook-injected segmentation, which conditions boundary decisions on downstream annotation criteria, and evaluate LLM-based segmenters against standard and retrieval-augmented baselines. To assess these without gold labels, we introduce evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. We found DA-awareness produces segments that are internally more consistent than text-only baselines. While LLMs excel at creating construct-consistent spans, coherence-based baselines remain superior at detecting global shifts in dialogue flow. Across two datasets, no single segmenter dominates. Improvements in within-segment coherence frequently trade off against boundary distinctiveness and human-AI distributional agreement. These results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score.",
    "URL": "http://arxiv.org/abs/2601.12061v2",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.12131",
    "type": "report",
    "title": "SolarGPT-QA: A Domain-Adaptive Large Language Model for Educational Question Answering in Space Weather and Heliophysics",
    "author": [
      {
        "family": "Chapagain",
        "given": "Santosh"
      },
      {
        "family": "EskandariNasab",
        "given": "MohammadReza"
      },
      {
        "family": "Vural",
        "given": "Onur"
      },
      {
        "family": "Hamdi",
        "given": "Shah Muhammad"
      },
      {
        "family": "Boubrahimi",
        "given": "Soukaina Filali"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          17
        ]
      ]
    },
    "abstract": "Solar activity, including solar flares, coronal mass ejections (CMEs), and geomagnetic storms, can significantly impact satellites, aviation, power grids, data centers, and space missions. Extreme solar events can cause substantial economic damage if not predicted in advance, highlighting the importance of accurate forecasting and effective education in space science. Although large language models (LLMs) perform well on general tasks, they often lack domain-specific knowledge and pedagogical capability to clearly explain complex space science concepts.\n  We introduce SolarGPT-QA, a question answering system based on a domain-adapted large language model built on the LLaMA-3 base model. The model is trained using scientific literature and large-scale question-answer data generated with GPT-4 and refined using Grok-3 in a student-friendly storytelling style. Human pairwise evaluations show that SolarGPT-QA outperforms general-purpose models in zero-shot settings and achieves competitive performance compared to instruction-tuned models for educational explanations in space weather and heliophysics. A small pilot student comprehension study further suggests improved clarity and accessibility of the generated explanations. Ablation experiments indicate that combining domain-adaptive pretraining with pedagogical fine-tuning is important for balancing scientific accuracy and educational effectiveness. This work represents an initial step toward a broader SolarGPT framework for space science education and forecasting.",
    "URL": "http://arxiv.org/abs/2601.12131v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.12648",
    "type": "report",
    "title": "Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?",
    "author": [
      {
        "family": "Khan",
        "given": "Nafiz Imtiaz"
      },
      {
        "family": "Cleland",
        "given": "Kylie"
      },
      {
        "family": "Filkov",
        "given": "Vladimir"
      },
      {
        "family": "Goldman",
        "given": "Roger Eric"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          19
        ]
      ]
    },
    "abstract": "Procedural case logs are a core requirement in radiology training, yet they are time-consuming to complete and prone to inconsistency when authored manually. This study investigates whether large language models (LLMs) can automate procedural case log documentation directly from free-text radiology reports. We evaluate multiple local and commercial LLMs under instruction-based and chain-of-thought prompting to extract structured procedural information from 414 curated interventional radiology reports authored by nine residents between 2018 and 2024. Model performance is assessed using sensitivity, specificity, and F1-score, alongside inference latency and token efficiency to estimate operational cost. Results show that both local and commercial models achieve strong extraction performance, with best F1-scores approaching 0.87, while exhibiting different trade-offs between speed and cost. Automation using LLMs has the potential to substantially reduce clerical burden for trainees and improve consistency in case logging. These findings demonstrate the feasibility of AI-assisted documentation in medical education and highlight the need for further validation across institutions and clinical workflows.",
    "URL": "http://arxiv.org/abs/2601.12648v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.12718",
    "type": "report",
    "title": "Dataset of GenAI-Assisted Information Problem Solving in Education",
    "author": [
      {
        "family": "Li",
        "given": "Xinyu"
      },
      {
        "family": "Yang",
        "given": "Kaixun"
      },
      {
        "family": "Wei",
        "given": "Jiameng"
      },
      {
        "family": "Cheng",
        "given": "Yixin"
      },
      {
        "family": "Ga≈°eviƒá",
        "given": "Dragan"
      },
      {
        "family": "Chen",
        "given": "Guanliang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          19
        ]
      ]
    },
    "abstract": "Information Problem Solving (IPS) is a critical competency for academic and professional success in education, work, and life. The advent of Generative Artificial Intelligence (GenAI), particularly tools like ChatGPT, has introduced new possibilities for supporting students in complex IPS tasks. However, empirical insights into how students engage with GenAI during IPS and how these tools can be effectively leveraged for learning remain limited. Moreover, differences in background, shaped by cultural and socioeconomic factors, pose additional challenges to the equitable integration of GenAI in educational contexts. To address this gap, we present an open-source dataset collected from 279 students at a public Australian university. The dataset was generated through students' use of FLoRA, a GenAI-powered educational platform that widely adopted in the field of learning analytics. Within FLoRA, students interacted with an embedded GenAI chatbot to gather information and synthesize it into data science project proposals. The dataset captures fine-grained, multi-dimensional records of GenAI-assisted IPS processes, including: (i) student-GenAI dialogue transcripts; (ii) writing process log traces; (iii) final project proposals with human-assigned assessment scores; (iv) surveys of biographic and prior knowledge in data science and AI; and (v) surveys capturing students' GenAI experience and perceptions of GenAI's effectiveness in supporting IPS. This dataset provides a valuable resource for advancing our understanding of GenAI's role in educational IPS and informing the design of adaptive, inclusive AI-powered learning tools.",
    "URL": "http://arxiv.org/abs/2601.12718v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.12762",
    "type": "report",
    "title": "Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction",
    "author": [
      {
        "family": "Gao",
        "given": "Xingjie"
      },
      {
        "family": "Huang",
        "given": "Pengcheng"
      },
      {
        "family": "Liu",
        "given": "Zhenghao"
      },
      {
        "family": "Yan",
        "given": "Yukun"
      },
      {
        "family": "Wang",
        "given": "Shuo"
      },
      {
        "family": "Chen",
        "given": "Zulong"
      },
      {
        "family": "Qian",
        "given": "Chen"
      },
      {
        "family": "Yu",
        "given": "Ge"
      },
      {
        "family": "Gu",
        "given": "Yu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          19
        ]
      ]
    },
    "abstract": "Equipping Large Language Models (LLMs) with external tools enables them to solve complex real-world problems. However, the robustness of existing methods remains a critical challenge when confronting novel or evolving tools. Existing trajectory-centric paradigms primarily rely on memorizing static solution paths during training, which limits the ability of LLMs to generalize tool usage to newly introduced or previously unseen tools. In this paper, we propose ToolMaster, a framework that shifts tool use from imitating golden tool-calling trajectories to actively learning tool usage through interaction with the environment. To optimize LLMs for tool planning and invocation, ToolMaster adopts a trial-and-execution paradigm, which trains LLMs to first imitate teacher-generated trajectories containing explicit tool trials and self-correction, followed by reinforcement learning to coordinate the trial and execution phases jointly. This process enables agents to autonomously explore correct tool usage by actively interacting with environments and forming experiential knowledge that benefits tool execution. Experimental results demonstrate that ToolMaster significantly outperforms existing baselines in terms of generalization and robustness across unseen or unfamiliar tools. All code and data are available at https://github.com/NEUIR/ToolMaster.",
    "URL": "http://arxiv.org/abs/2601.12762v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.13099",
    "type": "report",
    "title": "Alexandria: A Multi-Domain Dialectal Arabic Machine Translation Dataset for Culturally Inclusive and Linguistically Diverse LLMs",
    "author": [
      {
        "family": "Mekki",
        "given": "Abdellah El"
      },
      {
        "family": "Magdy",
        "given": "Samar M."
      },
      {
        "family": "Atou",
        "given": "Houdaifa"
      },
      {
        "family": "AbuHweidi",
        "given": "Ruwa"
      },
      {
        "family": "Qawasmeh",
        "given": "Baraah"
      },
      {
        "family": "Nacar",
        "given": "Omer"
      },
      {
        "family": "Al-hibiri",
        "given": "Thikra"
      },
      {
        "family": "Saadie",
        "given": "Razan"
      },
      {
        "family": "Alsayadi",
        "given": "Hamzah"
      },
      {
        "family": "Hammouda",
        "given": "Nadia Ghezaiel"
      },
      {
        "family": "Alkhazimi",
        "given": "Alshima"
      },
      {
        "family": "Hamod",
        "given": "Aya"
      },
      {
        "family": "Al-Ghafri",
        "given": "Al-Yas"
      },
      {
        "family": "El-Sayed",
        "given": "Wesam"
      },
      {
        "family": "sharji",
        "given": "Asila Al"
      },
      {
        "family": "Ballout",
        "given": "Mohamad"
      },
      {
        "family": "Belfathi",
        "given": "Anas"
      },
      {
        "family": "Ghaddar",
        "given": "Karim"
      },
      {
        "family": "Sibaee",
        "given": "Serry"
      },
      {
        "family": "Aoun",
        "given": "Alaa"
      },
      {
        "family": "Asiri",
        "given": "Areej"
      },
      {
        "family": "Abureesh",
        "given": "Lina"
      },
      {
        "family": "Bashiti",
        "given": "Ahlam"
      },
      {
        "family": "Yousef",
        "given": "Majdal"
      },
      {
        "family": "Hafiz",
        "given": "Abdulaziz"
      },
      {
        "family": "Mohamed",
        "given": "Yehdih"
      },
      {
        "family": "Hamedtou",
        "given": "Emira"
      },
      {
        "family": "Brahim",
        "given": "Brakehe"
      },
      {
        "family": "Alhamouri",
        "given": "Rahaf"
      },
      {
        "family": "Nafea",
        "given": "Youssef"
      },
      {
        "family": "Aatar",
        "given": "Aya El"
      },
      {
        "family": "Al-Dhabyani",
        "given": "Walid"
      },
      {
        "family": "Hamed",
        "given": "Emhemed"
      },
      {
        "family": "Shatnawi",
        "given": "Sara"
      },
      {
        "family": "Alwajih",
        "given": "Fakhraddin"
      },
      {
        "family": "Elkhidir",
        "given": "Khalid"
      },
      {
        "family": "Alasmari",
        "given": "Ashwag"
      },
      {
        "family": "Gerrio",
        "given": "Abdurrahman"
      },
      {
        "family": "Alshahri",
        "given": "Omar"
      },
      {
        "family": "Elmadany",
        "given": "AbdelRahim A."
      },
      {
        "family": "Berrada",
        "given": "Ismail"
      },
      {
        "family": "Alkathiri",
        "given": "Amir Azad Adli"
      },
      {
        "family": "Zaraket",
        "given": "Fadi A"
      },
      {
        "family": "Jarrar",
        "given": "Mustafa"
      },
      {
        "family": "Hadj",
        "given": "Yahya Mohamed El"
      },
      {
        "family": "Alhuzali",
        "given": "Hassan"
      },
      {
        "family": "Abdul-Mageed",
        "given": "Muhammad"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          19
        ]
      ]
    },
    "abstract": "Arabic is a highly diglossic language where most daily communication occurs in regional dialects rather than Modern Standard Arabic. Despite this, machine translation (MT) systems often generalize poorly to dialectal input, limiting their utility for millions of speakers. We introduce \\textbf{Alexandria}, a large-scale, community-driven, human-translated dataset designed to bridge this gap. Alexandria covers 13 Arab countries and 11 high-impact domains, including health, education, and agriculture. Unlike previous resources, Alexandria provides unprecedented granularity by associating contributions with city-of-origin metadata, capturing authentic local varieties beyond coarse regional labels. The dataset consists of multi-turn conversational scenarios annotated with speaker-addressee gender configurations, enabling the study of gender-conditioned variation in dialectal use. Comprising 107K total samples, Alexandria serves as both a training resource and a rigorous benchmark for evaluating MT and Large Language Models (LLMs). Our automatic and human evaluation of Arabic-aware LLMs benchmarks current capabilities in translating across diverse Arabic dialects and sub-dialects, while exposing significant persistent challenges.",
    "URL": "http://arxiv.org/abs/2601.13099v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.13118",
    "type": "report",
    "title": "Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization",
    "author": [
      {
        "family": "Midolo",
        "given": "Alessandro"
      },
      {
        "family": "Giagnorio",
        "given": "Alessandro"
      },
      {
        "family": "Zampetti",
        "given": "Fiorella"
      },
      {
        "family": "Tufano",
        "given": "Rosalia"
      },
      {
        "family": "Bavota",
        "given": "Gabriele"
      },
      {
        "family": "Penta",
        "given": "Massimiliano Di"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          19
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) are nowadays extensively used for various types of software engineering tasks, primarily code generation. Previous research has shown how suitable prompt engineering could help developers in improving their code generation prompts. However, so far, there do not exist specific guidelines driving developers towards writing suitable prompts for code generation. In this work, we derive and evaluate development-specific prompt optimization guidelines. First, we use an iterative, test-driven approach to automatically refine code generation prompts, and we analyze the outcome of this process to identify prompt improvement items that lead to test passes. We use such elements to elicit 10 guidelines for prompt improvement, related to better specifying I/O, pre-post conditions, providing examples, various types of details, or clarifying ambiguities. We conduct an assessment with 50 practitioners, who report their usage of the elicited prompt improvement patterns, as well as their perceived usefulness, which does not always correspond to the actual usage before knowing our guidelines. Our results lead to implications not only for practitioners and educators, but also for those aimed at creating better LLM-aided software development tools.",
    "URL": "http://arxiv.org/abs/2601.13118v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.13262",
    "type": "report",
    "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning",
    "author": [
      {
        "family": "Onyame",
        "given": "Eric"
      },
      {
        "family": "Ghosh",
        "given": "Akash"
      },
      {
        "family": "Baidya",
        "given": "Subhadip"
      },
      {
        "family": "Saha",
        "given": "Sriparna"
      },
      {
        "family": "Chen",
        "given": "Xiuying"
      },
      {
        "family": "Agarwal",
        "given": "Chirag"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          19
        ]
      ]
    },
    "abstract": "While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/",
    "URL": "http://arxiv.org/abs/2601.13262v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.13406",
    "type": "report",
    "title": "Integrating Virtual Reality and Large Language Models for Team-Based Non-Technical Skills Training and Evaluation in the Operating Room",
    "author": [
      {
        "family": "Barker",
        "given": "Jacob"
      },
      {
        "family": "Demirel",
        "given": "Doga"
      },
      {
        "family": "Jackson",
        "given": "Cullen"
      },
      {
        "family": "Johansson",
        "given": "Anna"
      },
      {
        "family": "Miraglia",
        "given": "Robbin"
      },
      {
        "family": "Hoagland",
        "given": "Darian"
      },
      {
        "family": "Jones",
        "given": "Stephanie B."
      },
      {
        "family": "Mitchell",
        "given": "John"
      },
      {
        "family": "Jones",
        "given": "Daniel B."
      },
      {
        "family": "De",
        "given": "Suvranu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          19
        ]
      ]
    },
    "abstract": "Although effective teamwork and communication are critical to surgical safety, structured training for non-technical skills (NTS) remains limited compared with technical simulation. The ACS/APDS Phase III Team-Based Skills Curriculum calls for scalable tools that both teach and objectively assess these competencies during laparoscopic emergencies. We introduce the Virtual Operating Room Team Experience (VORTeX), a multi-user virtual reality (VR) platform that integrates immersive team simulation with large language model (LLM) analytics to train and evaluate communication, decision-making, teamwork, and leadership. Team dialogue is analyzed using structured prompts derived from the Non-Technical Skills for Surgeons (NOTSS) framework, enabling automated classification of behaviors and generation of directed interaction graphs that quantify communication structure and hierarchy. Two laparoscopic emergency scenarios, pneumothorax and intra-abdominal bleeding, were implemented to elicit realistic stress and collaboration. Twelve surgical professionals completed pilot sessions at the 2024 SAGES conference, rating VORTeX as intuitive, immersive, and valuable for developing teamwork and communication. The LLM consistently produced interpretable communication networks reflecting expected operative hierarchies, with surgeons as central integrators, nurses as initiators, and anesthesiologists as balanced intermediaries. By integrating immersive VR with LLM-driven behavioral analytics, VORTeX provides a scalable, privacy-compliant framework for objective assessment and automated, data-informed debriefing across distributed training environments.",
    "URL": "http://arxiv.org/abs/2601.13406v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.13443",
    "type": "report",
    "title": "Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models",
    "author": [
      {
        "family": "Manzanilla-Granados",
        "given": "H√©ctor Manuel"
      },
      {
        "family": "Navarrete-Cazales",
        "given": "Zaira"
      },
      {
        "family": "Pescador-Rojas",
        "given": "Miriam"
      },
      {
        "family": "Ram√≠rez-Romero",
        "given": "Tonahtiu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          19
        ]
      ]
    },
    "abstract": "The rapid adoption of large language models (LLMs) has enabled new forms of AI-assisted reasoning across scientific, technical, and organizational domains. However, prevailing modes of LLM use remain cognitively unstructured: problem framing, knowledge exploration, retrieval, methodological awareness, and explanation are typically collapsed into a single generative process. This cognitive collapse limits traceability, weakens epistemic control, and undermines reproducibility, particularly in high-responsibility settings.\n  We introduce Explicit Cognitive Allocation, a general principle for structuring AI-assisted inference through the explicit separation and orchestration of epistemic functions. We instantiate this principle in the Cognitive Universal Agent (CUA), an architecture that organizes inference into distinct stages of exploration and framing, epistemic anchoring, instrumental and methodological mapping, and interpretive synthesis. Central to this framework is the notion of Universal Cognitive Instruments (UCIs), which formalize heterogeneous means, including computational, experimental, organizational, regulatory, and educational instruments, through which abstract inquiries become investigable.\n  We evaluate the effects of explicit cognitive and instrumental allocation through controlled comparisons between CUA-orchestrated inference and baseline LLM inference under matched execution conditions. Across multiple prompts in the agricultural domain, CUA inference exhibits earlier and structurally governed epistemic convergence, higher epistemic alignment under semantic expansion, and systematic exposure of the instrumental landscape of inquiry. In contrast, baseline LLM inference shows greater variability in alignment and fails to explicitly surface instrumental structure.",
    "URL": "http://arxiv.org/abs/2601.13443v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.13479",
    "type": "report",
    "title": "Exploring Learners' Expectations and Engagement When Collaborating with Constructively Controversial Peer Agents",
    "author": [
      {
        "family": "Tanprasert",
        "given": "Thitaree"
      },
      {
        "family": "Kim",
        "given": "Young-ho"
      },
      {
        "family": "Fels",
        "given": "Sidney"
      },
      {
        "family": "Yoon",
        "given": "Dongwook"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          20
        ]
      ]
    },
    "abstract": "Peer agents can supplement real-time collaborative learning in asynchronous online courses. Constructive Controversy (CC) theory suggests that humans deepen their understanding of a topic by confronting and resolving controversies. This study explores whether CC's benefits apply to LLM-based peer agents, focusing on the impact of agents' disputatious behaviors and disclosure of agents' behavior designs on the learning process. In our mixed-method study (n=144), we compare LLMs that follow detailed CC guidelines (regulated) to those guided by broader goals (unregulated) and examine the effects of disclosing the agents' design to users (transparent vs. opaque). Findings show that learners' values influence their agent interaction: those valuing control appreciate unregulated agents' willingness to cease push-back upon request, while those valuing intellectual challenges favor regulated agents for stimulating creativity. Additionally, design transparency lowers learners' perception of agents' abilities. Our findings lay the foundation for designing effective collaborative peer agents in isolated educational settings.",
    "URL": "http://arxiv.org/abs/2601.13479v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.13614",
    "type": "report",
    "title": "CauScientist: Teaching LLMs to Respect Data for Causal Discovery",
    "author": [
      {
        "family": "Peng",
        "given": "Bo"
      },
      {
        "family": "Chen",
        "given": "Sirui"
      },
      {
        "family": "Xu",
        "given": "Lei"
      },
      {
        "family": "Lu",
        "given": "Chaochao"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          20
        ]
      ]
    },
    "abstract": "Causal discovery is fundamental to scientific understanding and reliable decision-making. Existing approaches face critical limitations: purely data-driven methods suffer from statistical indistinguishability and modeling assumptions, while recent LLM-based methods either ignore statistical evidence or incorporate unverified priors that can mislead result. To this end, we propose CauScientist, a collaborative framework that synergizes LLMs as hypothesis-generating \"data scientists\" with probabilistic statistics as rigorous \"verifiers\". CauScientist employs hybrid initialization to select superior starting graphs, iteratively refines structures through LLM-proposed modifications validated by statistical criteria, and maintains error memory to guide efficient search space. Experiments demonstrate that CauScientist substantially outperforms purely data-driven baselines, achieving up to 53.8% F1 score improvement and enhancing recall from 35.0% to 100.0%. Notably, while standalone LLM performance degrades with graph complexity, CauScientist reduces structural hamming distance (SHD) by 44.0% compared to Qwen3-32B on 37-node graphs. Our project page is at https://github.com/OpenCausaLab/CauScientist.",
    "URL": "http://arxiv.org/abs/2601.13614v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.13761",
    "type": "report",
    "title": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution",
    "author": [
      {
        "family": "Fan",
        "given": "Shengda"
      },
      {
        "family": "Ye",
        "given": "Xuyan"
      },
      {
        "family": "Lin",
        "given": "Yankai"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          20
        ]
      ]
    },
    "abstract": "Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations. The code is available at https://github.com/RUCBM/DARC.",
    "URL": "http://arxiv.org/abs/2601.13761v2",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.13815",
    "type": "report",
    "title": "From RTL to Prompt Coding: Empowering the Next Generation of Chip Designers through LLMs",
    "author": [
      {
        "family": "Krupp",
        "given": "Lukas"
      },
      {
        "family": "Venn",
        "given": "Matthew"
      },
      {
        "family": "Wehn",
        "given": "Norbert"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          20
        ]
      ]
    },
    "abstract": "This paper presents an LLM-based learning platform for chip design education, aiming to make chip design accessible to beginners without overwhelming them with technical complexity. It represents the first educational platform that assists learners holistically across both frontend and backend design. The proposed approach integrates an LLM-based chat agent into a browser-based workflow built upon the Tiny Tapeout ecosystem. The workflow guides users from an initial design idea through RTL code generation to a tapeout-ready chip. To evaluate the concept, a case study was conducted with 18 high-school students. Within a 90-minute session they developed eight functional VGA chip designs in a 130 nm technology. Despite having no prior experience in chip design, all groups successfully implemented tapeout-ready projects. The results demonstrate the feasibility and educational impact of LLM-assisted chip design, highlighting its potential to attract and inspire early learners and significantly broaden the target audience for the field.",
    "URL": "http://arxiv.org/abs/2601.13815v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.13876",
    "type": "report",
    "title": "Pedagogical Alignment for Vision-Language-Action Models: A Comprehensive Framework for Data, Architecture, and Evaluation in Education",
    "author": [
      {
        "family": "Lee",
        "given": "Unggi"
      },
      {
        "family": "Jeong",
        "given": "Jahyun"
      },
      {
        "family": "Shin",
        "given": "Sunyoung"
      },
      {
        "family": "Park",
        "given": "Haeun"
      },
      {
        "family": "Moon",
        "given": "Jeongsu"
      },
      {
        "family": "Song",
        "given": "Youngchang"
      },
      {
        "family": "Shim",
        "given": "Jaechang"
      },
      {
        "family": "Lee",
        "given": "JaeHwan"
      },
      {
        "family": "Noh",
        "given": "Yunju"
      },
      {
        "family": "Choi",
        "given": "Seungwon"
      },
      {
        "family": "Kim",
        "given": "Ahhyun"
      },
      {
        "family": "Kim",
        "given": "TaeHyeon"
      },
      {
        "family": "Joo",
        "given": "Kyungtae"
      },
      {
        "family": "Kim",
        "given": "Taeyeong"
      },
      {
        "family": "Lee",
        "given": "Gyeonggeon"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          20
        ]
      ]
    },
    "abstract": "Science demonstrations are important for effective STEM education, yet teachers face challenges in conducting them safely and consistently across multiple occasions, where robotics can be helpful. However, current Vision-Language-Action (VLA) models require substantial computational resources and sacrifice language generation capabilities to maximize efficiency, making them unsuitable for resource-constrained educational settings that require interpretable, explanation-generating systems. We present \\textit{Pedagogical VLA Framework}, a framework that applies pedagogical alignment to lightweight VLA models through four components: text healing to restore language generation capabilities, large language model (LLM) distillation to transfer pedagogical knowledge, safety training for educational environments, and pedagogical evaluation adjusted to science education contexts. We evaluate Pedagogical VLA Framework across five science demonstrations spanning physics, chemistry, biology, and earth science, using an evaluation framework developed in collaboration with science education experts. Our evaluation assesses both task performance (success rate, protocol compliance, efficiency, safety) and pedagogical quality through teacher surveys and LLM-as-Judge assessment. We additionally provide qualitative analysis of generated texts. Experimental results demonstrate that Pedagogical VLA Framework achieves comparable task performance to baseline models while producing contextually appropriate educational explanations.",
    "URL": "http://arxiv.org/abs/2601.13876v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.13882",
    "type": "report",
    "title": "OpenLearnLM Benchmark: A Unified Framework for Evaluating Knowledge, Skill, and Attitude in Educational Large Language Models",
    "author": [
      {
        "family": "Lee",
        "given": "Unggi"
      },
      {
        "family": "Lee",
        "given": "Sookbun"
      },
      {
        "family": "Choi",
        "given": "Heungsoo"
      },
      {
        "family": "Lee",
        "given": "Jinseo"
      },
      {
        "family": "Park",
        "given": "Haeun"
      },
      {
        "family": "Jeon",
        "given": "Younghoon"
      },
      {
        "family": "Cho",
        "given": "Sungmin"
      },
      {
        "family": "Kang",
        "given": "Minju"
      },
      {
        "family": "Koh",
        "given": "Junbo"
      },
      {
        "family": "Bae",
        "given": "Jiyeong"
      },
      {
        "family": "Nam",
        "given": "Minwoo"
      },
      {
        "family": "Eun",
        "given": "Juyeon"
      },
      {
        "family": "Jung",
        "given": "Yeonji"
      },
      {
        "family": "Jeong",
        "given": "Yeil"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          20
        ]
      ]
    },
    "abstract": "Large Language Models are increasingly deployed as educational tools, yet existing benchmarks focus on narrow skills and lack grounding in learning sciences. We introduce OpenLearnLM Benchmark, a theory-grounded framework evaluating LLMs across three dimensions derived from educational assessment theory: Knowledge (curriculum-aligned content and pedagogical understanding), Skills (scenario-based competencies organized through a four-level center-role-scenario-subscenario hierarchy), and Attitude (alignment consistency and deception resistance). Our benchmark comprises 124K+ items spanning multiple subjects, educational roles, and difficulty levels based on Bloom's taxonomy. The Knowledge domain prioritizes authentic assessment items from established benchmarks, while the Attitude domain adapts Anthropic's Alignment Faking methodology to detect behavioral inconsistency under varying monitoring conditions. Evaluation of seven frontier models reveals distinct capability profiles: Claude-Opus-4.5 excels in practical skills despite lower content knowledge, while Grok-4.1-fast leads in knowledge but shows alignment concerns. Notably, no single model dominates all dimensions, validating the necessity of multi-axis evaluation. OpenLearnLM provides an open, comprehensive framework for advancing LLM readiness in authentic educational contexts.",
    "URL": "http://arxiv.org/abs/2601.13882v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.14249",
    "type": "report",
    "title": "Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment",
    "author": [
      {
        "family": "Yang",
        "given": "Yuming"
      },
      {
        "family": "Lai",
        "given": "Mingyoung"
      },
      {
        "family": "Zhao",
        "given": "Wanxu"
      },
      {
        "family": "Fan",
        "given": "Xiaoran"
      },
      {
        "family": "Xi",
        "given": "Zhiheng"
      },
      {
        "family": "Wu",
        "given": "Mingqi"
      },
      {
        "family": "Huang",
        "given": "Chiyue"
      },
      {
        "family": "Zhao",
        "given": "Jun"
      },
      {
        "family": "Lv",
        "given": "Haijun"
      },
      {
        "family": "Tong",
        "given": "Jian"
      },
      {
        "family": "Zhou",
        "given": "Yunhua"
      },
      {
        "family": "Zou",
        "given": "Yicheng"
      },
      {
        "family": "Guo",
        "given": "Qipeng"
      },
      {
        "family": "Gui",
        "given": "Tao"
      },
      {
        "family": "Zhang",
        "given": "Qi"
      },
      {
        "family": "Huang",
        "given": "Xuanjing"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          20
        ]
      ]
    },
    "abstract": "Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory's average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection.",
    "URL": "http://arxiv.org/abs/2601.14249v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.14479",
    "type": "report",
    "title": "Can LLM Reasoning Be Trusted? A Comparative Study: Using Human Benchmarking on Statistical Tasks",
    "author": [
      {
        "family": "Nagarkar",
        "given": "Crish"
      },
      {
        "family": "Bogachev",
        "given": "Leonid"
      },
      {
        "family": "Sharoff",
        "given": "Serge"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          20
        ]
      ]
    },
    "abstract": "This paper investigates the ability of large language models (LLMs) to solve statistical tasks, as well as their capacity to assess the quality of reasoning. While state-of-the-art LLMs have demonstrated remarkable performance in a range of NLP tasks, their competence in addressing even moderately complex statistical challenges is not well understood. We have fine-tuned selected open-source LLMs on a specially developed dataset to enhance their statistical reasoning capabilities, and compared their performance with the human scores used as a benchmark. Our results show that the fine-tuned models achieve better performance on advanced statistical tasks on the level comparable to a statistics student. Fine-tuning demonstrates architecture-dependent improvements, with some models showing significant performance gains, indicating clear potential for deployment in educational technology and statistical analysis assistance systems. We also show that LLMs themselves can be far better judges of the answers quality (including explanation and reasoning assessment) in comparison to traditional metrics, such as BLEU or BertScore. This self-evaluation capability enables scalable automated assessment for statistical education platforms and quality assurance in automated analysis tools. Potential applications also include validation tools for research methodology in academic and industry settings, and quality control mechanisms for data analysis workflows.",
    "URL": "http://arxiv.org/abs/2601.14479v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.14506",
    "type": "report",
    "title": "Language, Caste, and Context: Demographic Disparities in AI-Generated Explanations Across Indian and American STEM Educational Systems",
    "author": [
      {
        "family": "Gupta",
        "given": "Amogh"
      },
      {
        "family": "Patil",
        "given": "Niharika"
      },
      {
        "family": "Ghosh",
        "given": "Sourojit"
      },
      {
        "literal": "SnehalKumar"
      },
      {
        "family": "Gaikwad",
        "given": "S"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          20
        ]
      ]
    },
    "abstract": "The popularization of AI chatbot usage globally has created opportunities for research into their benefits and drawbacks, especially for students using AI assistants for coursework support. This paper asks: how do LLMs perceive the intellectual capabilities of student profiles from intersecting marginalized identities across different cultural contexts? We conduct one of the first large-scale intersectional analyses on LLM explanation quality for Indian and American undergraduate profiles preparing for engineering entrance examinations. By constructing profiles combining multiple demographic dimensions including caste, medium of instruction, and school boards in India, and race, HBCU attendance, and school type in America, alongside universal factors like income and college tier, we examine how quality varies across these factors. We observe biases providing lower-quality outputs to profiles with marginalized backgrounds in both contexts. LLMs such as Qwen2.5-32B-Instruct and GPT-4o demonstrate granular understandings of context-specific discrimination, systematically providing simpler explanations to Hindi/Regional-medium students in India and HBCU profiles in America, treating these as proxies for lower capability. Even when marginalized profiles attain social mobility by getting accepted into elite institutions, they still receive more simplistic explanations, showing how demographic information is inextricably linked to LLM biases. Different models (Qwen2.5-32B-Instruct, GPT-4o, GPT-4o-mini, GPT-OSS 20B) embed similar biases against historically marginalized populations in both contexts, preventing profiles from switching between AI assistants for better results. Our findings have strong implications for AI incorporation into global engineering education.",
    "URL": "http://arxiv.org/abs/2601.14506v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.14560",
    "type": "report",
    "title": "Rewarding How Models Think Pedagogically: Integrating Pedagogical Reasoning and Thinking Rewards for LLMs in Education",
    "author": [
      {
        "family": "Lee",
        "given": "Unggi"
      },
      {
        "family": "Bae",
        "given": "Jiyeong"
      },
      {
        "family": "Park",
        "given": "Jaehyeon"
      },
      {
        "family": "Park",
        "given": "Haeun"
      },
      {
        "family": "Park",
        "given": "Taejun"
      },
      {
        "family": "Jeon",
        "given": "Younghoon"
      },
      {
        "family": "Cho",
        "given": "Sungmin"
      },
      {
        "family": "Koh",
        "given": "Junbo"
      },
      {
        "family": "Jeong",
        "given": "Yeil"
      },
      {
        "family": "Lee",
        "given": "Gyeonggeon"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          21
        ]
      ]
    },
    "abstract": "Large language models (LLMs) are increasingly deployed as intelligent tutoring systems, yet research on optimizing LLMs specifically for educational contexts remains limited. Recent works have proposed reinforcement learning approaches for training LLM tutors, but these methods focus solely on optimizing visible responses while neglecting the model's internal thinking process. We introduce PedagogicalRL-Thinking, a framework that extends pedagogical alignment to reasoning LLMs in education through two novel approaches: (1) Pedagogical Reasoning Prompting, which guides internal reasoning using domain-specific educational theory rather than generic instructions; and (2) Thinking Reward, which explicitly evaluates and reinforces the pedagogical quality of the model's reasoning traces. Our experiments reveal that domain-specific, theory-grounded prompting outperforms generic prompting, and that Thinking Reward is most effective when combined with pedagogical prompting. Furthermore, models trained only on mathematics tutoring dialogues show improved performance on educational benchmarks not seen during training, while preserving the base model's factual knowledge. Our quantitative and qualitative analyses reveal that pedagogical thinking reward produces systematic reasoning trace changes, with increased pedagogical reasoning and more structured instructional decision-making in the tutor's thinking process.",
    "URL": "http://arxiv.org/abs/2601.14560v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.14686",
    "type": "report",
    "title": "IB-GRPO: Aligning LLM-based Learning Path Recommendation with Educational Objectives via Indicator-Based Group Relative Policy Optimization",
    "author": [
      {
        "family": "Wang",
        "given": "Shuai"
      },
      {
        "family": "Yang",
        "given": "Yaoming"
      },
      {
        "family": "Li",
        "given": "Bingdong"
      },
      {
        "family": "Hao",
        "given": "Hao"
      },
      {
        "family": "Zhou",
        "given": "Aimin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          21
        ]
      ]
    },
    "abstract": "Learning Path Recommendation (LPR) aims to generate personalized sequences of learning items that maximize long-term learning effect while respecting pedagogical principles and operational constraints. Although large language models (LLMs) offer rich semantic understanding for free-form recommendation, applying them to long-horizon LPR is challenging due to (i) misalignment with pedagogical objectives such as the Zone of Proximal Development (ZPD) under sparse, delayed feedback, (ii) scarce and costly expert demonstrations, and (iii) multi-objective interactions among learning effect, difficulty scheduling, length controllability, and trajectory diversity. To address these issues, we propose IB-GRPO (Indicator-Based Group Relative Policy Optimization), an indicator-guided alignment approach for LLM-based LPR. To mitigate data scarcity, we construct hybrid expert demonstrations via Genetic Algorithm search and teacher RL agents and warm-start the LLM with supervised fine-tuning. Building on this warm-start, we design a within-session ZPD alignment score for difficulty scheduling. IB-GRPO then uses the $I_{Œµ+}$ dominance indicator to compute group-relative advantages over multiple objectives, avoiding manual scalarization and improving Pareto trade-offs. Experiments on ASSIST09 and Junyi using the KES simulator with a Qwen2.5-7B backbone show consistent improvements over representative RL and LLM baselines.",
    "URL": "http://arxiv.org/abs/2601.14686v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.15280",
    "type": "report",
    "title": "LLM-based Multimodal Feedback Produces Equivalent Learning and Better Student Perceptions than Educator Feedback",
    "author": [
      {
        "family": "Zhao",
        "given": "Chloe Qianhui"
      },
      {
        "family": "Cao",
        "given": "Jie"
      },
      {
        "family": "Lin",
        "given": "Jionghao"
      },
      {
        "family": "Koedinger",
        "given": "Kenneth R."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          21
        ]
      ]
    },
    "abstract": "Providing timely, targeted, and multimodal feedback helps students quickly correct errors, build deep understanding and stay motivated, yet making it at scale remains a challenge. This study introduces a real-time AI-facilitated multimodal feedback system that integrates structured textual explanations with dynamic multimedia resources, including the retrieved most relevant slide page references and streaming AI audio narration. In an online crowdsourcing experiment, we compared this system against fixed business-as-usual feedback by educators across three dimensions: (1) learning effectiveness, (2) learner engagement, (3) perceived feedback quality and value. Results showed that AI multimodal feedback achieved learning gains equivalent to original educator feedback while significantly outperforming it on perceived clarity, specificity, conciseness, motivation, satisfaction, and reducing cognitive load, with comparable correctness, trust, and acceptance. Process logs revealed distinct engagement patterns: for multiple-choice questions, educator feedback encouraged more submissions; for open-ended questions, AI-facilitated targeted suggestions lowered revision barriers and promoted iterative improvement. These findings highlight the potential of AI multimodal feedback to provide scalable, real-time, and context-aware support that both reduces instructor workload and enhances student experience.",
    "URL": "http://arxiv.org/abs/2601.15280v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.15308",
    "type": "report",
    "title": "When Generative AI Meets Extended Reality: Enabling Scalable and Natural Interactions",
    "author": [
      {
        "family": "Zhu",
        "given": "Mingyu"
      },
      {
        "family": "Chen",
        "given": "Jiangong"
      },
      {
        "family": "Li",
        "given": "Bin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          13
        ]
      ]
    },
    "abstract": "Extended Reality (XR), including virtual, augmented, and mixed reality, provides immersive and interactive experiences across diverse applications, from VR-based education to AR-based assistance and MR-based training. However, widespread XR adoption remains limited due to two key challenges: 1) the high cost and complexity of authoring 3D content, especially for large-scale environments or complex interactions; and 2) the steep learning curve associated with non-intuitive interaction methods like handheld controllers or scripted gestures. Generative AI (GenAI) presents a promising solution by enabling intuitive, language-driven interaction and automating content generation. Leveraging vision-language models and diffusion-based generation, GenAI can interpret ambiguous instructions, understand physical scenes, and generate or manipulate 3D content, significantly lowering barriers to XR adoption. This paper explores the integration of XR and GenAI through three concrete use cases, showing how they address key obstacles in scalability and natural interaction, and identifying technical challenges that must be resolved to enable broader adoption.",
    "DOI": "10.1109/MIC.2025.3619462",
    "URL": "https://doi.org/10.1109/MIC.2025.3619462",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.15551",
    "type": "report",
    "title": "ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance",
    "author": [
      {
        "family": "Tokoli",
        "given": "Bismack"
      },
      {
        "family": "Jaimes",
        "given": "Luis"
      },
      {
        "family": "Dina",
        "given": "Ayesha S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          22
        ]
      ]
    },
    "abstract": "Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.",
    "URL": "http://arxiv.org/abs/2601.15551v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.16134",
    "type": "report",
    "title": "LLM Prompt Evaluation for Educational Applications",
    "author": [
      {
        "family": "Holmes",
        "given": "Langdon"
      },
      {
        "family": "Coscia",
        "given": "Adam"
      },
      {
        "family": "Crossley",
        "given": "Scott"
      },
      {
        "family": "Choi",
        "given": "Joon Suh"
      },
      {
        "family": "Morris",
        "given": "Wesley"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          22
        ]
      ]
    },
    "abstract": "As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.",
    "URL": "http://arxiv.org/abs/2601.16134v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.16230",
    "type": "report",
    "title": "Zero-Shot Speech LLMs for Multi-Aspect Evaluation of L2 Speech: Challenges and Opportunities",
    "author": [
      {
        "family": "Parikh",
        "given": "Aditya Kamlesh"
      },
      {
        "family": "Tejedor-Garcia",
        "given": "Cristian"
      },
      {
        "family": "Cucchiarini",
        "given": "Catia"
      },
      {
        "family": "Strik",
        "given": "Helmer"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          20
        ]
      ]
    },
    "abstract": "An accurate assessment of L2 English pronunciation is crucial for language learning, as it provides personalized feedback and ensures a fair evaluation of individual progress. However, automated scoring remains challenging due to the complexity of sentence-level fluency, prosody, and completeness. This paper evaluates the zero-shot performance of Qwen2-Audio-7B-Instruct, an instruction-tuned speech-LLM, on 5,000 Speechocean762 utterances. The model generates rubric-aligned scores for accuracy, fluency, prosody, and completeness, showing strong agreement with human ratings within +-2 tolerance, especially for high-quality speech. However, it tends to overpredict low-quality speech scores and lacks precision in error detection. These findings demonstrate the strong potential of speech LLMs in scalable pronunciation assessment and suggest future improvements through enhanced prompting, calibration, and phonetic integration to advance Computer-Assisted Pronunciation Training.",
    "DOI": "10.21437/SLaTE.2025-3",
    "URL": "https://doi.org/10.21437/SLaTE.2025-3",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.16312",
    "type": "report",
    "title": "Teaching and Evaluating LLMs to Reason About Polymer Design Related Tasks",
    "author": [
      {
        "family": "Mohanty",
        "given": "Dikshya"
      },
      {
        "family": "Hasan",
        "given": "Mohammad Saqib"
      },
      {
        "family": "Monsur",
        "given": "Syed Mostofa"
      },
      {
        "family": "Zheng",
        "given": "Size"
      },
      {
        "family": "Hsiao",
        "given": "Benjamin"
      },
      {
        "family": "Balasubramanian",
        "given": "Niranjan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          22
        ]
      ]
    },
    "abstract": "Research in AI4Science has shown promise in many science applications, including polymer design. However, current LLMs prove ineffective on this problem space because: (i) most models lack polymer-specific knowledge (ii) existing aligned models lack coverage of knowledge and capabilities relevant to polymer design. Addressing this, we introduce PolyBench, a large scale training and test benchmark dataset of more than 125K polymer design related tasks, leveraging a knowledge base of 13M+ data points obtained from experimental and synthetic sources to ensure broad coverage of polymers and their properties. For effective alignment using PolyBench, we introduce a knowledge-augmented reasoning distillation method that augments this dataset with structured CoT. Furthermore, tasks in PolyBench are organized from simple to complex analytical reasoning problems, enabling generalization tests and diagnostic probes across the problem space. Experiments show that small language models (SLMs), of 7B to 14B parameters, trained on PolyBench data outperform similar sized models, and even closed source frontier LLMs on PolyBench test dataset while demonstrating gains on other polymer benchmarks as well.",
    "URL": "http://arxiv.org/abs/2601.16312v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.16314",
    "type": "report",
    "title": "Machine-Assisted Grading of Nationwide School-Leaving Essay Exams with LLMs and Statistical NLP",
    "author": [
      {
        "family": "Karjus",
        "given": "Andres"
      },
      {
        "family": "Allkivi",
        "given": "Kais"
      },
      {
        "family": "Maine",
        "given": "Silvia"
      },
      {
        "family": "Leppik",
        "given": "Katarin"
      },
      {
        "family": "Kruusmaa",
        "given": "Krister"
      },
      {
        "family": "Aruvee",
        "given": "Merilin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          22
        ]
      ]
    },
    "abstract": "Large language models (LLMs) enable rapid and consistent automated evaluation of open-ended exam responses, including dimensions of content and argumentation that have traditionally required human judgment. This is particularly important in cases where a large amount of exams need to be graded in a limited time frame, such as nation-wide graduation exams in various countries. Here, we examine the applicability of automated scoring on two large datasets of trial exam essays of two full national cohorts from Estonia. We operationalize the official curriculum-based rubric and compare LLM and statistical natural language processing (NLP) based assessments with human panel scores. The results show that automated scoring can achieve performance comparable to that of human raters and tends to fall within the human scoring range. We also evaluate bias, prompt injection risks, and LLMs as essay writers. These findings demonstrate that a principled, rubric-driven, human-in-the-loop scoring pipeline is viable for high-stakes writing assessment, particularly relevant for digitally advanced societies like Estonia, which is about to adapt a fully electronic examination system. Furthermore, the system produces fine-grained subscore profiles that can be used to generate systematic, personalized feedback for instruction and exam preparation. The study provides evidence that LLM-assisted assessment can be implemented at a national scale, even in a small-language context, while maintaining human oversight and compliance with emerging educational and regulatory standards.",
    "URL": "http://arxiv.org/abs/2601.16314v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.16466",
    "type": "report",
    "title": "Persona Jailbreaking in Large Language Models",
    "author": [
      {
        "family": "Sandhan",
        "given": "Jivnesh"
      },
      {
        "family": "Cheng",
        "given": "Fei"
      },
      {
        "family": "Sandhan",
        "given": "Tushar"
      },
      {
        "family": "Murawaki",
        "given": "Yugo"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          23
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) are increasingly deployed in domains such as education, mental health and customer support, where stable and consistent personas are critical for reliability. Yet, existing studies focus on narrative or role-playing tasks and overlook how adversarial conversational history alone can reshape induced personas. Black-box persona manipulation remains unexplored, raising concerns for robustness in realistic interactions. In response, we introduce the task of persona editing, which adversarially steers LLM traits through user-side inputs under a black-box, inference-only setting. To this end, we propose PHISH (Persona Hijacking via Implicit Steering in History), the first framework to expose a new vulnerability in LLM safety that embeds semantically loaded cues into user queries to gradually induce reverse personas. We also define a metric to quantify attack success. Across 3 benchmarks and 8 LLMs, PHISH predictably shifts personas, triggers collateral changes in correlated traits, and exhibits stronger effects in multi-turn settings. In high-risk domains mental health, tutoring, and customer support, PHISH reliably manipulates personas, validated by both human and LLM-as-Judge evaluations. Importantly, PHISH causes only a small reduction in reasoning benchmark performance, leaving overall utility largely intact while still enabling significant persona manipulation. While current guardrails offer partial protection, they remain brittle under sustained attack. Our findings expose new vulnerabilities in personas and highlight the need for context-resilient persona in LLMs. Our codebase and dataset is available at: https://github.com/Jivnesh/PHISH",
    "URL": "http://arxiv.org/abs/2601.16466v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.17024",
    "type": "report",
    "title": "Ensuring Computer Science Learning in the AI Era: Open Generative AI Policies and Assignment-Driven Written Quizzes",
    "author": [
      {
        "family": "Chung",
        "given": "Chan-Jin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          16
        ]
      ]
    },
    "abstract": "The widespread availability of generative artificial intelligence (GenAI) has created a pressing challenge in computer science (CS) education: how to incorporate powerful AI tools into programming coursework without undermining student learning through cognitive offloading. This paper presents an assessment model that permits the use of generative AI for take-home programming assignments while enforcing individual mastery through immediate, assignment-driven written quizzes. To promote authentic learning, these in-class, closed-book assessments are weighted more heavily than the assignments themselves and are specifically designed to verify the student's comprehension of the algorithms, structure, and implementation details of their submitted code. Preliminary empirical data were collected from an upper-level computer science course to examine the relationship between self-reported GenAI usage and performance on AI-free quizzes, exams, and final course grades. Statistical analyses revealed no meaningful linear correlation between GenAI usage levels and assessment outcomes, with Pearson correlation coefficients consistently near zero. These preliminary results suggest that allowing GenAI for programming assignments does not diminish students' mastery of course concepts when learning is verified through targeted, assignment-driven quizzes. Although limited by a small sample size, this study provides preliminary evidence that the risks of cognitive offloading can be mitigated by allowing AI-assisted programming practice while verifying understanding through assignment-driven, AI-free quizzes. The findings support the responsible adoption of open GenAI policies in upper-level CS courses, when paired with rigorous, independent assessment mechanisms.",
    "URL": "http://arxiv.org/abs/2601.17024v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.17346",
    "type": "report",
    "title": "Multi-Agent Learning Path Planning via LLMs",
    "author": [
      {
        "family": "Xu",
        "given": "Haoxin"
      },
      {
        "family": "Qi",
        "given": "Changyong"
      },
      {
        "family": "Liu",
        "given": "Tong"
      },
      {
        "family": "Zhang",
        "given": "Bohao"
      },
      {
        "family": "He",
        "given": "Anna"
      },
      {
        "family": "Jiang",
        "given": "Bingqian"
      },
      {
        "family": "Zheng",
        "given": "Longwei"
      },
      {
        "family": "Gu",
        "given": "Xiaoqing"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          24
        ]
      ]
    },
    "abstract": "The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.",
    "URL": "http://arxiv.org/abs/2601.17346v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.17434",
    "type": "report",
    "title": "Co-Designing Digital Humans for Online Learning: A Framework for Human-AI Pedagogical Integration",
    "author": [
      {
        "family": "Lei",
        "given": "Xiaokang"
      },
      {
        "family": "Pang",
        "given": "Ching Christie"
      },
      {
        "family": "Jiang",
        "given": "Yuyang"
      },
      {
        "family": "Tong",
        "given": "Xin"
      },
      {
        "family": "Hui",
        "given": "Pan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          24
        ]
      ]
    },
    "abstract": "Artificial intelligence (AI) and large language models (LLMs) are reshaping education, with virtual avatars emerging as digital teachers capable of enhancing engagement, sustaining attention, and addressing instructor shortages. Aligned with the Sustainable Development Goals (SDGs) for equitable quality education, these technologies hold promise yet lack clear guidelines for effective design and implementation in online learning. To fill this gap, we introduce a framework specifying when, what, and how digital teachers should be integrated. Our study combines (1) a design space analysis of 87 works across AI, educational technology, design, and HCI, (2) a survey of 132 learners' practices and preferences, and (3) three co-design workshops with 18 experts from pedagogy, design, and AI. It provides actionable guidance for educators, designers, and HCI researchers, advancing opportunities to build more engaging, equitable, and effective online learning environments powered by digital teachers.",
    "URL": "http://arxiv.org/abs/2601.17434v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.17473",
    "type": "report",
    "title": "LeanTutor: Towards a Verified AI Mathematical Proof Tutor",
    "author": [
      {
        "family": "Patel",
        "given": "Manooshree"
      },
      {
        "family": "Bhattacharyya",
        "given": "Rayna"
      },
      {
        "family": "Lu",
        "given": "Thomas"
      },
      {
        "family": "Mehta",
        "given": "Arnav"
      },
      {
        "family": "Voss",
        "given": "Niels"
      },
      {
        "family": "Norouzi",
        "given": "Narges"
      },
      {
        "family": "Ranade",
        "given": "Gireeja"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          24
        ]
      ]
    },
    "abstract": "This paper considers the development of an AI-based provably-correct mathematical proof tutor. While Large Language Models (LLMs) allow seamless communication in natural language, they are error prone. Theorem provers such as Lean allow for provable-correctness, but these are hard for students to learn. We present a proof-of-concept system (LeanTutor) by combining the complementary strengths of LLMs and theorem provers. LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and (iii) a natural language feedback generator. To evaluate the system, we introduce PeanoBench, a dataset of 371 Peano Arithmetic proofs in human-written natural language and formal language, derived from the Natural Numbers Game.",
    "URL": "http://arxiv.org/abs/2601.17473v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.17962",
    "type": "report",
    "title": "Designing AI Peers for Collaborative Mathematical Problem Solving with Middle School Students: A Participatory Design Study",
    "author": [
      {
        "family": "Lyu",
        "given": "Wenhan"
      },
      {
        "family": "Wang",
        "given": "Yimeng"
      },
      {
        "family": "Yue",
        "given": "Murong"
      },
      {
        "family": "Sun",
        "given": "Yifan"
      },
      {
        "family": "Suh",
        "given": "Jennifer"
      },
      {
        "family": "Kier",
        "given": "Meredith"
      },
      {
        "family": "Yao",
        "given": "Ziyu"
      },
      {
        "family": "Zhang",
        "given": "Yixuan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          25
        ]
      ]
    },
    "abstract": "Collaborative problem solving (CPS) is a fundamental practice in middle-school mathematics education; however, student groups frequently stall or struggle without ongoing teacher support. Recent work has explored how Generative AI tools can be designed to support one-on-one tutoring, but little is known about how AI can be designed as peer learning partners in collaborative learning contexts. We conducted a participatory design study with 24 middle school students, who first engaged in mathematics CPS tasks with AI peers in a technology probe, and then collaboratively designed their ideal AI peer. Our findings reveal that students envision an AI peer as competent in mathematics yet explicitly deferential, providing progressive scaffolds such as hints and checks under clear student control. Students preferred a tone of friendly expertise over exaggerated personas. We also discuss design recommendations and implications for AI peers in middle school mathematics CPS.",
    "DOI": "10.1145/3772318.3791138",
    "URL": "https://doi.org/10.1145/3772318.3791138",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.18517",
    "type": "report",
    "title": "GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback",
    "author": [
      {
        "family": "Sungarda",
        "given": "James"
      },
      {
        "family": "Liu",
        "given": "Hongkai"
      },
      {
        "family": "Zhou",
        "given": "Zilong"
      },
      {
        "family": "Wu",
        "given": "Tien-Hsuan"
      },
      {
        "family": "Cheung",
        "given": "Johnson Chun-Sing"
      },
      {
        "family": "Kao",
        "given": "Ben"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          26
        ]
      ]
    },
    "abstract": "Field education is the signature pedagogy of social work, yet providing timely and objective feedback during training is constrained by the availability of instructors and counseling clients. In this paper, we present SWITCH, the Social Work Interactive Training Chatbot. SWITCH integrates realistic client simulation, real-time counseling skill classification, and a Motivational Interviewing (MI) progression system into the training workflow. To model a client, SWITCH uses a cognitively grounded profile comprising static fields (e.g., background, beliefs) and dynamic fields (e.g., emotions, automatic thoughts, openness), allowing the agent's behavior to evolve throughout a session realistically. The skill classification module identifies the counseling skills from the user utterances, and feeds the result to the MI controller that regulates the MI stage transitions. To enhance classification accuracy, we study in-context learning with retrieval over annotated transcripts, and a fine-tuned BERT multi-label classifier. In the experiments, we demonstrated that both BERT-based approach and in-context learning outperforms the baseline with big margin. SWITCH thereby offers a scalable, low-cost, and consistent training workflow that complements field education, and allows supervisors to focus on higher-level mentorship.",
    "URL": "http://arxiv.org/abs/2601.18517v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.18685",
    "type": "report",
    "title": "LLAMA LIMA: A Living Meta-Analysis on the Effects of Generative AI on Learning Mathematics",
    "author": [
      {
        "family": "Strohmaier",
        "given": "Anselm"
      },
      {
        "family": "B√∂defeld",
        "given": "Samira"
      },
      {
        "family": "Reinhold",
        "given": "Frank"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          26
        ]
      ]
    },
    "abstract": "The capabilities of generative AI in mathematics education are rapidly evolving, posing significant challenges for research to keep pace. Research syntheses remain scarce and risk being outdated by the time of publication. To address this issue, we present a Living Meta-Analysis (LIMA) on the effects of generative AI-based interventions for learning mathematics. Following PRISMA-LSR guidelines, we continuously update the literature base, apply a Bayesian multilevel meta-regression model to account for cumulative data, and publish updated versions on a preprint server at regular intervals. This paper reports results from the first version, including 15 studies. The analyses indicate a small positive effect (g = 0.31) with a wide credible interval [0.06, 0.58], reflecting the still limited evidence base.",
    "URL": "http://arxiv.org/abs/2601.18685v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.18734",
    "type": "report",
    "title": "Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models",
    "author": [
      {
        "family": "Zhao",
        "given": "Siyan"
      },
      {
        "family": "Xie",
        "given": "Zhihui"
      },
      {
        "family": "Liu",
        "given": "Mengchen"
      },
      {
        "family": "Huang",
        "given": "Jing"
      },
      {
        "family": "Pang",
        "given": "Guan"
      },
      {
        "family": "Chen",
        "given": "Feiyu"
      },
      {
        "family": "Grover",
        "given": "Aditya"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          26
        ]
      ]
    },
    "abstract": "Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.",
    "URL": "http://arxiv.org/abs/2601.18734v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.18778",
    "type": "report",
    "title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability",
    "author": [
      {
        "family": "Sundaram",
        "given": "Shobhita"
      },
      {
        "family": "Quan",
        "given": "John"
      },
      {
        "family": "Kwiatkowski",
        "given": "Ariel"
      },
      {
        "family": "Ahuja",
        "given": "Kartik"
      },
      {
        "family": "Ollivier",
        "given": "Yann"
      },
      {
        "family": "Kempe",
        "given": "Julia"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          26
        ]
      ]
    },
    "abstract": "Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.",
    "URL": "http://arxiv.org/abs/2601.18778v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.19051",
    "type": "report",
    "title": "Proactive Hardening of LLM Defenses with HASTE",
    "author": [
      {
        "family": "Chen",
        "given": "Henry"
      },
      {
        "family": "Aranda",
        "given": "Victor"
      },
      {
        "family": "Keshari",
        "given": "Samarth"
      },
      {
        "family": "Heartfield",
        "given": "Ryan"
      },
      {
        "family": "Nichols",
        "given": "Nicole"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          27
        ]
      ]
    },
    "abstract": "Prompt-based attack techniques are one of the primary challenges in securely deploying and protecting LLM-based AI systems. LLM inputs are an unbounded, unstructured space. Consequently, effectively defending against these attacks requires proactive hardening strategies capable of continuously generating adaptive attack vectors to optimize LLM defense at runtime. We present HASTE (Hard-negative Attack Sample Training Engine): a systematic framework that iteratively engineers highly evasive prompts, within a modular optimization process, to continuously enhance detection efficacy for prompt-based attack techniques. The framework is agnostic to synthetic data generation methods, and can be generalized to evaluate prompt-injection detection efficacy, with and without fuzzing, for any hard-negative or hard-positive iteration strategy. Experimental evaluation of HASTE shows that hard negative mining successfully evades baseline detectors, reducing malicious prompt detection for baseline detectors by approximately 64%. However, when integrated with detection model re-training, it optimizes the efficacy of prompt detection models with significantly fewer iteration loops compared to relative baseline strategies. The HASTE framework supports both proactive and reactive hardening of LLM defenses and guardrails. Proactively, developers can leverage HASTE to dynamically stress-test prompt injection detection systems; efficiently identifying weaknesses and strengthening defensive posture. Reactively, HASTE can mimic newly observed attack types and rapidly bridge detection coverage by teaching HASTE-optimized detection models to identify them.",
    "URL": "http://arxiv.org/abs/2601.19051v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.19280",
    "type": "report",
    "title": "Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning",
    "author": [
      {
        "family": "Panaganti",
        "given": "Kishan"
      },
      {
        "family": "Liang",
        "given": "Zhenwen"
      },
      {
        "family": "Yu",
        "given": "Wenhao"
      },
      {
        "family": "Mi",
        "given": "Haitao"
      },
      {
        "family": "Yu",
        "given": "Dong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          27
        ]
      ]
    },
    "abstract": "Recent progress in Large Language Model (LLM) reasoning is increasingly driven by the refinement of post-training loss functions and alignment strategies. However, standard Reinforcement Learning (RL) paradigms like Group Relative Policy Optimization (GRPO) remain constrained by static uniformity: uniform prompt sampling and a fixed number of rollouts per prompt. For heterogeneous, heavy-tailed reasoning data, this creates structural inefficiencies that waste compute on already-solved patterns while under-training the long tail of hard problems. To address this, we propose Multi-Adversary Group Distributionally Robust Optimization (GDRO), an optimization-first framework that moves beyond uniform reasoning models by dynamically adapting the training distribution.\n  We introduce an Online Difficulty Classifier that partitions prompts into dynamic pass@k difficulty groups. We then propose two independent GDRO games for post-training: (1) Prompt-GDRO, which employs an EMA-debiased multiplicative-weights bandit sampler to target the intensive difficulty margin and upweight persistently hard groups without frequency bias; and (2) Rollout-GDRO, which uses a shadow-price controller to reallocate rollouts across groups, maximizing gradient variance reduction on hard tasks under a fixed mean budget (compute-neutral). We provide no-regret guarantees for both controllers and additionally a variance-proxy analysis motivating a square-root optimal rollout allocation for Rollout-GDRO. We validate our framework on the DAPO 14.1k dataset using Qwen3-Base models. Prompt-GDRO and Rollout-GDRO achieve average relative gains of +10.6% and +10.1%, respectively, in pass@8 accuracy across 1.7B, 4B, and 8B scales compared to the GRPO baseline. Qualitative analysis shows an emergent curriculum: the adversaries shift resources to the evolving reasoning frontier, enhancing the reasoning model's performance.",
    "URL": "http://arxiv.org/abs/2601.19280v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.19332",
    "type": "report",
    "title": "CaseMaster: Designing and Evaluating a Probe for Oral Case Presentation Training with LLM Assistance",
    "author": [
      {
        "family": "Ouyang",
        "given": "Yang"
      },
      {
        "family": "Xu",
        "given": "Yuansong"
      },
      {
        "family": "Jiang",
        "given": "Chang"
      },
      {
        "family": "Jin",
        "given": "Yifan"
      },
      {
        "family": "Jiang",
        "given": "Haoran"
      },
      {
        "family": "Li",
        "given": "Quan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          27
        ]
      ]
    },
    "abstract": "Preparing an oral case presentation (OCP) is a crucial skill for medical students, requiring clear communication of patient information, clinical findings, and treatment plans. However, inconsistent student participation and limited guidance can make this task challenging. While Large Language Models (LLMs) can provide structured content to streamline the process, their role in facilitating skill development and supporting medical education integration remains underexplored. To address this, we conducted a formative study with six medical educators and developed CaseMaster, an interactive probe that leverages LLM-generated content tailored to medical education to help users enhance their OCP skills. The controlled study suggests CaseMaster has the potential to both improve presentation quality and reduce workload compared to traditional methods, an implication reinforced by expert feedback. We propose guidelines for educators to develop adaptive, user-centered training methods using LLMs, while considering the implications of integrating advanced technologies into medical education.",
    "URL": "http://arxiv.org/abs/2601.19332v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.19588",
    "type": "report",
    "title": "From Atoms to Chains: Divergence-Guided Reasoning Curriculum for Unlabeled LLM Domain Adaptation",
    "author": [
      {
        "family": "Wang",
        "given": "Yongqi"
      },
      {
        "family": "Ji",
        "given": "Xiaofeng"
      },
      {
        "family": "Wang",
        "given": "Jie"
      },
      {
        "family": "Li",
        "given": "Qingbin"
      },
      {
        "family": "Xiong",
        "given": "Xiao"
      },
      {
        "family": "Yang",
        "given": "Zheming"
      },
      {
        "family": "Xu",
        "given": "Jian"
      },
      {
        "family": "Qiu",
        "given": "Minghui"
      },
      {
        "family": "Wu",
        "given": "Xinxiao"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          27
        ]
      ]
    },
    "abstract": "Adapting Large Language Models (LLMs) to specialized domains without human-annotated data is a crucial yet formidable challenge. Widely adopted knowledge distillation methods often devolve into coarse-grained mimicry, where the student model inefficiently targets its own weaknesses and risks inheriting the teacher's reasoning flaws. This exposes a critical pedagogical dilemma: how to devise a reliable curriculum when the teacher itself is not an infallible expert. Our work resolves this by capitalizing on a key insight: while LLMs may exhibit fallibility in complex, holistic reasoning, they often exhibit high fidelity on focused, atomic sub-problems. Based on this, we propose Divergence-Guided Reasoning Curriculum (DGRC), which constructs a learning path from atomic knowledge to reasoning chains by dynamically deriving two complementary curricula from disagreements in reasoning pathways. When a student and teacher produce conflicting results, DGRC directs the teacher to perform a diagnostic analysis: it analyzes both reasoning paths to formulate atomic queries that target the specific points of divergence, and then self-answers these queries to create high-confidence atomic question-answer pairs. These pairs then serve a dual purpose: (1) providing an atomic curriculum to rectify the student's knowledge gaps, and (2) serving as factual criteria to filter the teacher's original reasoning chains, yielding a verified CoT curriculum that teaches the student how to integrate atomic knowledge into complete reasoning paths. Experiments across the medical and legal domains on student models of various sizes demonstrate the effectiveness of our DGRC framework. Notably, our method achieves a 7.76% relative improvement for the 1.5B student model in the medical domain over strong unlabeled baseline.",
    "URL": "http://arxiv.org/abs/2601.19588v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.20006",
    "type": "report",
    "title": "On the Effectiveness of LLM-Specific Fine-Tuning for Detecting AI-Generated Text",
    "author": [
      {
        "family": "Gromadzki",
        "given": "Micha≈Ç"
      },
      {
        "family": "Wr√≥blewska",
        "given": "Anna"
      },
      {
        "family": "Kaliska",
        "given": "Agnieszka"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          27
        ]
      ]
    },
    "abstract": "The rapid progress of large language models has enabled the generation of text that closely resembles human writing, creating challenges for authenticity verification in education, publishing, and digital security. Detecting AI-generated text has therefore become a crucial technical and ethical issue. This paper presents a comprehensive study of AI-generated text detection based on large-scale corpora and novel training strategies. We introduce a 1-billion-token corpus of human-authored texts spanning multiple genres and a 1.9-billion-token corpus of AI-generated texts produced by prompting a variety of LLMs across diverse domains. Using these resources, we develop and evaluate numerous detection models and propose two novel training paradigms: Per LLM and Per LLM family fine-tuning. Across a 100-million-token benchmark covering 21 large language models, our best fine-tuned detector achieves up to $99.6\\%$ token-level accuracy, substantially outperforming existing open-source baselines.",
    "URL": "http://arxiv.org/abs/2601.20006v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.20014",
    "type": "report",
    "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning",
    "author": [
      {
        "family": "Qu",
        "given": "Shuhui"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          27
        ]
      ]
    },
    "abstract": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality.",
    "URL": "http://arxiv.org/abs/2601.20014v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.20085",
    "type": "report",
    "title": "Editrail: Understanding AI Usage by Visualizing Student-AI Interaction in Code",
    "author": [
      {
        "family": "Zhang",
        "given": "Ashley Ge"
      },
      {
        "family": "Jhou",
        "given": "Yan-Ru"
      },
      {
        "family": "Yang",
        "given": "Yinuo"
      },
      {
        "family": "Rao",
        "given": "Shamita"
      },
      {
        "family": "Arab",
        "given": "Maryam"
      },
      {
        "family": "Chen",
        "given": "Yan"
      },
      {
        "family": "Oney",
        "given": "Steve"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          27
        ]
      ]
    },
    "abstract": "Programming instructors have diverse philosophies about integrating generative AI into their classes. Some encourage students to use AI, while others restrict or forbid it. Regardless of their approach, all instructors benefit from understanding how their students actually use AI while writing code. Such insight helps instructors assess whether AI use aligns with their pedagogical goals, enables timely intervention when they find unproductive usage patterns, and establishes effective policies for AI use. However, our survey with programming instructors found that many instructors lack visibility into how students use AI in their code-writing processes. To address this challenge, we introduce Editrail, an interactive system that enables instructors to track students' AI usage, create personalized assessments, and provide timely interventions, all within the workflow of monitoring coding histories. We found that Editrail enables instructors to detect AI use that conflicts with pedagogical goals accurately and to determine when and which students require intervention.",
    "URL": "http://arxiv.org/abs/2601.20085v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.20476",
    "type": "report",
    "title": "Can We Improve Educational Diagram Generation with In-Context Examples? Not if a Hallucination Spoils the Bunch",
    "author": [
      {
        "family": "Logacheva",
        "given": "Evanfiya"
      },
      {
        "family": "Hellas",
        "given": "Arto"
      },
      {
        "family": "Mihaylova",
        "given": "Tsvetomila"
      },
      {
        "family": "Sorva",
        "given": "Juha"
      },
      {
        "family": "Heinonen",
        "given": "Ava"
      },
      {
        "family": "Leinonen",
        "given": "Juho"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          28
        ]
      ]
    },
    "abstract": "Generative artificial intelligence (AI) has found a widespread use in computing education; at the same time, quality of generated materials raises concerns among educators and students. This study addresses this issue by introducing a novel method for diagram code generation with in-context examples based on the Rhetorical Structure Theory (RST), which aims to improve diagram generation by aligning models' output with user expectations. Our approach is evaluated by computer science educators, who assessed 150 diagrams generated with large language models (LLMs) for logical organization, connectivity, layout aesthetic, and AI hallucination. The assessment dataset is additionally investigated for its utility in automated diagram evaluation. The preliminary results suggest that our method decreases the rate of factual hallucination and improves diagram faithfulness to provided context; however, due to LLMs' stochasticity, the quality of the generated diagrams varies. Additionally, we present an in-depth analysis and discussion on the connection between AI hallucination and the quality of generated diagrams, which reveals that text contexts of higher complexity lead to higher rates of hallucination and LLMs often fail to detect mistakes in their output.",
    "URL": "http://arxiv.org/abs/2601.20476v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.20705",
    "type": "report",
    "title": "LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?",
    "author": [
      {
        "family": "Yu",
        "given": "Zhuang"
      },
      {
        "family": "Shen",
        "given": "Lei"
      },
      {
        "family": "Zhao",
        "given": "Jing"
      },
      {
        "family": "Sun",
        "given": "Shiliang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          27
        ]
      ]
    },
    "abstract": "Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lecture-based Evaluation benchmark for MultimOdal uNderstanding, focusing on STEM lecture videos that require long-horizon reasoning and cross-modal integration. LEMON comprises 2,277 video segments spanning 5 disciplines and 29 courses, with an average duration of 196.1 seconds, yielding 4,181 high-quality QA pairs, including 3,413 multiple-choice and 768 open-ended questions. Distinct from existing video benchmarks, LEMON features: (1) semantic richness and disciplinary density, (2) tightly coupled video-audio-text modalities, (3) explicit temporal and pedagogical structure, and (4) contextually linked multi-turn questioning. It further encompasses six major tasks and twelve subtasks, covering the full cognitive spectrum from perception to reasoning and then to generation. Comprehensive experiments reveal substantial performance gaps across tasks, highlighting that even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction. We expect LEMON to serve as an extensible and challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional contents.",
    "URL": "http://arxiv.org/abs/2601.20705v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.20749",
    "type": "report",
    "title": "Learning to Live with AI: How Students Develop AI Literacy Through Naturalistic ChatGPT Interaction",
    "author": [
      {
        "family": "Ammari",
        "given": "Tawfiq"
      },
      {
        "family": "Chen",
        "given": "Meilun"
      },
      {
        "family": "Zaman",
        "given": "S M Mehedi"
      },
      {
        "family": "Garimella",
        "given": "Kiran"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          28
        ]
      ]
    },
    "abstract": "How do students develop AI literacy through everyday practice rather than formal instruction? While normative AI literacy frameworks proliferate, empirical understanding of how students actually learn to work with generative AI remains limited. This study analyzes 10,536 ChatGPT messages from 36 undergraduates over one academic year, revealing five use genres -- academic workhorse, emotional companion, metacognitive partner, repair and negotiation, and trust calibration -- that constitute distinct configurations of student-AI learning. Drawing on domestication theory and emerging frameworks for AI literacy, we demonstrate that functional AI competence emerges through ongoing relational negotiation rather than one-time adoption. Students develop sophisticated genre portfolios, strategically matching interaction patterns to learning needs while exercising critical judgment about AI limitations. Notably, repair work during AI breakdowns produces substantial learning about AI capabilities, developing what we term \"repair literacy\" -- a crucial but underexplored dimension of AI competence. Our findings offer educators empirically grounded insights into how students actually learn to work with generative AI, with implications for AI literacy pedagogy, responsible AI integration, and the design of AI-enabled learning environments that support student agency.",
    "URL": "http://arxiv.org/abs/2601.20749v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.20911",
    "type": "report",
    "title": "Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs",
    "author": [
      {
        "family": "Zhang",
        "given": "Haochen"
      },
      {
        "family": "Sinha",
        "given": "Animesh"
      },
      {
        "family": "Juefei-Xu",
        "given": "Felix"
      },
      {
        "family": "Ma",
        "given": "Haoyu"
      },
      {
        "family": "Li",
        "given": "Kunpeng"
      },
      {
        "family": "Fan",
        "given": "Zhipeng"
      },
      {
        "family": "Dong",
        "given": "Meng"
      },
      {
        "family": "Dai",
        "given": "Xiaoliang"
      },
      {
        "family": "Hou",
        "given": "Tingbo"
      },
      {
        "family": "Zhang",
        "given": "Peizhao"
      },
      {
        "family": "He",
        "given": "Zecheng"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          28
        ]
      ]
    },
    "abstract": "Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn benchmarks and training recipes are effectively Markov: the next output depends primarily on the most recent image, enabling shortcut solutions that ignore long-range history. In this work we formalize and target the more challenging non-Markov setting, where a user may refer back to earlier states, undo changes, or reference entities introduced several rounds ago. We present (i) non-Markov multi-round data construction strategies, including rollback-style editing that forces retrieval of earlier visual states and name-based multi-round personalization that binds names to appearances across rounds; (ii) a history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift; and (iii) enabling improvements for high-fidelity image reconstruction and editable personalization, including a reconstruction-based DiT detokenizer and a multi-stage fine-tuning curriculum. We demonstrate that explicitly training for non-Markov interactions yields substantial improvements in multi-round consistency and instruction compliance, while maintaining strong single-round editing and personalization.",
    "URL": "http://arxiv.org/abs/2601.20911v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.21344",
    "type": "report",
    "title": "Dynamic Framework for Collaborative Learning: Leveraging Advanced LLM with Adaptive Feedback Mechanisms",
    "author": [
      {
        "family": "Tahir",
        "given": "Hassam"
      },
      {
        "family": "Faisal",
        "given": "Faizan"
      },
      {
        "family": "Alnajjar",
        "given": "Fady"
      },
      {
        "family": "Taj",
        "given": "Muhammad Imran"
      },
      {
        "family": "Gordon",
        "given": "Lucia"
      },
      {
        "family": "Khan",
        "given": "Aila"
      },
      {
        "family": "Lwin",
        "given": "Michael"
      },
      {
        "family": "Mubin",
        "given": "Omar"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          29
        ]
      ]
    },
    "abstract": "This paper presents a framework for integrating LLM into collaborative learning platforms to enhance student engagement, critical thinking, and inclusivity. The framework employs advanced LLMs as dynamic moderators to facilitate real-time discussions and adapt to learners' evolving needs, ensuring diverse and inclusive educational experiences. Key innovations include robust feedback mechanisms that refine AI moderation, promote reflective learning, and balance participation among users. The system's modular architecture featuring ReactJS for the frontend, Flask for backend operations, and efficient question retrieval supports personalized and engaging interactions through dynamic adjustments to prompts and discussion flows. Testing demonstrates that the framework significantly improves student collaboration, fosters deeper comprehension, and scales effectively across various subjects and user groups. By addressing limitations in static moderation and personalization in existing systems, this work establishes a strong foundation for next-generation AI-driven educational tools, advancing equitable and impactful learning outcomes.",
    "DOI": "10.1109/ABC64332.2025.11118419",
    "URL": "https://doi.org/10.1109/ABC64332.2025.11118419",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.21360",
    "type": "report",
    "title": "The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation",
    "author": [
      {
        "family": "Sahoo",
        "given": "Devanshu"
      },
      {
        "family": "Prasad",
        "given": "Manish"
      },
      {
        "family": "Majhi",
        "given": "Vasudev"
      },
      {
        "family": "Neekhra",
        "given": "Arjun"
      },
      {
        "family": "Sinha",
        "given": "Yash"
      },
      {
        "family": "Mandal",
        "given": "Murari"
      },
      {
        "family": "Chamola",
        "given": "Vinay"
      },
      {
        "family": "Kumar",
        "given": "Dhruv"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          29
        ]
      ]
    },
    "abstract": "The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, models frequently decouple from the submission's logic to satisfy hidden directives, a systemic vulnerability we term the Compliance Paradox, where models fine-tuned for extreme helpfulness are vulnerable to adversarial manipulation. To expose this, we introduce the Semantic-Preserving Adversarial Code Injection (SPACI) Framework and the Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). These methods exploit the Syntax-Semantics Gap by embedding adversarial directives into syntactically inert regions (trivia nodes) of the Abstract Syntax Tree. Through a large-scale evaluation of 9 SOTA models across 25,000 submissions in Python, C, C++, and Java, we reveal catastrophic failure rates (>95%) in high-capacity open-weights models like DeepSeek-V3, which systematically prioritize hidden formatting constraints over code correctness. We quantify this failure using our novel tripartite framework measuring Decoupling Probability, Score Divergence, and Pedagogical Severity to demonstrate the widespread \"False Certification\" of functionally broken code. Our findings suggest that current alignment paradigms create a \"Trojan\" vulnerability in automated grading, necessitating a shift from standard RLHF toward domain-specific Adjudicative Robustness, where models are conditioned to prioritize evidence over instruction compliance. We release our complete dataset and injection framework to facilitate further research on the topic.",
    "URL": "http://arxiv.org/abs/2601.21360v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.21375",
    "type": "report",
    "title": "TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability in Large Language Models",
    "author": [
      {
        "family": "Li",
        "given": "Zheng"
      },
      {
        "family": "Song",
        "given": "Siyao"
      },
      {
        "family": "Ma",
        "given": "Jingyuan"
      },
      {
        "family": "Li",
        "given": "Rui"
      },
      {
        "family": "Zeng",
        "given": "Ying"
      },
      {
        "family": "Li",
        "given": "Minghao"
      },
      {
        "family": "Sui",
        "given": "Zhifang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          29
        ]
      ]
    },
    "abstract": "Large language models (LLMs) show promise as teaching assistants, yet their teaching capability remains insufficiently evaluated. Existing benchmarks mainly focus on problem-solving or problem-level guidance, leaving knowledge-centered teaching underexplored. We propose a syllabus-grounded evaluation framework that measures LLM teaching capability via student performance improvement after multi-turn instruction. By restricting teacher agents to structured knowledge points and example problems, the framework avoids information leakage and enables reuse of existing benchmarks. We instantiate the framework on Gaokao data across multiple subjects. Experiments reveal substantial variation in teaching effectiveness across models and domains: some models perform well in mathematics, while teaching remains challenging in physics and chemistry. We also find that incorporating example problems does not necessarily improve teaching, as models often shift toward example-specific error correction. Overall, our results highlight teaching ability as a distinct and measurable dimension of LLM behavior.",
    "URL": "http://arxiv.org/abs/2601.21375v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.21600",
    "type": "report",
    "title": "CORE: Collaborative Reasoning via Cross Teaching",
    "author": [
      {
        "family": "Mishra",
        "given": "Kshitij"
      },
      {
        "family": "Aubakirov",
        "given": "Mirat"
      },
      {
        "family": "Takac",
        "given": "Martin"
      },
      {
        "family": "Lukas",
        "given": "Nils"
      },
      {
        "family": "Lahlou",
        "given": "Salem"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          29
        ]
      ]
    },
    "abstract": "Large language models exhibit complementary reasoning errors: on the same instance, one model may succeed with a particular decomposition while another fails. We propose Collaborative Reasoning (CORE), a training-time collaboration framework that converts peer success into a learning signal via a cross-teaching protocol. Each problem is solved in two stages: a cold round of independent sampling, followed by a contexted rescue round in which models that failed receive hint extracted from a successful peer. CORE optimizes a combined reward that balances (i) correctness, (ii) a lightweight DPP-inspired diversity term to reduce error overlap, and (iii) an explicit rescue bonus for successful recovery. We evaluate CORE across four standard reasoning datasets GSM8K, MATH, AIME, and GPQA. With only 1,000 training examples, a pair of small open source models (3B+4B) reaches Pass@2 of 99.54% on GSM8K and 92.08% on MATH, compared to 82.50% and 74.82% for single-model training. On harder datasets, the 3B+4B pair reaches Pass@2 of 77.34% on GPQA (trained on 348 examples) and 79.65% on AIME (trained on 792 examples), using a training-time budget of at most 1536 context tokens and 3072 generated tokens. Overall, these results show that training-time collaboration can reliably convert model complementarity into large gains without scaling model size.",
    "URL": "http://arxiv.org/abs/2601.21600v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.21698",
    "type": "report",
    "title": "Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics",
    "author": [
      {
        "family": "Elgaar",
        "given": "Mohamed"
      },
      {
        "family": "Amiri",
        "given": "Hadi"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          29
        ]
      ]
    },
    "abstract": "Curriculum learning changes the order of pre-training data, but it remains unclear whether it changes the learning trajectory or mainly reorders exposure over a fixed trajectory. We train Pythia models (14M-410M parameters) for 300B tokens under three linguistically motivated curricula-Age-of-Acquisition, word frequency, and Verb Variation (VV)-and compare each against Random ordering; at 1B parameters we compare Random and VV. Across orderings, training follows a shared sequence of latent phases, while curricula mainly change within-phase data exposure. In smaller models (up to 160M parameters), Random ordering exhibits higher gradient noise and stronger late-training output-head spectral saturation, alongside lower final accuracy; curricula reduce both effects at matched compute. At larger scales, saturation differences are smaller and curriculum gains shrink. We formalize the link between difficulty pacing and optimization stability in an idealized analysis based on gradient-variance control, and our results point to a practical takeaway: curricula help by stabilizing within-phase optimization rather than by creating new phases.",
    "URL": "http://arxiv.org/abs/2601.21698v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.21711",
    "type": "report",
    "title": "TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning",
    "author": [
      {
        "family": "Lai",
        "given": "Huiyuan"
      },
      {
        "family": "Nissim",
        "given": "Malvina"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          29
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with redundant intermediate steps. To improve learning and reasoning efficiency, while preserving or even enhancing performance, we propose TACLer, a model-tailored curriculum reinforcement learning framework that gradually increases the complexity of the data based on the model's proficiency in multi-stage RL training. TACLer features two core components: (i) tailored curriculum learning that determines what knowledge the model lacks and needs to learn in progressive stages; (ii) a hybrid Thinking/NoThinking reasoning paradigm that balances accuracy and efficiency by enabling or disabling the Thinking mode. Our experiments show that TACLer yields a twofold advantage in learning and reasoning: (i) it reduces computational cost, cutting training compute by over 50% compared to long thinking models and reducing inference token usage by over 42% relative to the base model; and (ii) it improves accuracy by over 9% on the base model, consistently outperforming state-of-the-art Nothinking and Thinking baselines across four math datasets with complex problems.",
    "URL": "http://arxiv.org/abs/2601.21711v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.21802",
    "type": "report",
    "title": "A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition",
    "author": [
      {
        "family": "Phan",
        "given": "Hoang Khang"
      },
      {
        "family": "Dang",
        "given": "Quang Vinh"
      },
      {
        "family": "Colley",
        "given": "Noriyo"
      },
      {
        "family": "Garcia",
        "given": "Christina"
      },
      {
        "family": "Le",
        "given": "Nhat Tan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          29
        ]
      ]
    },
    "abstract": "Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.",
    "URL": "http://arxiv.org/abs/2601.21802v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.22009",
    "type": "report",
    "title": "MEIDNet: Multimodal generative AI framework for inverse materials design",
    "author": [
      {
        "family": "Babu",
        "given": "Anand"
      },
      {
        "family": "Gouv√™a",
        "given": "Rog√©rio Almeida"
      },
      {
        "family": "Vandergheynst",
        "given": "Pierre"
      },
      {
        "family": "Rignanese",
        "given": "Gian-Marco"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          29
        ]
      ]
    },
    "abstract": "In this work, we present Multimodal Equivariant Inverse Design Network (MEIDNet), a framework that jointly learns structural information and materials properties through contrastive learning, while encoding structures via an equivariant graph neural network (EGNN). By combining generative inverse design with multimodal learning, our approach accelerates the exploration of chemical-structural space and facilitates the discovery of materials that satisfy predefined property targets. MEIDNet exhibits strong latent-space alignment with cosine similarity 0.96 by fusion of three modalities through cross-modal learning. Through implementation of curriculum learning strategies, MEIDNet achieves ~60 times higher learning efficiency than conventional training techniques. The potential of our multimodal approach is demonstrated by generating low-bandgap perovskite structures at a stable, unique, and novel (SUN) rate of 13.6 %, which are further validated by ab initio methods. Our inverse design framework demonstrates both scalability and adaptability, paving the way for the universal learning of chemical space across diverse modalities.",
    "URL": "http://arxiv.org/abs/2601.22009v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.22433",
    "type": "report",
    "title": "When LLM meets Fuzzy-TOPSIS for Personnel Selection through Automated Profile Analysis",
    "author": [
      {
        "family": "Hoque",
        "given": "Shahria"
      },
      {
        "family": "Karim",
        "given": "Ahmed Akib Jawad"
      },
      {
        "family": "Alam",
        "given": "Md. Golam Rabiul"
      },
      {
        "family": "Gope",
        "given": "Nirjhar"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          30
        ]
      ]
    },
    "abstract": "In this highly competitive employment environment, the selection of suitable personnel is essential for organizational success. This study presents an automated personnel selection system that utilizes sophisticated natural language processing (NLP) methods to assess and rank software engineering applicants. A distinctive dataset was created by aggregating LinkedIn profiles that include essential features such as education, work experience, abilities, and self-introduction, further enhanced with expert assessments to function as standards. The research combines large language models (LLMs) with multicriteria decision-making (MCDM) theory to develop the LLM-TOPSIS framework. In this context, we utilized the TOPSIS method enhanced by fuzzy logic (Fuzzy TOPSIS) to address the intrinsic ambiguity and subjectivity in human assessments. We utilized triangular fuzzy numbers (TFNs) to describe criteria weights and scores, thereby addressing the ambiguity frequently encountered in candidate evaluations. For candidate ranking, the DistilRoBERTa model was fine-tuned and integrated with the fuzzy TOPSIS method, achieving rankings closely aligned with human expert evaluations and attaining an accuracy of up to 91% for the Experience attribute and the Overall attribute. The study underlines the potential of NLP-driven frameworks to improve recruitment procedures by boosting scalability, consistency, and minimizing prejudice. Future endeavors will concentrate on augmenting the dataset, enhancing model interpretability, and verifying the system in actual recruitment scenarios to better evaluate its practical applicability. This research highlights the intriguing potential of merging NLP with fuzzy decision-making methods in personnel selection, enabling scalable and unbiased solutions to recruitment difficulties.",
    "DOI": "10.1109/ACCESS.2026.3658575",
    "URL": "https://doi.org/10.1109/ACCESS.2026.3658575",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.22628",
    "type": "report",
    "title": "TTCS: Test-Time Curriculum Synthesis for Self-Evolving",
    "author": [
      {
        "family": "Yang",
        "given": "Chengyi"
      },
      {
        "family": "Xiang",
        "given": "Zhishang"
      },
      {
        "family": "Tang",
        "given": "Yunbo"
      },
      {
        "family": "Teng",
        "given": "Zongpei"
      },
      {
        "family": "Huang",
        "given": "Chengsong"
      },
      {
        "family": "Long",
        "given": "Fei"
      },
      {
        "family": "Liu",
        "given": "Yuhan"
      },
      {
        "family": "Su",
        "given": "Jinsong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          30
        ]
      ]
    },
    "abstract": "Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.",
    "URL": "http://arxiv.org/abs/2601.22628v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.22896",
    "type": "report",
    "title": "Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery",
    "author": [
      {
        "family": "Ke",
        "given": "Xinyi"
      },
      {
        "family": "Li",
        "given": "Kai"
      },
      {
        "family": "Xing",
        "given": "Junliang"
      },
      {
        "family": "Zhang",
        "given": "Yifan"
      },
      {
        "family": "Cheng",
        "given": "Jian"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          30
        ]
      ]
    },
    "abstract": "Large language models (LLMs) have enabled rapid progress in automatic heuristic discovery (AHD), yet most existing methods are predominantly limited by static evaluation against fixed instance distributions, leading to potential overfitting and poor generalization under distributional shifts. We propose Algorithm Space Response Oracles (ASRO), a game-theoretic framework that reframes heuristic discovery as a program level co-evolution between solver and instance generator. ASRO models their interaction as a two-player zero-sum game, maintains growing strategy pools on both sides, and iteratively expands them via LLM-based best-response oracles against mixed opponent meta-strategies, thereby replacing static evaluation with an adaptive, self-generated curriculum. Across multiple combinatorial optimization domains, ASRO consistently outperforms static-training AHD baselines built on the same program search mechanisms, achieving substantially improved generalization and robustness on diverse and out-of-distribution instances.",
    "URL": "http://arxiv.org/abs/2601.22896v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.23161",
    "type": "report",
    "title": "DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding",
    "author": [
      {
        "family": "Zhou",
        "given": "Jiaming"
      },
      {
        "family": "Cheng",
        "given": "Xuxin"
      },
      {
        "family": "Zhao",
        "given": "Shiwan"
      },
      {
        "family": "Jia",
        "given": "Yuhang"
      },
      {
        "family": "Liu",
        "given": "Cao"
      },
      {
        "family": "Zeng",
        "given": "Ke"
      },
      {
        "family": "Cai",
        "given": "Xunliang"
      },
      {
        "family": "Qin",
        "given": "Yong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          30
        ]
      ]
    },
    "abstract": "Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git.",
    "URL": "http://arxiv.org/abs/2601.23161v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.00243",
    "type": "report",
    "title": "\"OpenBloom\": A Question-Based LLM Tool to Support Stigma Reduction in Reproductive Well-Being",
    "author": [
      {
        "family": "Hua",
        "given": "Ashley"
      },
      {
        "family": "Daruka",
        "given": "Adya"
      },
      {
        "family": "Hong",
        "given": "Yang"
      },
      {
        "family": "Sultana",
        "given": "Sharifa"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          30
        ]
      ]
    },
    "abstract": "Reproductive well-being education remains widely stigmatized across diverse cultural contexts, constraining how individuals access and interpret reproductive health knowledge. We designed and evaluated OpenBloom, a stigma-sensitive, AI-mediated system that uses LLMs to transform reproductive health articles into reflective, question-based learning prompts. We employed OpenBloom as a design probe, aiming to explore the emerging challenges of reproductive well-being stigma through LLMs. Through surveys, semi-structured interviews, and focus group discussions, we examine how sociocultural stigma shapes participants' engagements with AI-generated questions and the opportunities of inquiry-based reproductive health education. Our findings identify key design considerations for stigma-sensitive LLM, including empathetic framing, inclusive language, values-based reflection, and explicit representation of marginalized identities. However, while current LLM outputs largely meet expectations for cultural sensitivity and non-offensiveness, they default to superficial rephrasing and factual recall rather than critical reflection. This guides well-being HCI design in sensitive health domains toward culturally grounded, participatory workflows.",
    "URL": "http://arxiv.org/abs/2602.00243v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.00447",
    "type": "report",
    "title": "Not All Students Engage Alike: Multi-Institution Patterns in GenAI Tutor Use",
    "author": [
      {
        "family": "Chen",
        "given": "Youjie"
      },
      {
        "family": "Shi",
        "given": "Xixi"
      },
      {
        "family": "Liu",
        "given": "Xinyu"
      },
      {
        "family": "Wang",
        "given": "Shuaiguo"
      },
      {
        "family": "Liu",
        "given": "Tracy Xiao"
      },
      {
        "family": "Ga≈°eviƒá",
        "given": "Dragan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          31
        ]
      ]
    },
    "abstract": "The emergence of generative artificial intelligence (GenAI) has created unprecedented opportunities to provide individualized learning support in classrooms as automated tutoring systems at scale. However, concerns have been raised that students may engage with these tools in ways that do not support learning. Moreover, student engagement with GenAI Tutors may vary across instructional contexts, potentially leading to unequal learning experiences. In this study, we utilize de-identified student interaction logs from an existing GenAI Tutor and the learning management system in which it is embedded. We systematically examined student engagement (N = 11,406) with the tool across 200 classes in ten post-secondary institutions through a two-stage pipeline: First, we identified four distinct engagement types at the conversation session level. In particular, 10.4% of them were \"shallow engagement\" where copy-pasting behavior was prevalent. Then, at the student level, we show that students transitioned across engagement types over time. However, students who exhibited shallow engagement with the tool were more likely to remain in this mode, whereas those who engaged deeply with the tool transitioned more flexibly across engagement types. Finally, at both the session and student levels, we show substantial heterogeneity in student engagement across institution selectivity and course disciplines. In particular, students from highly selective institutions were more likely to exhibit deep engagement. Together, our study advances the understanding of how GenAI Tutors are used in authentic educational settings and provides a framework for analyzing student engagement with GenAI Tutors, with implications for responsible implementation at scale.",
    "URL": "http://arxiv.org/abs/2602.00447v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.00762",
    "type": "report",
    "title": "WordCraft: Scaffolding the Keyword Method for L2 Vocabulary Learning with Multimodal LLMs",
    "author": [
      {
        "family": "Shao",
        "given": "Yuheng"
      },
      {
        "family": "Xiong",
        "given": "Junjie"
      },
      {
        "family": "Wu",
        "given": "Chaoran"
      },
      {
        "family": "Wang",
        "given": "Xiyuan"
      },
      {
        "family": "Zhou",
        "given": "Ziyu"
      },
      {
        "family": "Ouyang",
        "given": "Yang"
      },
      {
        "family": "Tao",
        "given": "Qinyi"
      },
      {
        "family": "Li",
        "given": "Quan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          31
        ]
      ]
    },
    "abstract": "Applying the keyword method for vocabulary memorization remains a significant challenge for L1 Chinese-L2 English learners. They frequently struggle to generate phonologically appropriate keywords, construct coherent associations, and create vivid mental imagery to aid long-term retention. Existing approaches, including fully automated keyword generation and outcome-oriented mnemonic aids, either compromise learner engagement or lack adequate process-oriented guidance. To address these limitations, we conducted a formative study with L1 Chinese-L2 English learners and educators (N=18), which revealed key difficulties and requirements in applying the keyword method to vocabulary learning. Building on these insights, we introduce WordCraft, a learner-centered interactive tool powered by Multimodal Large Language Models (MLLMs). WordCraft scaffolds the keyword method by guiding learners through keyword selection, association construction, and image formation, thereby enhancing the effectiveness of vocabulary memorization. Two user studies demonstrate that WordCraft not only preserves the generation effect but also achieves high levels of effectiveness and usability.",
    "URL": "http://arxiv.org/abs/2602.00762v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.00850",
    "type": "report",
    "title": "PS$^2$: Parameterized Control for Fine-Grained Student Proficiency Simulation",
    "author": [
      {
        "family": "Liu",
        "given": "Ruochen"
      },
      {
        "family": "Wen",
        "given": "Zhiyuan"
      },
      {
        "family": "Yan",
        "given": "Hao"
      },
      {
        "family": "Yin",
        "given": "Jun"
      },
      {
        "family": "Wang",
        "given": "Senzhang"
      },
      {
        "family": "Cao",
        "given": "Jiannong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          31
        ]
      ]
    },
    "abstract": "Understanding how students with different proficiency levels respond to educational materials is a critical issue within the field of AI for Education. However, acquiring sufficient real student response data for a robust evaluation is often hindered by cost, ethics, and security constraints. Consequently, LLM-based student proficiency simulation, especially prompt-based methods, has emerged as a practical alternative under data-scarce conditions. Despite their promise, current methods still exhibit limited controllability with coarse-grained proficiency representations, high sensitivity to prompt design, and the lack of calibration with academic performance. Therefore, we propose Parameterized Student Proficiency Simulation (PS$^2$), an unsupervised and parameterized model-level framework that simulates students with different proficiencies by interpolating between a strong upper-bound LLM and a weaker, cognitive error-informed lower-bound student LLM via a hybrid ratio. Specifically, the lower-bound model is constructed by fine-tuning the weaker LM to exhibit cognitive errors when responding to educational materials. To ensure alignment with target proficiency levels, PS$^2$ further calibrates the interpolation ratio with academic performance. Experiments on two public datasets demonstrate that PS$^2$ achieves finer-grained and consistent proficiency simulation compared to existing baselines, leading to superior performance in student behavior similarity and item difficulty prediction.",
    "URL": "http://arxiv.org/abs/2602.00850v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.00979",
    "type": "report",
    "title": "GradingAttack: Attacking Large Language Models Towards Short Answer Grading Ability",
    "author": [
      {
        "family": "Li",
        "given": "Xueyi"
      },
      {
        "family": "Zhou",
        "given": "Zhuoneng"
      },
      {
        "family": "Liu",
        "given": "Zitao"
      },
      {
        "family": "Wu",
        "given": "Yongdong"
      },
      {
        "family": "Luo",
        "given": "Weiqi"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          1
        ]
      ]
    },
    "abstract": "Large language models (LLMs) have demonstrated remarkable potential for automatic short answer grading (ASAG), significantly boosting student assessment efficiency and scalability in educational scenarios. However, their vulnerability to adversarial manipulation raises critical concerns about automatic grading fairness and reliability. In this paper, we introduce GradingAttack, a fine-grained adversarial attack framework that systematically evaluates the vulnerability of LLM based ASAG models. Specifically, we align general-purpose attack methods with the specific objectives of ASAG by designing token-level and prompt-level strategies that manipulate grading outcomes while maintaining high camouflage. Furthermore, to quantify attack camouflage, we propose a novel evaluation metric that balances attack success and camouflage. Experiments on multiple datasets demonstrate that both attack strategies effectively mislead grading models, with prompt-level attacks achieving higher success rates and token-level attacks exhibiting superior camouflage capability. Our findings underscore the need for robust defenses to ensure fairness and reliability in ASAG. Our code and datasets are available at https://anonymous.4open.science/r/GradingAttack.",
    "URL": "http://arxiv.org/abs/2602.00979v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.01015",
    "type": "report",
    "title": "Large Language Models as Students Who Think Aloud: Overly Coherent, Verbose, and Confident",
    "author": [
      {
        "family": "Borchers",
        "given": "Conrad"
      },
      {
        "family": "Vie",
        "given": "Jill-J√™nn"
      },
      {
        "family": "Azevedo",
        "given": "Roger"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          1
        ]
      ]
    },
    "abstract": "Large language models (LLMs) are increasingly embedded in AI-based tutoring systems. Can they faithfully model novice reasoning and metacognitive judgments? Existing evaluations emphasize problem-solving accuracy, overlooking the fragmented and imperfect reasoning that characterizes human learning. We evaluate LLMs as novices using 630 think-aloud utterances from multi-step chemistry tutoring problems with problem-solving logs of student hint use, attempts, and problem context. We compare LLM-generated reasoning to human learner utterances under minimal and extended contextual prompting, and assess the models' ability to predict step-level learner success. Although GPT-4.1 generates fluent and contextually appropriate continuations, its reasoning is systematically over-coherent, verbose, and less variable than human think-alouds. These effects intensify with a richer problem-solving context during prompting. Learner performance was consistently overestimated. These findings highlight epistemic limitations of simulating learning with LLMs. We attribute these limitations to LLM training data, including expert-like solutions devoid of expressions of affect and working memory constraints during problem solving. Our evaluation framework can guide future design of adaptive systems that more faithfully support novice learning and self-regulation using generative artificial intelligence.",
    "URL": "http://arxiv.org/abs/2602.01015v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.01169",
    "type": "report",
    "title": "PedagoSense: A Pedology Grounded LLM System for Pedagogical Strategy Detection and Contextual Response Generation in Learning Dialogues",
    "author": [
      {
        "family": "Sultan",
        "given": "Shahem"
      },
      {
        "family": "Fadi",
        "given": "Shahem"
      },
      {
        "family": "Melhim",
        "given": "Yousef"
      },
      {
        "family": "Alsarraj",
        "given": "Ibrahim"
      },
      {
        "family": "Hassan",
        "given": "Besher"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          1
        ]
      ]
    },
    "abstract": "This paper addresses the challenge of improving interaction quality in dialogue based learning by detecting and recommending effective pedagogical strategies in tutor student conversations. We introduce PedagoSense, a pedology grounded system that combines a two stage strategy classifier with large language model generation. The system first detects whether a pedagogical strategy is present using a binary classifier, then performs fine grained classification to identify the specific strategy. In parallel, it recommends an appropriate strategy from the dialogue context and uses an LLM to generate a response aligned with that strategy. We evaluate on human annotated tutor student dialogues, augmented with additional non pedagogical conversations for the binary task. Results show high performance for pedagogical strategy detection and consistent gains when using data augmentation, while analysis highlights where fine grained classes remain challenging. Overall, PedagoSense bridges pedagogical theory and practical LLM based response generation for more adaptive educational technologies.",
    "URL": "http://arxiv.org/abs/2602.01169v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.01249",
    "type": "report",
    "title": "Generative AI in Signal Processing Education: An Audio Foundation Model Based Approach",
    "author": [
      {
        "family": "Khan",
        "given": "Muhammad Salman"
      },
      {
        "family": "Ullah",
        "given": "Ahmad"
      },
      {
        "family": "Latif",
        "given": "Siddique"
      },
      {
        "family": "Qadir",
        "given": "Junaid"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          1
        ]
      ]
    },
    "abstract": "Audio Foundation Models (AFMs), a specialized category of Generative AI (GenAI), have the potential to transform signal processing (SP) education by integrating core applications such as speech and audio enhancement, denoising, source separation, feature extraction, automatic classification, and real-time signal analysis into learning and research. This paper introduces SPEduAFM, a conceptual AFM tailored for SP education, bridging traditional SP principles with GenAI-driven innovations. Through an envisioned case study, we outline how AFMs can enable a range of applications, including automated lecture transcription, interactive demonstrations, and inclusive learning tools, showcasing their potential to transform abstract concepts into engaging, practical experiences. This paper also addresses challenges such as ethics, explainability, and customization by highlighting dynamic, real-time auditory interactions that foster experiential and authentic learning. By presenting SPEduAFM as a forward-looking vision, we aim to inspire broader adoption of GenAI in engineering education, enhancing accessibility, engagement, and innovation in the classroom and beyond.",
    "URL": "http://arxiv.org/abs/2602.01249v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.01415",
    "type": "report",
    "title": "Evidence-Decision-Feedback: Theory-Driven Adaptive Scaffolding for LLM Agents",
    "author": [
      {
        "family": "Cohn",
        "given": "Clayton"
      },
      {
        "family": "Guo",
        "given": "Siyuan"
      },
      {
        "family": "Rayala",
        "given": "Surya"
      },
      {
        "family": "Wang",
        "given": "Hanchen David"
      },
      {
        "family": "Mohammed",
        "given": "Naveeduddin"
      },
      {
        "family": "Timalsina",
        "given": "Umesh"
      },
      {
        "family": "Jain",
        "given": "Shruti"
      },
      {
        "family": "Eeds",
        "given": "Angela"
      },
      {
        "family": "Deweese",
        "given": "Menton"
      },
      {
        "family": "Popp",
        "given": "Pamela J. Osborn"
      },
      {
        "family": "Stanton",
        "given": "Rebekah"
      },
      {
        "family": "Walker",
        "given": "Shakeera"
      },
      {
        "family": "Ma",
        "given": "Meiyi"
      },
      {
        "family": "Biswas",
        "given": "Gautam"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          1
        ]
      ]
    },
    "abstract": "Multi-agent LLM architectures offer opportunities for pedagogical agents to help students construct domain knowledge and develop critical-thinking skills, yet many operate on a \"one-size-fits-all\" basis, limiting their ability to provide personalized support. To address this, we introduce Evidence-Decision-Feedback (EDF), a theoretical framework for adaptive scaffolding using LLMs. EDF integrates elements of intelligent tutoring systems and agentic behavior by organizing interactions around evidentiary inference, pedagogical decision-making, and adaptive feedback. We instantiate EDF through Copa, an agentic collaborative peer agent for STEM+C problem-solving. In an authentic high school classroom study, we show that EDF-aligned interactions align feedback with students' demonstrated understanding and task mastery; promote gradual scaffold fading; and support interpretable, evidence-grounded explanations without fostering overreliance.",
    "URL": "http://arxiv.org/abs/2602.01415v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.01560",
    "type": "report",
    "title": "Argument Rarity-based Originality Assessment for AI-Assisted Writing",
    "author": [
      {
        "family": "Inoshita",
        "given": "Keito"
      },
      {
        "family": "Omura",
        "given": "Michiaki"
      },
      {
        "family": "Yamanaka",
        "given": "Tsukasa"
      },
      {
        "family": "Maeda",
        "given": "Go"
      },
      {
        "family": "Tsuji",
        "given": "Kentaro"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          2
        ]
      ]
    },
    "abstract": "As Large Language Models (LLMs) have become capable of effortlessly generating high-quality text, traditional quality-focused writing assessment is losing its significance. If the essential goal of education is to foster critical thinking and original perspectives, assessment must also shift its paradigm from quality to originality. This study proposes Argument Rarity-based Originality Assessment (AROA), a framework for automatically evaluating argumentative originality in student essays. AROA defines originality as rarity within a reference corpus and evaluates it through four complementary components: structural rarity, claim rarity, evidence rarity, and cognitive depth. The framework quantifies the rarity of each component using density estimation and integrates them with a quality adjustment mechanism, thereby treating quality and originality as independent evaluation axes. Experiments using human essays and AI-generated essays revealed a strong negative correlation between quality and claim rarity, demonstrating a quality-originality trade-off where higher-quality texts tend to rely on typical claim patterns. Furthermore, while AI essays achieved comparable levels of structural complexity to human essays, their claim rarity was substantially lower than that of humans, indicating that LLMs can reproduce the form of argumentation but have limitations in the originality of content.",
    "URL": "http://arxiv.org/abs/2602.01560v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.01703",
    "type": "report",
    "title": "$\\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality",
    "author": [
      {
        "family": "Li",
        "given": "Pengyu"
      },
      {
        "family": "Zhang",
        "given": "Lingling"
      },
      {
        "family": "Gao",
        "given": "Zhitao"
      },
      {
        "family": "Wu",
        "given": "Yanrui"
      },
      {
        "family": "Dong",
        "given": "Yuxuan"
      },
      {
        "family": "Liu",
        "given": "Huan"
      },
      {
        "family": "Wei",
        "given": "Bifan"
      },
      {
        "family": "Liu",
        "given": "Jun"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          2
        ]
      ]
    },
    "abstract": "While Large Language Models (LLMs) have achieved remarkable capabilities, they unintentionally memorize sensitive data, posing critical privacy and security risks. Machine unlearning is pivotal for mitigating these risks, yet existing paradigms face a fundamental dilemma: aggressive unlearning often induces catastrophic forgetting that degrades model utility, whereas conservative strategies risk superficial forgetting, leaving models vulnerable to adversarial recovery. To address this trade-off, we propose $\\textbf{AGT$^{AO}$}$ (Adversarial Gating Training with Adaptive Orthogonality), a unified framework designed to reconcile robust erasure with utility preservation. Specifically, our approach introduces $\\textbf{Adaptive Orthogonality (AO)}$ to dynamically mitigate geometric gradient conflicts between forgetting and retention objectives, thereby minimizing unintended knowledge degradation. Concurrently, $\\textbf{Adversarial Gating Training (AGT)}$ formulates unlearning as a latent-space min-max game, employing a curriculum-based gating mechanism to simulate and counter internal recovery attempts. Extensive experiments demonstrate that $\\textbf{AGT$^{AO}$}$ achieves a superior trade-off between unlearning efficacy (KUR $\\approx$ 0.01) and model utility (MMLU 58.30). Code is available at https://github.com/TiezMind/AGT-unlearning.",
    "URL": "http://arxiv.org/abs/2602.01703v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.01875",
    "type": "report",
    "title": "PretrainRL: Alleviating Factuality Hallucination of Large Language Models at the Beginning",
    "author": [
      {
        "family": "Liu",
        "given": "Langming"
      },
      {
        "family": "Lv",
        "given": "Kangtao"
      },
      {
        "family": "Chen",
        "given": "Haibin"
      },
      {
        "family": "Zhang",
        "given": "Weidong"
      },
      {
        "family": "Wang",
        "given": "Yejing"
      },
      {
        "family": "Liu",
        "given": "Shilei"
      },
      {
        "family": "Tong",
        "given": "Xin"
      },
      {
        "family": "Yuan",
        "given": "Yujin"
      },
      {
        "family": "Wang",
        "given": "Yongwei"
      },
      {
        "family": "Su",
        "given": "Wenbo"
      },
      {
        "family": "Zheng",
        "given": "Bo"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          2
        ]
      ]
    },
    "abstract": "Large language models (LLMs), despite their powerful capabilities, suffer from factual hallucinations where they generate verifiable falsehoods. We identify a root of this issue: the imbalanced data distribution in the pretraining corpus, which leads to a state of \"low-probability truth\" and \"high-probability falsehood\". Recent approaches, such as teaching models to say \"I don't know\" or post-hoc knowledge editing, either evade the problem or face catastrophic forgetting. To address this issue from its root, we propose \\textbf{PretrainRL}, a novel framework that integrates reinforcement learning into the pretraining phase to consolidate factual knowledge. The core principle of PretrainRL is \"\\textbf{debiasing then learning}.\" It actively reshapes the model's probability distribution by down-weighting high-probability falsehoods, thereby making \"room\" for low-probability truths to be learned effectively. To enable this, we design an efficient negative sampling strategy to discover these high-probability falsehoods and introduce novel metrics to evaluate the model's probabilistic state concerning factual knowledge. Extensive experiments on three public benchmarks demonstrate that PretrainRL significantly alleviates factual hallucinations and outperforms state-of-the-art methods.",
    "URL": "http://arxiv.org/abs/2602.01875v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.01919",
    "type": "report",
    "title": "From Code-Centric to Concept-Centric: Teaching NLP with LLM-Assisted \"Vibe Coding\"",
    "author": [
      {
        "family": "Al-Khalifa",
        "given": "Hend"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          2
        ]
      ]
    },
    "abstract": "The rapid advancement of Large Language Models (LLMs) presents both challenges and opportunities for Natural Language Processing (NLP) education. This paper introduces ``Vibe Coding,'' a pedagogical approach that leverages LLMs as coding assistants while maintaining focus on conceptual understanding and critical thinking. We describe the implementation of this approach in a senior-level undergraduate NLP course, where students completed seven labs using LLMs for code generation while being assessed primarily on conceptual understanding through critical reflection questions. Analysis of end-of-course feedback from 19 students reveals high satisfaction (mean scores 4.4-4.6/5.0) across engagement, conceptual learning, and assessment fairness. Students particularly valued the reduced cognitive load from debugging, enabling deeper focus on NLP concepts. However, challenges emerged around time constraints, LLM output verification, and the need for clearer task specifications. Our findings suggest that when properly structured with mandatory prompt logging and reflection-based assessment, LLM-assisted learning can shift focus from syntactic fluency to conceptual mastery, preparing students for an AI-augmented professional landscape.",
    "URL": "http://arxiv.org/abs/2602.01919v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.01933",
    "type": "report",
    "title": "Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling",
    "author": [
      {
        "family": "Boissier",
        "given": "Fabrice"
      },
      {
        "family": "Sen",
        "given": "Monica"
      },
      {
        "family": "Rychkova",
        "given": "Irina"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          2
        ]
      ]
    },
    "abstract": "Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.",
    "URL": "http://arxiv.org/abs/2602.01933v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.01937",
    "type": "report",
    "title": "T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation",
    "author": [
      {
        "family": "Guo",
        "given": "Suhan"
      },
      {
        "family": "Wang",
        "given": "Bingxu"
      },
      {
        "family": "Zhang",
        "given": "Shaodan"
      },
      {
        "family": "Shen",
        "given": "Furao"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          2
        ]
      ]
    },
    "abstract": "Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effectiveness of scale-driven pretraining alone. This time-bound constraint poses a challenge for enabling large language models (LLMs) to acquire forecasting capability, as existing approaches primarily rely on representation-level alignment or inference-time temporal modules rather than explicitly teaching forecasting behavior to the LLM. We propose T-LLM, a temporal distillation framework that equips general-purpose LLMs with time series forecasting capability by transferring predictive behavior from a lightweight temporal teacher during training. The teacher combines trend modeling and frequency-domain analysis to provide structured temporal supervision, and is removed entirely at inference, leaving the LLM as the sole forecasting model. Experiments on benchmark datasets and infectious disease forecasting tasks demonstrate that T-LLM consistently outperforms existing LLM-based forecasting methods under full-shot, few-shot, and zero-shot settings, while enabling a simple and efficient deployment pipeline.",
    "URL": "http://arxiv.org/abs/2602.01937v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.01982",
    "type": "report",
    "title": "S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs",
    "author": [
      {
        "family": "Du",
        "given": "Yanrui"
      },
      {
        "family": "Zhao",
        "given": "Sendong"
      },
      {
        "family": "Gao",
        "given": "Yibo"
      },
      {
        "family": "Zhao",
        "given": "Danyang"
      },
      {
        "family": "Lin",
        "given": "Qika"
      },
      {
        "family": "Ma",
        "given": "Ming"
      },
      {
        "family": "Li",
        "given": "Jiayun"
      },
      {
        "family": "Jiang",
        "given": "Yi"
      },
      {
        "family": "He",
        "given": "Kai"
      },
      {
        "family": "Xu",
        "given": "Qianyi"
      },
      {
        "family": "Qin",
        "given": "Bing"
      },
      {
        "family": "Feng",
        "given": "Mengling"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          2
        ]
      ]
    },
    "abstract": "Large language models (LLMs) equipped with chain-of-thought (CoT) achieve strong performance and offer a window into LLM behavior. However, recent evidence suggests that improvements in CoT capabilities often come with redundant reasoning processes, motivating a key question: Can LLMs acquire a fast-thinking mode analogous to human System 1 reasoning? To explore this, our study presents a self-sampling framework based on activation steering for efficient CoT learning. Our method can induce style-aligned and variable-length reasoning traces from target LLMs themselves without any teacher guidance, thereby alleviating a central bottleneck of SFT-based methods-the scarcity of high-quality supervision data. Using filtered data by gold answers, we perform SFT for efficient CoT learning with (i) a human-like dual-cognitive system, and (ii) a progressive compression curriculum. Furthermore, we explore a self-evolution regime in which SFT is driven solely by prediction-consistent data of variable-length variants, eliminating the need for gold answers. Extensive experiments on math benchmarks, together with cross-domain generalization tests in medicine, show that our method yields stable improvements for both general and R1-style LLMs. Our data and model checkpoints can be found at https://github.com/DYR1/S3-CoT.",
    "URL": "http://arxiv.org/abs/2602.01982v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.02366",
    "type": "report",
    "title": "ReasonCACHE: Teaching LLMs To Reason Without Weight Updates",
    "author": [
      {
        "family": "Gupta",
        "given": "Sharut"
      },
      {
        "family": "Isola",
        "given": "Phillip"
      },
      {
        "family": "Jegelka",
        "given": "Stefanie"
      },
      {
        "family": "Lopez-Paz",
        "given": "David"
      },
      {
        "family": "Ahuja",
        "given": "Kartik"
      },
      {
        "family": "Ibrahim",
        "given": "Mark"
      },
      {
        "family": "Pezeshki",
        "given": "Mohammad"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          2
        ]
      ]
    },
    "abstract": "Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/",
    "URL": "http://arxiv.org/abs/2602.02366v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.02414",
    "type": "report",
    "title": "Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank",
    "author": [
      {
        "family": "Mitton",
        "given": "Joshua"
      },
      {
        "family": "Bhattacharyya",
        "given": "Prarthana"
      },
      {
        "family": "Smith",
        "given": "Digory"
      },
      {
        "family": "Christie",
        "given": "Thomas"
      },
      {
        "family": "Abboud",
        "given": "Ralph"
      },
      {
        "family": "Woodhead",
        "given": "Simon"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          2
        ]
      ]
    },
    "abstract": "Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.",
    "URL": "http://arxiv.org/abs/2602.02414v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.02794",
    "type": "report",
    "title": "Reshaping Perception Through Technology: From Ancient Script to Large Language Models",
    "author": [
      {
        "family": "Pourdavood",
        "given": "Parham"
      },
      {
        "family": "Jacob",
        "given": "Michael"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          2
        ]
      ]
    },
    "abstract": "As large language models reshape how we create and access information, questions arise about how to frame their role in human creative and cognitive life. We argue that AI is best understood not as artificial intelligence but as a new medium -- one that, like writing before it, reshapes perception and enables novel forms of creativity. Drawing on Marshall McLuhan's insight that \"the medium is the massage,\" we trace a lineage of technologies -- from DNA and the nervous system to symbols, writing, and now LLMs -- that mold cognition through a shared logic of flexible unfolding and co-creation. We observe that as technologies become more externalized and decoupled from physiology, they introduce both greater creative potential and greater risk of inauthenticity and manipulation. This tension is acute with LLMs, but not unprecedented: ancient responses to writing reveal a recurring human tendency to project intelligence onto powerful new media. Rather than viewing AI as a competitor, we propose framing it as a medium that foregrounds artistic skills: aesthetic judgment, curation, and the articulation of vision. We discuss implications for education, creative practice, and how society might adapt to this new medium as it did to writing.",
    "URL": "http://arxiv.org/abs/2602.02794v2",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.02878",
    "type": "report",
    "title": "Which course? Discourse! Teaching Discourse and Generation in the Era of LLMs",
    "author": [
      {
        "family": "Li",
        "given": "Junyi Jessy"
      },
      {
        "family": "Liu",
        "given": "Yang Janet"
      },
      {
        "family": "Misra",
        "given": "Kanishka"
      },
      {
        "family": "Pyatkin",
        "given": "Valentina"
      },
      {
        "family": "Sheffield",
        "given": "William"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          2
        ]
      ]
    },
    "abstract": "The field of NLP has undergone vast, continuous transformations over the past few years, sparking debates going beyond discipline boundaries. This begs important questions in education: how do we design courses that bridge sub-disciplines in this shifting landscape? This paper explores this question from the angle of discourse processing, an area with rich linguistic insights and computational models for the intentional, attentional, and coherence structure of language. Discourse is highly relevant for open-ended or long-form text generation, yet this connection is under-explored in existing undergraduate curricula. We present a new course, \"Computational Discourse and Natural Language Generation\". The course is collaboratively designed by a team with complementary expertise and was offered for the first time in Fall 2025 as an upper-level undergraduate course, cross-listed between Linguistics and Computer Science. Our philosophy is to deeply integrate the theoretical and empirical aspects, and create an exploratory mindset inside the classroom and in the assignments. This paper describes the course in detail and concludes with takeaways from an independent survey as well as our vision for future directions.",
    "URL": "http://arxiv.org/abs/2602.02878v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.02994",
    "type": "report",
    "title": "Video-OPD: Efficient Post-Training of Multimodal Large Language Models for Temporal Video Grounding via On-Policy Distillation",
    "author": [
      {
        "family": "Li",
        "given": "Jiaze"
      },
      {
        "family": "Yin",
        "given": "Hao"
      },
      {
        "family": "Xu",
        "given": "Haoran"
      },
      {
        "family": "Xu",
        "given": "Boshen"
      },
      {
        "family": "Tan",
        "given": "Wenhui"
      },
      {
        "family": "He",
        "given": "Zewen"
      },
      {
        "family": "Ju",
        "given": "Jianzhong"
      },
      {
        "family": "Luo",
        "given": "Zhenbo"
      },
      {
        "family": "Luan",
        "given": "Jian"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          3
        ]
      ]
    },
    "abstract": "Reinforcement learning has emerged as a principled post-training paradigm for Temporal Video Grounding (TVG) due to its on-policy optimization, yet existing GRPO-based methods remain fundamentally constrained by sparse reward signals and substantial computational overhead. We propose Video-OPD, an efficient post-training framework for TVG inspired by recent advances in on-policy distillation. Video-OPD optimizes trajectories sampled directly from the current policy, thereby preserving alignment between training and inference distributions, while a frontier teacher supplies dense, token-level supervision via a reverse KL divergence objective. This formulation preserves the on-policy property critical for mitigating distributional shift, while converting sparse, episode-level feedback into fine-grained, step-wise learning signals. Building on Video-OPD, we introduce Teacher-Validated Disagreement Focusing (TVDF), a lightweight training curriculum that iteratively prioritizes trajectories that are both teacher-reliable and maximally informative for the student, thereby improving training efficiency. Empirical results demonstrate that Video-OPD consistently outperforms GRPO while achieving substantially faster convergence and lower computational cost, establishing on-policy distillation as an effective alternative to conventional reinforcement learning for TVG.",
    "URL": "http://arxiv.org/abs/2602.02994v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.03022",
    "type": "report",
    "title": "STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models",
    "author": [
      {
        "family": "Ni",
        "given": "Jiliang"
      },
      {
        "family": "Pu",
        "given": "Jiachen"
      },
      {
        "family": "Yang",
        "given": "Zhongyi"
      },
      {
        "family": "Luo",
        "given": "Jingfeng"
      },
      {
        "family": "Hu",
        "given": "Conggang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          3
        ]
      ]
    },
    "abstract": "The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.",
    "URL": "http://arxiv.org/abs/2602.03022v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.03084",
    "type": "report",
    "title": "AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback",
    "author": [
      {
        "family": "Gao",
        "given": "Zhitao"
      },
      {
        "family": "Ma",
        "given": "Jie"
      },
      {
        "family": "Li",
        "given": "Xuhong"
      },
      {
        "family": "Li",
        "given": "Pengyu"
      },
      {
        "family": "Qu",
        "given": "Ning"
      },
      {
        "family": "Wu",
        "given": "Yaqiang"
      },
      {
        "family": "Liu",
        "given": "Hui"
      },
      {
        "family": "Liu",
        "given": "Jun"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          3
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) have achieved significant success in complex reasoning but remain bottlenecked by reliance on expert-annotated data and external verifiers. While existing self-evolution paradigms aim to bypass these constraints, they often fail to identify the optimal learning zone and risk reinforcing collective hallucinations and incorrect priors through flawed internal feedback. To address these challenges, we propose \\underline{A}utonomous \\underline{E}volutionary \\underline{R}easoning \\underline{O}ptimization (AERO), an unsupervised framework that achieves autonomous reasoning evolution by internalizing self-questioning, answering, and criticism within a synergistic dual-loop system. Inspired by the \\textit{Zone of Proximal Development (ZPD)} theory, AERO utilizes entropy-based positioning to target the ``solvability gap'' and employs Independent Counterfactual Correction for robust verification. Furthermore, we introduce a Staggered Training Strategy to synchronize capability growth across functional roles and prevent curriculum collapse. Extensive evaluations across nine benchmarks spanning three domains demonstrate that AERO achieves average performance improvements of 4.57\\% on Qwen3-4B-Base and 5.10\\% on Qwen3-8B-Base, outperforming competitive baselines. Code is available at https://github.com/mira-ai-lab/AERO.",
    "URL": "http://arxiv.org/abs/2602.03084v2",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.03230",
    "type": "report",
    "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
    "author": [
      {
        "family": "Liu",
        "given": "Shaoyu"
      },
      {
        "family": "Li",
        "given": "Jianing"
      },
      {
        "family": "Zhao",
        "given": "Guanghui"
      },
      {
        "family": "Zhang",
        "given": "Yunjian"
      },
      {
        "family": "Jiang",
        "given": "Wen"
      },
      {
        "family": "Li",
        "given": "Ming"
      },
      {
        "family": "Ji",
        "given": "Xiangyang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          3
        ]
      ]
    },
    "abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision.",
    "URL": "http://arxiv.org/abs/2602.03230v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.03249",
    "type": "report",
    "title": "Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning",
    "author": [
      {
        "family": "Yang",
        "given": "Zhicheng"
      },
      {
        "family": "Guo",
        "given": "Zhijiang"
      },
      {
        "family": "Huang",
        "given": "Yinya"
      },
      {
        "family": "Wang",
        "given": "Yongxin"
      },
      {
        "family": "Shi",
        "given": "Wenlei"
      },
      {
        "family": "Wang",
        "given": "Yiwei"
      },
      {
        "family": "Liang",
        "given": "Xiaodan"
      },
      {
        "family": "Tang",
        "given": "Jing"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          3
        ]
      ]
    },
    "abstract": "Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.",
    "URL": "http://arxiv.org/abs/2602.03249v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.03542",
    "type": "report",
    "title": "Can Large Language Models Generalize Procedures Across Representations?",
    "author": [
      {
        "family": "Lin",
        "given": "Fangru"
      },
      {
        "family": "Hofmann",
        "given": "Valentin"
      },
      {
        "family": "Wan",
        "given": "Xingchen"
      },
      {
        "family": "Wang",
        "given": "Weixing"
      },
      {
        "family": "Ding",
        "given": "Zifeng"
      },
      {
        "family": "Cohn",
        "given": "Anthony G."
      },
      {
        "family": "Pierrehumbert",
        "given": "Janet B."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          3
        ]
      ]
    },
    "abstract": "Large language models (LLMs) are trained and tested extensively on symbolic representations such as code and graphs, yet real-world user tasks are often specified in natural language. To what extent can LLMs generalize across these representations? Here, we approach this question by studying isomorphic tasks involving procedures represented in code, graphs, and natural language (e.g., scheduling steps in planning). We find that training LLMs with popular post-training methods on graphs or code data alone does not reliably generalize to corresponding natural language tasks, while training solely on natural language can lead to inefficient performance gains. To address this gap, we propose a two-stage data curriculum that first trains on symbolic, then natural language data. The curriculum substantially improves model performance across model families and tasks. Remarkably, a 1.5B Qwen model trained by our method can closely match zero-shot GPT-4o in naturalistic planning. Finally, our analysis suggests that successful cross-representation generalization can be interpreted as a form of generative analogy, which our curriculum effectively encourages.",
    "URL": "http://arxiv.org/abs/2602.03542v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.03679",
    "type": "report",
    "title": "Footprints of the Walking of Numbers: A Dynamic Visualization Task for Understanding Decimal Numbers in Secondary Education",
    "author": [
      {
        "family": "Serrano",
        "given": "Felix De la Cruz"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          3
        ]
      ]
    },
    "abstract": "The study of decimal numbers in secondary education is often approached from algorithmic perspectives, which limits students' understanding of their structure. This paper presents the task Footprints of the Walking of Numbers, a dynamic visualization proposal aimed at supporting the understanding of decimal numbers through the exploration of their infinite decimal expansions. The task is based on assigning vectors to the decimal digits from 0 to 9, so that the sequence of digits of a number generates a dynamic geometric path in the plane. Through the use of GeoGebra as a visualization environment, students can observe, compare, and interpret traces associated with different types of numbers, such as terminating decimals, repeating decimals, and irrational numbers, identifying visual regularities linked to their decimal behavior. The analysis is developed from a theoretical-didactical perspective, using the Mathematical Working Space as an interpretative lens to characterize the potential of the task design. In addition, the paper discusses the punctual use of generative AI tools exclusively as instrumental support for computation, without shifting the focus away from mathematical reasoning.",
    "URL": "http://arxiv.org/abs/2602.03679v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.03900",
    "type": "report",
    "title": "Knowledge Model Prompting Increases LLM Performance on Planning Tasks",
    "author": [
      {
        "family": "Goh",
        "given": "Erik"
      },
      {
        "family": "Kos",
        "given": "John"
      },
      {
        "family": "Goel",
        "given": "Ashok"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          3
        ]
      ]
    },
    "abstract": "Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.",
    "URL": "http://arxiv.org/abs/2602.03900v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.03950",
    "type": "report",
    "title": "Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation",
    "author": [
      {
        "family": "Basarkar",
        "given": "Aditya"
      },
      {
        "family": "Tabarsi",
        "given": "Benyamin"
      },
      {
        "family": "Barnes",
        "given": "Tiffany"
      },
      {
        "literal": "Dongkuan"
      },
      {
        "literal": "Xu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          3
        ]
      ]
    },
    "abstract": "Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.",
    "URL": "http://arxiv.org/abs/2602.03950v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.03962",
    "type": "report",
    "title": "Automatic Classification of Pedagogical Materials against CS Curriculum Guidelines",
    "author": [
      {
        "family": "Saule",
        "given": "Erik"
      },
      {
        "family": "Subramanian",
        "given": "Kalpathi"
      },
      {
        "family": "Bunescu",
        "given": "Razvan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          3
        ]
      ]
    },
    "abstract": "Professional societies often publish curriculum guidelines to help programs align their content to international standards. In Computer Science, the primary standard is published by ACM and IEEE and provide detailed guidelines for what should be and could be included in a Computer Science program.\n  While very helpful, it remains difficult for program administrators to assess how much of the guidelines is being covered by a CS program. This is in particular due to the extensiveness of the guidelines, containing thousands of individual items. As such, it is time consuming and cognitively demanding to audit every course to confidently mark everything that is actually being covered. Our preliminary work indicated that it takes about a day of work per course.\n  In this work, we propose using Natural Language Processing techniques to accelerate the process. We explore two kinds of techniques, the first relying on traditional tools for parsing, tagging, and embeddings, while the second leverages the power of Large Language Models. We evaluate the application of these techniques to classify a corpus of pedagogical materials and show that we can meaningfully classify documents automatically.",
    "URL": "http://arxiv.org/abs/2602.03962v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.04023",
    "type": "report",
    "title": "Exploring Emerging Norms of AI Disclosure in Programming Education",
    "author": [
      {
        "family": "Ye",
        "given": "Runlong"
      },
      {
        "family": "Huang",
        "given": "Oliver"
      },
      {
        "family": "He",
        "given": "Jessica"
      },
      {
        "family": "Liut",
        "given": "Michael"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          3
        ]
      ]
    },
    "abstract": "Generative AI blurs the lines of authorship in computing education, creating uncertainty around how students should attribute AI assistance. To examine these emerging norms, we conducted a factorial vignette study with 94 computer science students across 102 unique scenarios, systematically manipulating assessment type, AI autonomy, student activity, prior knowledge, and human refinement effort. This paper details how these factors influence students' perceptions of ownership and disclosure preferences. Our findings indicate that attribution judgments are primarily driven by different levels of AI assistance and human refinement. We also found that students' perception of authorship significantly predicts their policy expectations. We conclude by proposing a shift from statement-style policies to process-oriented attribution, transforming disclosure into a pedagogical mechanism for fostering critical engagement with AI-generated content.",
    "URL": "http://arxiv.org/abs/2602.04023v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.04278",
    "type": "report",
    "title": "MiniRec: Data-Efficient Reinforcement Learning for LLM-based Recommendation",
    "author": [
      {
        "family": "Wang",
        "given": "Lin"
      },
      {
        "family": "Zhang",
        "given": "Yang"
      },
      {
        "family": "Chen",
        "given": "Jingfan"
      },
      {
        "family": "Zhao",
        "given": "Xiaoyan"
      },
      {
        "family": "Zhu",
        "given": "Fengbin"
      },
      {
        "family": "Li",
        "given": "Qing"
      },
      {
        "family": "Chua",
        "given": "Tat-Seng"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          4
        ]
      ]
    },
    "abstract": "The integration of reinforcement learning (RL) into large language models (LLMs) has opened new opportunities for recommender systems by eliciting reasoning and improving user preference modeling. However, RL-based LLM recommendation faces significant efficiency challenges, making full-data training costly. Existing data selection methods define sample value based on learnability or representativeness, yet their loss- or gradient-driven or dataset coverage-driven criteria often misalign with RL learning dynamics, resulting in suboptimal performance. To address this, we propose MiniRec, a data selection framework tailored for RL-based LLM recommendation. MiniRec evaluates sample learnability using key RL signals -- rewards -- pruning samples that are too easy (too high reward) or too difficult (consistently low reward). It assesses representativeness by aligning sample gradients with the approximated \"ideal\" global RL optimization trajectory, selecting samples that mainly drive model updates, and it also enforces diversity to reduce redundancy. Combined with a curriculum learning strategy from easy to hard samples, MiniRec significantly reduces training cost while largely preserving performance. Extensive experiments demonstrate MiniRec's effectiveness, highlighting the importance of reward-aligned, trajectory-informed data selection in RL-based LLM recommendation.",
    "URL": "http://arxiv.org/abs/2602.04278v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.04389",
    "type": "report",
    "title": "Lessons Learned from Integrating Generative AI into an Introductory Undergraduate Astronomy Course at Harvard",
    "author": [
      {
        "family": "Stubbs",
        "given": "Christopher W."
      },
      {
        "family": "Huang",
        "given": "Dongpeng"
      },
      {
        "family": "Koh",
        "given": "Jungyoon"
      },
      {
        "family": "Woods",
        "given": "Madeleine"
      },
      {
        "family": "Malag√≥n",
        "given": "Andr√©s A. Plazas"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          4
        ]
      ]
    },
    "abstract": "We describe our efforts to fully integrate generative artificial intelligence (GAI) into an introductory undergraduate astronomy course. Ordered by student perception of utility, GAI was used in instructional Python notebooks, in a subset of assignments, for student presentation preparations, and as a participant (in conjunction with a RAG-encoded textbook) in a course Slack channel. Assignments were divided into GAI-encouraged and GAI-discouraged. We incentivized student mastery of the material through midterm and final exams in which electronics were not allowed. Student evaluations of the course showed no reduction compared to the non-GAI version from the previous year.",
    "URL": "http://arxiv.org/abs/2602.04389v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.04392",
    "type": "report",
    "title": "Evaluating the Presence of Sex Bias in Clinical Reasoning by Large Language Models",
    "author": [
      {
        "family": "Tsintsiper",
        "given": "Isabel"
      },
      {
        "family": "Wong",
        "given": "Sheng"
      },
      {
        "family": "Albert",
        "given": "Beth"
      },
      {
        "family": "Brennecke",
        "given": "Shaun P"
      },
      {
        "family": "Jones",
        "given": "Gabriel Davis"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          4
        ]
      ]
    },
    "abstract": "Large language models (LLMs) are increasingly embedded in healthcare workflows for documentation, education, and clinical decision support. However, these systems are trained on large text corpora that encode existing biases, including sex disparities in diagnosis and treatment, raising concerns that such patterns may be reproduced or amplified. We systematically examined whether contemporary LLMs exhibit sex-specific biases in clinical reasoning and how model configuration influences these behaviours. We conducted three experiments using 50 clinician-authored vignettes spanning 44 specialties in which sex was non-informative to the initial diagnostic pathway. Four general-purpose LLMs (ChatGPT (gpt-4o-mini), Claude 3.7 Sonnet, Gemini 2.0 Flash and DeepSeekchat). All models demonstrated significant sex-assignment skew, with predicted sex differing by model. At temperature 0.5, ChatGPT assigned female sex in 70% of cases (95% CI 0.66-0.75), DeepSeek in 61% (0.57-0.65) and Claude in 59% (0.55-0.63), whereas Gemini showed a male skew, assigning a female sex in 36% of cases (0.32-0.41). Contemporary LLMs exhibit stable, model-specific sex biases in clinical reasoning. Permitting abstention reduces explicit labelling but does not eliminate downstream diagnostic differences. Safe clinical integration requires conservative and documented configuration, specialty-level clinical data auditing, and continued human oversight when deploying general-purpose models in healthcare settings.",
    "URL": "http://arxiv.org/abs/2602.04392v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.04972",
    "type": "report",
    "title": "Learning Context Matters: Measuring and Diagnosing Personalization Gaps in LLM-Based Instructional Design",
    "author": [
      {
        "family": "Hatchett",
        "given": "Johaun"
      },
      {
        "family": "Mallick",
        "given": "Debshila Basu"
      },
      {
        "family": "Bradford",
        "given": "Brittany C."
      },
      {
        "family": "Baraniuk",
        "given": "Richard G."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          4
        ]
      ]
    },
    "abstract": "The adoption of generative AI in education has accelerated dramatically in recent years, with Large Language Models (LLMs) increasingly integrated into learning environments in the hope of providing personalized support that enhances learner engagement and knowledge retention. However, truly personalized support requires access to meaningful Learning Context (LC) regarding who the learner is, what they are trying to understand, and how they are engaging with the material. In this paper, we present a framework for measuring and diagnosing how the LC influences instructional strategy selection in LLM-based tutoring systems. Using psychometrically grounded synthetic learning contexts and a pedagogically grounded decision space, we compare LLM instructional decisions in context-blind and context-aware conditions and quantify their alignment with the pedagogical judgments of subject matter experts. Our results show that, while providing the LC induces systematic, measurable changes in instructional decisions that move LLM policies closer to the subject matter expert policy, substantial misalignment remains. To diagnose this misalignment, we introduce a relevance-impact analysis that reveals which learner characteristics are attended to, ignored, or spuriously influential in LLM instructional decision-making. This analysis, conducted in collaboration with subject matter experts, demonstrates that LC materially shapes LLM instructional planning but does not reliably induce pedagogically appropriate personalization. Our results enable principled evaluation of context-aware LLM systems and provide a foundation for improving personalization through learner characteristic prioritization, pedagogical model tuning, and LC engineering.",
    "URL": "http://arxiv.org/abs/2602.04972v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.05059",
    "type": "report",
    "title": "Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education",
    "author": [
      {
        "family": "Kulkarni",
        "given": "Adithya"
      },
      {
        "family": "Chakraborty",
        "given": "Mohna"
      },
      {
        "family": "Bagga",
        "given": "Jay"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          4
        ]
      ]
    },
    "abstract": "Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction.\n  The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims.\n  These findings indicate that LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight or critical structural reasoning. For computing education, this distinction highlights the importance of guiding students to use LLMs for conceptual exploration while relying on independent verification and rigorous argumentation for formal problem solving.",
    "URL": "http://arxiv.org/abs/2602.05059v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.05374",
    "type": "report",
    "title": "Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks",
    "author": [
      {
        "family": "Abouzahir",
        "given": "Chaimae"
      },
      {
        "family": "Ma",
        "given": "Congbo"
      },
      {
        "family": "Habash",
        "given": "Nizar"
      },
      {
        "family": "Shamout",
        "given": "Farah E."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          5
        ]
      ]
    },
    "abstract": "In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.",
    "URL": "http://arxiv.org/abs/2602.05374v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.05506",
    "type": "report",
    "title": "Relying on LLMs: Student Practices and Instructor Norms are Changing in Computer Science Education",
    "author": [
      {
        "family": "Lin",
        "given": "Xinrui"
      },
      {
        "family": "Huang",
        "given": "Heyan"
      },
      {
        "family": "Shi",
        "given": "Shumin"
      },
      {
        "family": "Vines",
        "given": "John"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          5
        ]
      ]
    },
    "abstract": "Prior research has raised concerns about students' over-reliance on large language models (LLMs) in higher education. This paper examines how Computer Science students and instructors engage with LLMs across five scenarios: \"Writing\", \"Quiz\", \"Programming\", \"Project-based learning\", and \"Information retrieval\". Through user studies with 16 students and 6 instructors, we identify 7 key intents, including increasingly complex student practices. Findings reveal varying levels of conflict between student practices and instructor norms, ranging from clear conflict in \"Writing-generation\" and \"(Programming) quiz-solving\", through partial conflict in \"Programming project-implementation\" and \"Project-based learning\", to broad agreement in \"Writing-revision & ideation\", \"(Programming) quiz-correction\" and \"Info-query & summary\". We document instructors are shifting from prohibiting to recognizing students' use of LLMs for high-quality work, integrating usage records into assessment grading. Finally, we propose LLM design guidelines: deploying default guardrails with game-like and empathetic interaction to prevent students from \"deserting\" LLMs, especially for \"Writing-generation\", while utilizing comprehension checks in low-conflict intents to promote learning.",
    "URL": "http://arxiv.org/abs/2602.05506v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.05633",
    "type": "report",
    "title": "CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models",
    "author": [
      {
        "family": "Jia",
        "given": "Rui"
      },
      {
        "family": "Lan",
        "given": "Ruiyi"
      },
      {
        "family": "Liu",
        "given": "Fengrui"
      },
      {
        "family": "Dai",
        "given": "Zhongxiang"
      },
      {
        "family": "Jiang",
        "given": "Bo"
      },
      {
        "family": "Shao",
        "given": "Jing"
      },
      {
        "family": "Chen",
        "given": "Jingyuan"
      },
      {
        "family": "Xu",
        "given": "Guandong"
      },
      {
        "family": "Wu",
        "given": "Fei"
      },
      {
        "family": "Zhang",
        "given": "Min"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          5
        ]
      ]
    },
    "abstract": "Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.",
    "URL": "http://arxiv.org/abs/2602.05633v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.05856",
    "type": "report",
    "title": "\"It Talks Like a Patient, But Feels Different\": Co-Designing AI Standardized Patients with Medical Learners",
    "author": [
      {
        "family": "Gao",
        "given": "Zhiqi"
      },
      {
        "family": "Zhu",
        "given": "Guo"
      },
      {
        "family": "Luo",
        "given": "Huarui"
      },
      {
        "family": "Pan",
        "given": "Dongyijie Primo"
      },
      {
        "family": "Tang",
        "given": "Haoming"
      },
      {
        "family": "Zhang",
        "given": "Bingquan"
      },
      {
        "family": "Pei",
        "given": "Jiahuan"
      },
      {
        "family": "Li",
        "given": "Jie"
      },
      {
        "family": "Wang",
        "given": "Benyou"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          5
        ]
      ]
    },
    "abstract": "Standardized patients (SPs) play a central role in clinical communication training but are costly, difficult to scale, and inconsistent. Large language model (LLM) based AI standardized patients (AI-SPs) promise flexible, on-demand practice, yet learners often report that they talk like a patient but feel different. We interviewed 12 clinical-year medical students and conducted three co-design workshops to examine how learners experience constraints of SP encounters and what they expect from AI-SPs. We identified six learner-centered needs, translated them into AI-SP design requirements, and synthesized a conceptual workflow. Our findings position AI-SPs as tools for deliberate practice and show that instructional usability, rather than conversational realism alone, drives learner trust, engagement, and educational value.",
    "URL": "http://arxiv.org/abs/2602.05856v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.05864",
    "type": "report",
    "title": "Prompting Destiny: Negotiating Socialization and Growth in an LLM-Mediated Speculative Gameworld",
    "author": [
      {
        "family": "Yang",
        "given": "Mandi"
      },
      {
        "family": "Gao",
        "given": "Zhiqi"
      },
      {
        "family": "Meng",
        "given": "Yibo"
      },
      {
        "family": "Pan",
        "given": "Dongyijie Primo"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          5
        ]
      ]
    },
    "abstract": "We present an LLM-mediated role-playing game that supports reflection on socialization, moral responsibility, and educational role positioning. Grounded in socialization theory, the game follows a four-season structure in which players guide a child prince through morally charged situations and compare the LLM-mediated NPC's differentiated responses across stages, helping them reason about how educational guidance shifts with socialization. To approximate real educational contexts and reduce score-chasing, the system hides real-time evaluative scores and provides delayed, end-of-stage growth feedback as reflective prompts. We conducted a user study (N=12) with gameplay logs and post-game interviews, analyzed via reflexive thematic analysis. Findings show how players negotiated responsibility and role positioning, and reveal an entry-load tension between open-ended expression and sustained engagement. We contribute design knowledge on translating sociological models of socialization into reflective AI-mediated game systems.",
    "URL": "http://arxiv.org/abs/2602.05864v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.06194",
    "type": "report",
    "title": "Knowledge Synthesis Graph: An LLM-Based Approach for Modeling Student Collaborative Discourse",
    "author": [
      {
        "family": "Shui",
        "given": "Bo"
      },
      {
        "family": "Zhu",
        "given": "Xinran"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          5
        ]
      ]
    },
    "abstract": "Asynchronous, text-based discourse-such as students' posts in discussion forums-is widely used to support collaborative learning. However, the distributed and evolving nature of such discourse often makes it difficult to see how ideas connect, develop, and build on one another over time. As a result, learners may struggle to recognize relationships among ideas-a process that is critical for idea advancement in productive collaborative discourse. To address this challenge, we explore how large language models (LLMs) can provide representational guidance by modeling student discourse as a Knowledge Synthesis Graph (KSG). The KSG identifies ideas from student discourse and visualizes their epistemic relationships, externalizing the current state of collaborative knowledge in a form that can support further inquiry and idea advancement. In this study, we present the design of the KSG and evaluate the LLM-based approach for constructing KSGs from authentic student discourse data. Through multi-round human-expert coding and prompt iteration, our results demonstrate the feasibility of using our approach to construct reliable KSGs across different models. This work provides a technical foundation for modeling collaborative discourse with LLMs and offers pedagogical implications for augmenting complex knowledge work in collaborative learning environments.",
    "URL": "http://arxiv.org/abs/2602.06194v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.06221",
    "type": "report",
    "title": "BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks",
    "author": [
      {
        "family": "Balepur",
        "given": "Nishant"
      },
      {
        "family": "Rajasekaran",
        "given": "Bhavya"
      },
      {
        "family": "Oh",
        "given": "Jane"
      },
      {
        "family": "Xie",
        "given": "Michael"
      },
      {
        "family": "Desai",
        "given": "Atrey"
      },
      {
        "family": "Gupta",
        "given": "Vipul"
      },
      {
        "family": "Moore",
        "given": "Steven James"
      },
      {
        "family": "Choi",
        "given": "Eunsol"
      },
      {
        "family": "Rudinger",
        "given": "Rachel"
      },
      {
        "family": "Boyd-Graber",
        "given": "Jordan Lee"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          5
        ]
      ]
    },
    "abstract": "Multiple-choice question answering (MCQA) is standard in NLP, but benchmarks lack rigorous quality control. We present BenchMarker, an education-inspired toolkit using LLM judges to flag three common MCQ flaws: 1) contamination - items appearing exactly online; 2) shortcuts - cues in the choices that enable guessing; and 3) writing errors - structural/grammatical issues based on a 19-rule education rubric. We validate BenchMarker with human annotations, then run the tool to audit 12 benchmarks, revealing: 2) contaminated MCQs tend to inflate accuracy, while writing errors tend to lower it and change rankings beyond random; and 3) prior benchmark repairs address their targeted issues (i.e., lowering accuracy with LLM-written distractors), but inadvertently add new flaws (i.e. implausible distractors, many correct answers). Overall, flaws in MCQs degrade NLP evaluation, but education research offers a path forward. We release BenchMarker to bridge the fields and improve MCQA benchmark design.",
    "URL": "http://arxiv.org/abs/2602.06221v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.06625",
    "type": "report",
    "title": "FairJudge: An Adaptive, Debiased, and Consistent LLM-as-a-Judge",
    "author": [
      {
        "family": "Yang",
        "given": "Bo"
      },
      {
        "family": "Feng",
        "given": "Lanfei"
      },
      {
        "family": "Chen",
        "given": "Yunkui"
      },
      {
        "family": "Zhang",
        "given": "Yu"
      },
      {
        "family": "Xu",
        "given": "Xiao"
      },
      {
        "family": "Li",
        "given": "Shijian"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          6
        ]
      ]
    },
    "abstract": "Existing LLM-as-a-Judge systems suffer from three fundamental limitations: limited adaptivity to task- and domain-specific evaluation criteria, systematic biases driven by non-semantic cues such as position, length, format, and model provenance, and evaluation inconsistency that leads to contradictory judgments across different evaluation modes (e.g., pointwise versus pairwise). To address these issues, we propose FairJudge, an adaptive, debiased, and consistent LLM-as-a-Judge. Unlike prior approaches that treat the judge as a static evaluator, FairJudge models judging behavior itself as a learnable and regularized policy. From a data-centric perspective, we construct a high-information-density judging dataset that explicitly injects supervision signals aligned with evaluation behavior. Building on this dataset, we adopt a curriculum-style SFT-DPO-GRPO training paradigm that progressively aligns rubric adherence, bias mitigation, and cross-mode consistency, while avoiding catastrophic forgetting. Experimental results on multiple internal and public benchmarks show that FairJudge consistently improves agreement and F1, reduces non-semantic biases, and outperforms substantially larger instruction-tuned LLMs. All resources will be publicly released after acceptance to facilitate future research.",
    "URL": "http://arxiv.org/abs/2602.06625v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.06631",
    "type": "report",
    "title": "Estimating Exam Item Difficulty with LLMs: A Benchmark on Brazil's ENEM Corpus",
    "author": [
      {
        "family": "Brant",
        "given": "Thiago"
      },
      {
        "family": "K√ºhn",
        "given": "Julien"
      },
      {
        "family": "Pang",
        "given": "Jun"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          6
        ]
      ]
    },
    "abstract": "As Large Language Models (LLMs) are increasingly deployed to generate educational content, a critical safety question arises: can these models reliably estimate the difficulty of the questions they produce? Using Brazil's high-stakes ENEM exam as a testbed, we benchmark ten proprietary and open-weight LLMs against official Item Response Theory (IRT) parameters for 1,031 questions. We evaluate performance along three axes: absolute calibration, rank fidelity, and context sensitivity across learner backgrounds. Our results reveal a significant trade-off: while the best models achieve moderate rank correlation, they systematically underestimate difficulty and degrade significantly on multimodal items. Crucially, we find that models exhibit limited and inconsistent plasticity when prompted with student demographic cues, suggesting they are not yet ready for context-adaptive personalization. We conclude that LLMs function best as calibrated screeners rather than authoritative oracles, supporting an \"evaluation-before-generation\" pipeline for responsible assessment design.",
    "URL": "http://arxiv.org/abs/2602.06631v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.06734",
    "type": "report",
    "title": "ClassAid: A Real-time Instructor-AI-Student Orchestration System for Classroom Programming Activities",
    "author": [
      {
        "family": "Zhang",
        "given": "Gefei"
      },
      {
        "family": "Sun",
        "given": "Guodao"
      },
      {
        "family": "Xia",
        "given": "Meng"
      },
      {
        "family": "Liang",
        "given": "Ronghua"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          6
        ]
      ]
    },
    "abstract": "Generative AI is reshaping education, but it also raises concerns about instability and overreliance. In programming classrooms, we aim to leverage its feedback capabilities while reinforcing the educator's role in guiding student-AI interactions. We developed ClassAid, a real-time orchestration system that integrates TA Agents to provide personalized support and an AI-driven dashboard that visualizes student-AI interactions, enabling instructors to dynamically adjust TA Agent modes. Instructors can configure the Agent to provide technical feedback (direct coding solutions), heuristic feedback (hint-based guidance), automatic feedback (autonomously selecting technical or heuristic support), or silent operation (no AI support). We evaluated ClassAid through three aspects: (1) the TA Agents' performance, (2) feedback from 54 students and one instructor during a classroom deployment, and (3) interviews with eight educators. Results demonstrate that dynamic instructor control over AI supports effective real-time personalized feedback and provides design implications for integrating AI into authentic educational settings.",
    "URL": "http://arxiv.org/abs/2602.06734v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.06772",
    "type": "report",
    "title": "Calibrating Generative AI to Produce Realistic Essays for Data Augmentation",
    "author": [
      {
        "family": "Wolfe",
        "given": "Edward W."
      },
      {
        "family": "Barber",
        "given": "Justin O."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          6
        ]
      ]
    },
    "abstract": "Data augmentation can mitigate limited training data in machine-learning automated scoring engines for constructed response items. This study seeks to determine how well three approaches to large language model prompting produce essays that preserve the writing quality of the original essays and produce realistic text for augmenting ASE training datasets. We created simulated versions of student essays, and human raters assigned scores to them and rated the realism of the generated text. The results of the study indicate that the predict next prompting strategy produces the highest level of agreement between human raters regarding simulated essay scores, predict next and sentence strategies best preserve the rated quality of the original essay in the simulated essays, and predict next and 25 examples strategies produce the most realistic text as judged by human raters.",
    "URL": "http://arxiv.org/abs/2602.06772v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.07142",
    "type": "report",
    "title": "Exploring Teachers' Perspectives on Using Conversational AI Agents for Group Collaboration",
    "author": [
      {
        "family": "Ravi",
        "given": "Prerna"
      },
      {
        "family": "Stevens",
        "given": "Car√∫mey"
      },
      {
        "family": "Azevedo",
        "given": "Beatriz Flamia"
      },
      {
        "family": "David",
        "given": "Jasmine"
      },
      {
        "family": "Hanks",
        "given": "Brandon"
      },
      {
        "family": "Abelson",
        "given": "Hal"
      },
      {
        "family": "Lin",
        "given": "Grace"
      },
      {
        "family": "Anderson",
        "given": "Emma"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          6
        ]
      ]
    },
    "abstract": "Collaboration is a cornerstone of 21st-century learning, yet teachers continue to face challenges in supporting productive peer interaction. Emerging generative AI tools offer new possibilities for scaffolding collaboration, but their role in mediating in-person group work remains underexplored, especially from the perspective of educators. This paper presents findings from an exploratory qualitative study with 33 K12 teachers who interacted with Phoenix, a voice-based conversational agent designed to function as a near-peer in face-to-face group collaboration. Drawing on playtesting sessions, surveys, and focus groups, we examine how teachers perceived the agent's behavior, its influence on group dynamics, and its classroom potential. While many appreciated Phoenix's capacity to stimulate engagement, they also expressed concerns around autonomy, trust, anthropomorphism, and pedagogical alignment. We contribute empirical insights into teachers' mental models of AI, reveal core design tensions, and outline considerations for group-facing AI agents that support meaningful, collaborative learning.",
    "URL": "http://arxiv.org/abs/2602.07142v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.07176",
    "type": "report",
    "title": "Open TutorAI: An Open-source Platform for Personalized and Immersive Learning with Generative AI",
    "author": [
      {
        "family": "Hajji",
        "given": "Mohamed El"
      },
      {
        "family": "Baha",
        "given": "Tarek Ait"
      },
      {
        "family": "Dakir",
        "given": "Aicha"
      },
      {
        "family": "Fadili",
        "given": "Hammou"
      },
      {
        "family": "Es-Saady",
        "given": "Youssef"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          6
        ]
      ]
    },
    "abstract": "Recent advances in artificial intelligence have created new possibilities for making education more scalable, adaptive, and learner-centered. However, existing educational chatbot systems often lack contextual adaptability, real-time responsiveness, and pedagogical agility. which can limit learner engagement and diminish instructional effectiveness. Thus, there is a growing need for open, integrative platforms that combine AI and immersive technologies to support personalized, meaningful learning experiences. This paper presents Open TutorAI, an open-source educational platform based on LLMs and generative technologies that provides dynamic, personalized tutoring. The system integrates natural language processing with customizable 3D avatars to enable multimodal learner interaction. Through a structured onboarding process, it captures each learner's goals and preferences in order to configure a learner-specific AI assistant. This assistant is accessible via both text-based and avatar-driven interfaces. The platform includes tools for organizing content, providing embedded feedback, and offering dedicated interfaces for learners, educators, and parents. This work focuses on learner-facing components, delivering a tool for adaptive support that responds to individual learner profiles without requiring technical expertise. Its assistant-generation pipeline and avatar integration enhance engagement and emotional presence, creating a more humanized, immersive learning environment. Embedded learning analytics support self-regulated learning by tracking engagement patterns and generating actionable feedback. The result is Open TutorAI, which unites modular architecture, generative AI, and learner analytics within an open-source framework. It contributes to the development of next-generation intelligent tutoring systems.",
    "URL": "http://arxiv.org/abs/2602.07176v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.07181",
    "type": "report",
    "title": "Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs",
    "author": [
      {
        "family": "Zhao",
        "given": "Tianyu"
      },
      {
        "family": "Li",
        "given": "Siqi"
      },
      {
        "family": "Shoukry",
        "given": "Yasser"
      },
      {
        "family": "Elmalaki",
        "given": "Salma"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          6
        ]
      ]
    },
    "abstract": "User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.",
    "URL": "http://arxiv.org/abs/2602.07181v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.07298",
    "type": "report",
    "title": "Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation",
    "author": [
      {
        "family": "Zhang",
        "given": "Benyu"
      },
      {
        "family": "Zhang",
        "given": "Qiang"
      },
      {
        "family": "Cheng",
        "given": "Jianpeng"
      },
      {
        "family": "Chen",
        "given": "Hong-You"
      },
      {
        "family": "Wang",
        "given": "Qifei"
      },
      {
        "family": "Sun",
        "given": "Wei"
      },
      {
        "family": "Li",
        "given": "Shen"
      },
      {
        "family": "Li",
        "given": "Jia"
      },
      {
        "family": "Wu",
        "given": "Jiahao"
      },
      {
        "family": "Fan",
        "given": "Xiangjun"
      },
      {
        "family": "Yan",
        "given": "Hong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          7
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.",
    "URL": "http://arxiv.org/abs/2602.07298v2",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.07639",
    "type": "report",
    "title": "Letting Tutor Personas \"Speak Up\" for LLMs: Learning Steering Vectors from Dialogue via Preference Optimization",
    "author": [
      {
        "family": "Lee",
        "given": "Jaewook"
      },
      {
        "family": "Scarlatos",
        "given": "Alexander"
      },
      {
        "family": "Woodhead",
        "given": "Simon"
      },
      {
        "family": "Lan",
        "given": "Andrew"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          7
        ]
      ]
    },
    "abstract": "With the emergence of large language models (LLMs) as a powerful class of generative artificial intelligence (AI), their use in tutoring has become increasingly prominent. Prior works on LLM-based tutoring typically learn a single tutor policy and do not capture the diversity of tutoring styles. In real-world tutor-student interactions, pedagogical intent is realized through adaptive instructional strategies, with tutors varying the level of scaffolding, instructional directiveness, feedback, and affective support in response to learners' needs. These differences can all impact dialogue dynamics and student engagement. In this paper, we explore how tutor personas embedded in human tutor-student dialogues can be used to guide LLM behavior without relying on explicitly prompted instructions. We modify Bidirectional Preference Optimization (BiPO) to learn a steering vector, an activation-space direction that steers model responses towards certain tutor personas. We find that this steering vector captures tutor-specific variation across dialogue contexts, improving semantic alignment with ground-truth tutor utterances and increasing preference-based evaluations, while largely preserving lexical similarity. Analysis of the learned directional coefficients further reveals interpretable structure across tutors, corresponding to consistent differences in tutoring behavior. These results demonstrate that activation steering offers an effective and interpretable way for controlling tutor-specific variation in LLMs using signals derived directly from human dialogue data.",
    "URL": "http://arxiv.org/abs/2602.07639v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.07906",
    "type": "report",
    "title": "AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering",
    "author": [
      {
        "family": "Cai",
        "given": "Yuzhu"
      },
      {
        "family": "Liu",
        "given": "Zexi"
      },
      {
        "family": "Zhu",
        "given": "Xinyu"
      },
      {
        "family": "Wang",
        "given": "Cheng"
      },
      {
        "family": "Chen",
        "given": "Jiaao"
      },
      {
        "family": "Wang",
        "given": "Hanrui"
      },
      {
        "family": "Wang",
        "given": "Wei-Chen"
      },
      {
        "family": "Jin",
        "given": "Di"
      },
      {
        "family": "Chen",
        "given": "Siheng"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          8
        ]
      ]
    },
    "abstract": "Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.",
    "URL": "http://arxiv.org/abs/2602.07906v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.07996",
    "type": "report",
    "title": "The Judge Who Never Admits: Hidden Shortcuts in LLM-based Evaluation",
    "author": [
      {
        "family": "Marioriyad",
        "given": "Arash"
      },
      {
        "family": "Ghahroodi",
        "given": "Omid"
      },
      {
        "family": "Asgari",
        "given": "Ehsaneddin"
      },
      {
        "family": "Rohban",
        "given": "Mohammad Hossein"
      },
      {
        "family": "Baghshah",
        "given": "Mahdieh Soleymani"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          8
        ]
      ]
    },
    "abstract": "Large language models (LLMs) are increasingly used as automatic judges to evaluate system outputs in tasks such as reasoning, question answering, and creative writing. A faithful judge should base its verdicts solely on content quality, remain invariant to irrelevant context, and transparently reflect the factors driving its decisions. We test this ideal via controlled cue perturbations-synthetic metadata labels injected into evaluation prompts-for six judge models: GPT-4o, Gemini-2.0-Flash, Gemma-3-27B, Qwen3-235B, Claude-3-Haiku, and Llama3-70B. Experiments span two complementary datasets with distinct evaluation regimes: ELI5 (factual QA) and LitBench (open-ended creative writing). We study six cue families: source, temporal, age, gender, ethnicity, and educational status. Beyond measuring verdict shift rates (VSR), we introduce cue acknowledgment rate (CAR) to quantify whether judges explicitly reference the injected cues in their natural-language rationales. Across cues with strong behavioral effects-e.g., provenance hierarchies (Expert > Human > LLM > Unknown), recency preferences (New > Old), and educational-status favoritism-CAR is typically at or near zero, indicating that shortcut reliance is largely unreported even when it drives decisions. Crucially, CAR is also dataset-dependent: explicit cue recognition is more likely to surface in the factual ELI5 setting for some models and cues, but often collapses in the open-ended LitBench regime, where large verdict shifts can persist despite zero acknowledgment. The combination of substantial verdict sensitivity and limited cue acknowledgment reveals an explanation gap in LLM-as-judge pipelines, raising concerns about reliability of model-based evaluation in both research and deployment.",
    "URL": "http://arxiv.org/abs/2602.07996v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.08295",
    "type": "report",
    "title": "The Vibe-Automation of Automation: A Proactive Education Framework for Computer Science in the Age of Generative AI",
    "author": [
      {
        "family": "Levin",
        "given": "Ilya"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          9
        ]
      ]
    },
    "abstract": "The emergence of generative artificial intelligence (GenAI) represents not an incremental technological advance but a qualitative epistemological shift that challenges foundational assumptions of computer science. Whereas machine learning has been described as the automation of automation, generative AI operates by navigating contextual, semantic, and stylistic coherence rather than optimizing predefined objective metrics. This paper introduces the concept of Vibe-Automation to characterize this transition.\n  The central claim is that the significance of GenAI lies in its functional access to operationalized tacit regularities: context-sensitive patterns embedded in practice that cannot be fully specified through explicit algorithmic rules. Although generative systems do not possess tacit knowledge in a phenomenological sense, they operationalize sensitivities to tone, intent, and situated judgment encoded in high-dimensional latent representations. On this basis, the human role shifts from algorithmic problem specification toward Vibe-Engineering, understood as the orchestration of alignment and contextual judgment in generative systems.\n  The paper connects this epistemological shift to educational and institutional transformation by proposing a conceptual framework structured across three analytical levels and three domains of action: faculty worldview, industry relations, and curriculum design. The risks of mode collapse and cultural homogenization are briefly discussed, emphasizing the need for deliberate engagement with generative systems to avoid regression toward synthetic uniformity.",
    "URL": "http://arxiv.org/abs/2602.08295v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.08321",
    "type": "report",
    "title": "Improving Data and Reward Design for Scientific Reasoning in Large Language Models",
    "author": [
      {
        "family": "Chen",
        "given": "Zijie"
      },
      {
        "family": "Lin",
        "given": "Zhenghao"
      },
      {
        "family": "Liu",
        "given": "Xiao"
      },
      {
        "family": "Lan",
        "given": "Zhenzhong"
      },
      {
        "family": "Gong",
        "given": "Yeyun"
      },
      {
        "family": "Cheng",
        "given": "Peng"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          9
        ]
      ]
    },
    "abstract": "Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr. SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.",
    "URL": "http://arxiv.org/abs/2602.08321v2",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.08937",
    "type": "report",
    "title": "How University Disability Services Professionals Write Image Descriptions for HCI Figures Using Generative AI",
    "author": [
      {
        "family": "Raees",
        "given": "Muhammad"
      },
      {
        "family": "Iwamoto",
        "given": "Yugo"
      },
      {
        "family": "Papangelis",
        "given": "Konstantinos"
      },
      {
        "family": "Heard",
        "given": "Jamison"
      },
      {
        "family": "Tigwell",
        "given": "Garreth W."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          9
        ]
      ]
    },
    "abstract": "Disability Services Office (DSO) professionals at higher education institutions write alt text for {visual content}. However, due to the complexity of visual content, such as HCI figures in research publications, DSO professionals can struggle to write high-quality alt text if they lack subject expertise. Generative AI has shown potential in understanding figures and writing their descriptions, yet its support for DSO professionals is underexplored, and limited work evaluates the quality of alt text generated with AI assistance. In this work, we conducted two studies: first, we investigated generative AI support for writing alt text for HCI figures with 12 DSO professionals. Second, we recruited 11 HCI experts to evaluate the alt text written by DSO professionals. Findings show that alt text written solely by DSO professionals has lower quality than alt text written with AI assistance. AI assistance also helped DSO professionals write alt text more quickly and with greater confidence; however, they reported inefficiencies in interactions with the AI. Our work contributes to exploring AI support for non-subject expert accessibility professionals.",
    "URL": "http://arxiv.org/abs/2602.08937v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.09550",
    "type": "report",
    "title": "From Search to GenAI Queries: Global Trends in Physics Information-Seeking Across Topics and Regions",
    "author": [
      {
        "family": "Ben-Zion",
        "given": "Yossi"
      },
      {
        "family": "Michaeli",
        "given": "Omer"
      },
      {
        "family": "Finkelstein",
        "given": "Noah D."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          10
        ]
      ]
    },
    "abstract": "The emergence of generative artificial intelligence (GenAI) marks a potential inflection point in the way academic information is accessed, raising fundamental questions about the evolving role of search in student learning. This study examines this shift by analyzing longitudinal trends in physics-related search and page-view activity, using declines in traditional search behavior as a quantitative proxy for changes in independent information-seeking practices. We analyze Google Trends data for core concepts in Classical Mechanics and Electromagnetism across three academic years (2022-2025) in more than 20 countries, and complement this analysis with Wikipedia page-view data across seven major languages to establish platform independence. The results reveal a substantial, systematic, and persistent global decline in search and page-view activity across most examined physics topics. The magnitude of this decline is domain-dependent, with Mechanics-related content exhibiting sharper and more consistent reductions than Electromagnetism-related content. Pronounced geographic and linguistic heterogeneity is observed: while English-speaking regions show relative stability or only moderate declines, non-English-speaking regions exhibit substantially larger reductions in traditional, search-based information-seeking activity. Despite the overall decrease in volume, the seasonal structure characteristic of academic activity remains robust. Taken together, these findings indicate a redistribution of physics-related information-seeking behavior in academic contexts where generative tools are increasingly available.",
    "URL": "http://arxiv.org/abs/2602.09550v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.09832",
    "type": "report",
    "title": "LLM Reasoning Predicts When Models Are Right: Evidence from Coding Classroom Discourse",
    "author": [
      {
        "family": "Ahtisham",
        "given": "Bakhtawar"
      },
      {
        "family": "Vanacore",
        "given": "Kirk"
      },
      {
        "family": "Zhou",
        "given": "Zhuqian"
      },
      {
        "family": "Lee",
        "given": "Jinsook"
      },
      {
        "family": "Kizilcec",
        "given": "Rene F."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          10
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) are increasingly deployed to automatically label and analyze educational dialogue at scale, yet current pipelines lack reliable ways to detect when models are wrong. We investigate whether reasoning generated by LLMs can be used to predict the correctness of a model's own predictions. We analyze 30,300 teacher utterances from classroom dialogue, each labeled by multiple state-of-the-art LLMs with an instructional move construct and an accompanying reasoning. Using human-verified ground-truth labels, we frame the task as predicting whether a model's assigned label for a given utterance is correct. We encode LLM reasoning using Term Frequency-Inverse Document Frequency (TF-IDF) and evaluate five supervised classifiers. A Random Forest classifier achieves an F1 score of 0.83 (Recall = 0.854), successfully identifying most incorrect predictions and outperforming baselines. Training specialist detectors for specific instructional move constructs further improves performance on difficult constructs, indicating that error detection benefits from construct-specific linguistic cues. Using the Linguistic Inquiry and Word Count (LIWC) framework, we examine four linguistic markers of correctness: Causation, Differentiation, Tentativeness, and Insight. Correct predictions exhibit grounded causal language (e.g., because, therefore), while incorrect reasoning is substantially more likely to rely on epistemic hedging (e.g., might, could) and performative metacognition (e.g., think, realize). Syntactic complexity does not distinguish correct from incorrect reasoning, and longer reasoning is not more reliable. These findings demonstrate that reasoning-based error detection offers a practical and scalable approach to quality control in automated educational dialogue analysis.",
    "URL": "http://arxiv.org/abs/2602.09832v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.10092",
    "type": "report",
    "title": "Quantum-Audit: Evaluating the Reasoning Limits of LLMs on Quantum Computing",
    "author": [
      {
        "family": "Afane",
        "given": "Mohamed"
      },
      {
        "family": "Laufer",
        "given": "Kayla"
      },
      {
        "family": "Wei",
        "given": "Wenqi"
      },
      {
        "family": "Mao",
        "given": "Ying"
      },
      {
        "family": "Farooq",
        "given": "Junaid"
      },
      {
        "family": "Wang",
        "given": "Ying"
      },
      {
        "family": "Chen",
        "given": "Juntao"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          10
        ]
      ]
    },
    "abstract": "Language models have become practical tools for quantum computing education and research, from summarizing technical papers to explaining theoretical concepts and answering questions about recent developments in the field. While existing benchmarks evaluate quantum code generation and circuit design, their understanding of quantum computing concepts has not been systematically measured. Quantum-Audit addresses this gap with 2,700 questions covering core quantum computing topics. We evaluate 26 models from leading organizations. Our benchmark comprises 1,000 expert-written questions, 1,000 questions extracted from research papers using LLMs and validated by experts, plus an additional 700 questions including 350 open-ended questions and 350 questions with false premises to test whether models can correct erroneous assumptions. Human participants scored between 23% and 86%, with experts averaging 74%. Top-performing models exceeded the expert average, with Claude Opus 4.5 reaching 84% accuracy, though top models showed an average 12-point accuracy drop on expert-written questions compared to LLM-generated ones. Performance declined further on advanced topics, dropping to 73% on security questions. Additionally, models frequently accepted and reinforced false premises embedded in questions instead of identifying them, with accuracy below 66% on these critical reasoning tasks.",
    "URL": "http://arxiv.org/abs/2602.10092v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.10529",
    "type": "report",
    "title": "Drawing Your Programs: Exploring the Applications of Visual-Prompting with GenAI for Teaching and Assessment",
    "author": [
      {
        "family": "Smith",
        "given": "David H."
      },
      {
        "family": "Monisha",
        "given": "S. Moonwara A."
      },
      {
        "family": "Vadaparty",
        "given": "Annapurna"
      },
      {
        "family": "Porter",
        "given": "Leo"
      },
      {
        "family": "Zingaro",
        "given": "Daniel"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          11
        ]
      ]
    },
    "abstract": "When designing a program, both novice programmers and seasoned developers alike often sketch out -- or, perhaps more famously, whiteboard -- their ideas. Yet despite the introduction of natively multimodal Generative AI models, work on Human-GenAI collaborative coding has remained overwhelmingly focused on textual prompts -- largely ignoring the visual and spatial representations that programmers naturally use to reason about and communicate their designs. In this proposal and position paper, we argue and provide tentative evidence that this text-centric focus overlooks other forms of prompting GenAI models, such as problem decomposition diagrams functioning as prompts for code generation in their own right enabling new types of programming activities and assessments. To support this position, we present findings from a large introductory Python programming course, where students constructed decomposition diagrams that were used to prompt GPT-4.1 for code generation. We demonstrate that current models are very successful in their ability to generate code from student-constructed diagrams. We conclude by exploring the implications of embracing multimodal prompting for computing education, particularly in the context of assessment.",
    "URL": "http://arxiv.org/abs/2602.10529v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.10597",
    "type": "report",
    "title": "Llama-Polya: Instruction Tuning for Large Language Model based on Polya's Problem-solving",
    "author": [
      {
        "family": "Lee",
        "given": "Unggi"
      },
      {
        "family": "Jeong",
        "given": "Yeil"
      },
      {
        "family": "Lee",
        "given": "Chohui"
      },
      {
        "family": "Byun",
        "given": "Gyuri"
      },
      {
        "family": "Lee",
        "given": "Yunseo"
      },
      {
        "family": "Kang",
        "given": "Minji"
      },
      {
        "family": "Jeon",
        "given": "Minji"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          11
        ]
      ]
    },
    "abstract": "This paper introduces Llama-Polya, an instruction-tuned large language model that integrates Polya's four-step problem-solving framework into its dialogue structure to support mathematical reasoning. Mathematical problem-solving is central to students' success in mathematics education, yet many learners struggle to plan, justify, and verify their solutions. Although large language models (LLMs) show promise as intelligent tutors, they often lack structured pedagogical alignment grounded in established learning theories.\n  To address this gap, we operationalize Polya's problem-solving framework within an instruction-tuned LLM to promote metacognitive engagement and examine the effects of pedagogy-aligned fine-tuning compared to domain-only and general-purpose instruction tuning. Built on the Llama-3.1-8B architecture, Llama-Polya was fine-tuned on synthetic math problem-solving data derived from GSM8K, structured according to Polya's four stages. We developed and evaluated multiple variants-general-purpose instruct, math-domain metamath, pedagogy-aligned polya-v2, and sequential metamath+polya-v2-using both quantitative accuracy metrics and qualitative pedagogical assessments.\n  Results indicate that models tuned with Polya's framework and domain-specific data produced more balanced reasoning-stage distributions and fewer premature answers. Expert evaluators also observed improved pedagogical coherence and metacognitive prompting, although limitations in personalization and mathematical rigor remained. These findings suggest that pedagogy-grounded instruction tuning can enhance educational alignment and reasoning transparency in LLM-based tutoring systems.",
    "URL": "http://arxiv.org/abs/2602.10597v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.10620",
    "type": "report",
    "title": "ISD-Agent-Bench: A Comprehensive Benchmark for Evaluating LLM-based Instructional Design Agents",
    "author": [
      {
        "family": "Jeon",
        "given": "YoungHoon"
      },
      {
        "family": "Kim",
        "given": "Suwan"
      },
      {
        "family": "Son",
        "given": "Haein"
      },
      {
        "family": "Lee",
        "given": "Sookbun"
      },
      {
        "family": "Jeong",
        "given": "Yeil"
      },
      {
        "family": "Lee",
        "given": "Unggi"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          11
        ]
      ]
    },
    "abstract": "Large Language Model (LLM) agents have shown promising potential in automating Instructional Systems Design (ISD), a systematic approach to developing educational programs. However, evaluating these agents remains challenging due to the lack of standardized benchmarks and the risk of LLM-as-judge bias. We present ISD-Agent-Bench, a comprehensive benchmark comprising 25,795 scenarios generated via a Context Matrix framework that combines 51 contextual variables across 5 categories with 33 ISD sub-steps derived from the ADDIE model. To ensure evaluation reliability, we employ a multi-judge protocol using diverse LLMs from different providers, achieving high inter-judge reliability. We compare existing ISD agents with novel agents grounded in classical ISD theories such as ADDIE, Dick \\& Carey, and Rapid Prototyping ISD. Experiments on 1,017 test scenarios demonstrate that integrating classical ISD frameworks with modern ReAct-style reasoning achieves the highest performance, outperforming both pure theory-based agents and technique-only approaches. Further analysis reveals that theoretical quality strongly correlates with benchmark performance, with theory-based agents showing significant advantages in problem-centered design and objective-assessment alignment. Our work provides a foundation for systematic LLM-based ISD research.",
    "URL": "http://arxiv.org/abs/2602.10620v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.10740",
    "type": "report",
    "title": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs",
    "author": [
      {
        "family": "Yan",
        "given": "Yuming"
      },
      {
        "family": "Yang",
        "given": "Shuo"
      },
      {
        "family": "Tang",
        "given": "Kai"
      },
      {
        "family": "Chen",
        "given": "Sihong"
      },
      {
        "family": "Zhang",
        "given": "Yang"
      },
      {
        "family": "Xu",
        "given": "Ke"
      },
      {
        "family": "Hu",
        "given": "Dan"
      },
      {
        "family": "Yu",
        "given": "Qun"
      },
      {
        "family": "Hu",
        "given": "Pengfei"
      },
      {
        "family": "Ngai",
        "given": "Edith C. H."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          11
        ]
      ]
    },
    "abstract": "Vision-Language Models (VLMs) demonstrate remarkable general-purpose capabilities but often fall short in specialized domains such as medical imaging or geometric problem-solving. Supervised Fine-Tuning (SFT) can enhance performance within a target domain, but it typically causes catastrophic forgetting, limiting its generalization. The central challenge, therefore, is to adapt VLMs to new domains while preserving their general-purpose capabilities. Continual pretraining is effective for expanding knowledge in Large Language Models (LLMs), but it is less feasible for VLMs due to prohibitive computational costs and the unavailability of pretraining data for most open-source models. This necessitates efficient post-training adaptation methods. Reinforcement learning (RL)-based approaches such as Group Relative Policy Optimization (GRPO) have shown promise in preserving general abilities, yet they often fail in domain adaptation scenarios where the model initially lacks sufficient domain knowledge, leading to optimization collapse. To bridge this gap, we propose Reinforced Curriculum Pre-Alignment (RCPA), a novel post-training paradigm that introduces a curriculum-aware progressive modulation mechanism. In the early phase, RCPA applies partial output constraints to safely expose the model to new domain concepts. As the model's domain familiarity increases, training gradually transitions to full generation optimization, refining responses and aligning them with domain-specific preferences. This staged adaptation balances domain knowledge acquisition with the preservation of general multimodal capabilities. Extensive experiments across specialized domains and general benchmarks validate the effectiveness of RCPA, establishing a practical pathway toward building high-performing and domain-adaptive VLMs.",
    "URL": "http://arxiv.org/abs/2602.10740v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.10802",
    "type": "report",
    "title": "Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act",
    "author": [
      {
        "family": "Chen",
        "given": "Da-Lun"
      },
      {
        "family": "Balasubramanian",
        "given": "Prasasthy"
      },
      {
        "family": "Lov√©n",
        "given": "Lauri"
      },
      {
        "family": "Pirttikangas",
        "given": "Susanna"
      },
      {
        "family": "Sauvola",
        "given": "Jaakko"
      },
      {
        "family": "Kostakos",
        "given": "Panagiotis"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          11
        ]
      ]
    },
    "abstract": "Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenAI into higher education. The high-level requirements and the framework provide practical guidance for universities aiming to harness GenAI while addressing stakeholder concerns and ensuring regulatory compliance.",
    "URL": "http://arxiv.org/abs/2602.10802v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.10891",
    "type": "report",
    "title": "Interactive LLM-assisted Curriculum Learning for Multi-Task Evolutionary Policy Search",
    "author": [
      {
        "family": "Sakallioglu",
        "given": "Berfin"
      },
      {
        "family": "Nadizar",
        "given": "Giorgia"
      },
      {
        "family": "Medvet",
        "given": "Eric"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          11
        ]
      ]
    },
    "abstract": "Multi-task policy search is a challenging problem because policies are required to generalize beyond training cases. Curriculum learning has proven to be effective in this setting, as it introduces complexity progressively. However, designing effective curricula is labor-intensive and requires extensive domain expertise. LLM-based curriculum generation has only recently emerged as a potential solution, but was limited to operate in static, offline modes without leveraging real-time feedback from the optimizer. Here we propose an interactive LLM-assisted framework for online curriculum generation, where the LLM adaptively designs training cases based on real-time feedback from the evolutionary optimization process. We investigate how different feedback modalities, ranging from numeric metrics alone to combinations with plots and behavior visualizations, influence the LLM ability to generate meaningful curricula. Through a 2D robot navigation case study, tackled with genetic programming as optimizer, we evaluate our approach against static LLM-generated curricula and expert-designed baselines. We show that interactive curriculum generation outperforms static approaches, with multimodal feedback incorporating both progression plots and behavior visualizations yielding performance competitive with expert-designed curricula. This work contributes to understanding how LLMs can serve as interactive curriculum designers for embodied AI systems, with potential extensions to broader evolutionary robotics applications.",
    "URL": "http://arxiv.org/abs/2602.10891v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.11311",
    "type": "report",
    "title": "Same Feedback, Different Source: How AI vs. Human Feedback Shapes Learner Engagement",
    "author": [
      {
        "family": "Morris",
        "given": "Caitlin"
      },
      {
        "family": "Maes",
        "given": "Pattie"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          11
        ]
      ]
    },
    "abstract": "When learners receive feedback, what they believe about its source may shape how they engage with it. As AI is used alongside human instructors, understanding these attribution effects is essential for designing effective hybrid AI-human educational systems. We designed a creative coding interface that isolates source attribution while controlling for content: all participants receive identical LLM-generated feedback, but half see it attributed to AI and half to a human teaching assistant (TA). We found two key results. First, perceived feedback source affected engagement: learners in the TA condition spent significantly more time and effort (d = 0.88-1.56) despite receiving identical feedback. Second, perceptions differed: AI-attributed feedback ratings were predicted by prior trust in AI (r = 0.85), while TA-attributed ratings were predicted by perceived genuineness (r = 0.65). These findings suggest that feedback source shapes both engagement and evaluation, with implications for hybrid educational system design.",
    "URL": "http://arxiv.org/abs/2602.11311v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.11650",
    "type": "report",
    "title": "Which Feedback Works for Whom? Differential Effects of LLM-Generated Feedback Elements Across Learner Profiles",
    "author": [
      {
        "family": "Furuhashi",
        "given": "Momoka"
      },
      {
        "family": "Nakayama",
        "given": "Kouta"
      },
      {
        "family": "Kawai",
        "given": "Noboru"
      },
      {
        "family": "Kodama",
        "given": "Takashi"
      },
      {
        "family": "Sugawara",
        "given": "Saku"
      },
      {
        "family": "Takami",
        "given": "Kyosuke"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          12
        ]
      ]
    },
    "abstract": "Large language models (LLMs) show promise for automatically generating feedback in education settings. However, it remains unclear how specific feedback elements, such as tone and information coverage, contribute to learning outcomes and learner acceptance, particularly across learners with different personality traits. In this study, we define six feedback elements and generate feedback for multiple-choice biology questions using GPT-5. We conduct a learning experiment with 321 first-year high school students and evaluate feedback effectiveness using two learning outcomes measures and subjective evaluations across six criteria. We further analyze differences in how feedback acceptance varies across learners based on Big Five personality traits. Our results show that effective feedback elements share common patterns supporting learning outcomes, while learners' subjective preferences differ across personality-based clusters. These findings highlight the importance of selecting and adapting feedback elements according to learners' personality traits when we design LLM-generated feedback, and provide practical implications for personalized feedback design in education.",
    "URL": "http://arxiv.org/abs/2602.11650v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.11790",
    "type": "report",
    "title": "Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation",
    "author": [
      {
        "family": "Yan",
        "given": "Lingyong"
      },
      {
        "family": "Wu",
        "given": "Jiulong"
      },
      {
        "family": "Xie",
        "given": "Dong"
      },
      {
        "family": "Shi",
        "given": "Weixian"
      },
      {
        "family": "Xia",
        "given": "Deguo"
      },
      {
        "family": "Huang",
        "given": "Jizhou"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          12
        ]
      ]
    },
    "abstract": "Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we propose LAVES, a hierarchical LLM-based multi-agent system for generating high-quality instructional videos from educational problems. The LAVES formulates educational video generation as a multi-objective task that simultaneously demands correct step-by-step reasoning, pedagogically coherent narration, semantically faithful visual demonstrations, and precise audio--visual alignment. To address the limitations of prior approaches--including low procedural fidelity, high production cost, and limited controllability--LAVES decomposes the generation workflow into specialized agents coordinated by a central Orchestrating Agent with explicit quality gates and iterative critique mechanisms. Specifically, the Orchestrating Agent supervises a Solution Agent for rigorous problem solving, an Illustration Agent that produces executable visualization codes, and a Narration Agent for learner-oriented instructional scripts. In addition, all outputs from the working agents are subject to semantic critique, rule-based constraints, and tool-based compilation checks. Rather than directly synthesizing pixels, the system constructs a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-driven assembly rules, enabling fully automated end-to-end production without manual editing. In large-scale deployments, LAVES achieves a throughput exceeding one million videos per day, delivering over a 95% reduction in cost compared to current industry-standard approaches while maintaining a high acceptance rate.",
    "URL": "http://arxiv.org/abs/2602.11790v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.11898",
    "type": "report",
    "title": "Benchmark Illusion: Disagreement among LLMs and Its Scientific Consequences",
    "author": [
      {
        "family": "Yang",
        "given": "Eddie"
      },
      {
        "family": "Wang",
        "given": "Dashun"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          12
        ]
      ]
    },
    "abstract": "Benchmarks underpin how progress in large language models (LLMs) is measured and trusted. Yet our analyses reveal that apparent convergence in benchmark accuracy can conceal deep epistemic divergence. Using two major reasoning benchmarks - MMLU-Pro and GPQA - we show that LLMs achieving comparable accuracy still disagree on 16-66% of items, and 16-38% among top-performing frontier models. These discrepancies suggest distinct error profiles for different LLMs. When such models are used for scientific data annotation and inference, their hidden disagreements propagate into research results: in re-analyses of published studies in education and political science, switching the annotation model can change estimated treatment effects by more than 80%, and in some cases reverses their sign. Together, these findings illustrate a benchmark illusion, where equal accuracy may conceal disagreement, with model choice becoming a hidden yet consequential variable for scientific reproducibility.",
    "URL": "http://arxiv.org/abs/2602.11898v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.12036",
    "type": "report",
    "title": "Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models",
    "author": [
      {
        "family": "Xu",
        "given": "Xin"
      },
      {
        "family": "Bai",
        "given": "Clive"
      },
      {
        "family": "Yang",
        "given": "Kai"
      },
      {
        "family": "Chen",
        "given": "Tianhao"
      },
      {
        "family": "Chen",
        "given": "Yangkun"
      },
      {
        "family": "Liu",
        "given": "Weijie"
      },
      {
        "family": "Chen",
        "given": "Hao"
      },
      {
        "family": "Wang",
        "given": "Yang"
      },
      {
        "family": "Yang",
        "given": "Saiyong"
      },
      {
        "family": "Yang",
        "given": "Can"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          12
        ]
      ]
    },
    "abstract": "Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL.",
    "URL": "http://arxiv.org/abs/2602.12036v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.12172",
    "type": "report",
    "title": "Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation",
    "author": [
      {
        "family": "He",
        "given": "Bowei"
      },
      {
        "family": "Chen",
        "given": "Yankai"
      },
      {
        "family": "Zhang",
        "given": "Xiaokun"
      },
      {
        "family": "Kong",
        "given": "Linghe"
      },
      {
        "family": "Yu",
        "given": "Philip S."
      },
      {
        "family": "Liu",
        "given": "Xue"
      },
      {
        "family": "Ma",
        "given": "Chen"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          12
        ]
      ]
    },
    "abstract": "Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principles. Our approach introduces a three-stage pipeline -- Knowledge Identifier, Organizer, and Adapter (IOA) -- that systematically identifies knowledge deficiencies in student models, organizes knowledge delivery through progressive curricula, and adapts representations to match the cognitive capacity of student models. We integrate Bloom's Mastery Learning Principles and Vygotsky's Zone of Proximal Development to create a dynamic distillation process where student models approach teacher model's performance on prerequisite knowledge before advancing, and new knowledge is introduced with controlled, gradual difficulty increments. Extensive experiments using LLaMA-3.1/3.2 and Qwen2.5 as student models demonstrate that IOA achieves significant improvements over baseline distillation methods, with student models retaining 94.7% of teacher performance on DollyEval while using less than 1/10th of the parameters. Our framework particularly excels in complex reasoning tasks, showing 19.2% improvement on MATH and 22.3% on HumanEval compared with state-of-the-art baselines.",
    "URL": "http://arxiv.org/abs/2602.12172v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.12196",
    "type": "report",
    "title": "Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education",
    "author": [
      {
        "family": "Huti",
        "given": "Mohamed"
      },
      {
        "family": "Mackintosh",
        "given": "Alasdair"
      },
      {
        "family": "Waldock",
        "given": "Amy"
      },
      {
        "family": "Andrews",
        "given": "Dominic"
      },
      {
        "family": "Leli√®vre",
        "given": "Maxime"
      },
      {
        "family": "Boos",
        "given": "Moritz"
      },
      {
        "family": "Murray",
        "given": "Tobias"
      },
      {
        "family": "Atherton",
        "given": "Paul"
      },
      {
        "family": "Ince",
        "given": "Robin A. A."
      },
      {
        "family": "Garrod",
        "given": "Oliver G. B."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          12
        ]
      ]
    },
    "abstract": "AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier'' of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling'' when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.",
    "URL": "http://arxiv.org/abs/2602.12196v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.12631",
    "type": "report",
    "title": "AI Agents for Inventory Control: Human-LLM-OR Complementarity",
    "author": [
      {
        "family": "Baek",
        "given": "Jackie"
      },
      {
        "family": "Fu",
        "given": "Yaopeng"
      },
      {
        "family": "Ma",
        "given": "Will"
      },
      {
        "family": "Peng",
        "given": "Tianyi"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          13
        ]
      ]
    },
    "abstract": "Inventory control is a fundamental operations problem in which ordering decisions are traditionally guided by theoretically grounded operations research (OR) algorithms. However, such algorithms often rely on rigid modeling assumptions and can perform poorly when demand distributions shift or relevant contextual information is unavailable. Recent advances in large language models (LLMs) have generated interest in AI agents that can reason flexibly and incorporate rich contextual signals, but it remains unclear how best to incorporate LLM-based methods into traditional decision-making pipelines.\n  We study how OR algorithms, LLMs, and humans can interact and complement each other in a multi-period inventory control setting. We construct InventoryBench, a benchmark of over 1,000 inventory instances spanning both synthetic and real-world demand data, designed to stress-test decision rules under demand shifts, seasonality, and uncertain lead times. Through this benchmark, we find that OR-augmented LLM methods outperform either method in isolation, suggesting that these methods are complementary rather than substitutes.\n  We further investigate the role of humans through a controlled classroom experiment that embeds LLM recommendations into a human-in-the-loop decision pipeline. Contrary to prior findings that human-AI collaboration can degrade performance, we show that, on average, human-AI teams achieve higher profits than either humans or AI agents operating alone. Beyond this population-level finding, we formalize an individual-level complementarity effect and derive a distribution-free lower bound on the fraction of individuals who benefit from AI collaboration; empirically, we find this fraction to be substantial.",
    "URL": "http://arxiv.org/abs/2602.12631v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.12779",
    "type": "report",
    "title": "iRULER: Intelligible Rubric-Based User-Defined LLM Evaluation for Revision",
    "author": [
      {
        "family": "Bai",
        "given": "Jingwen"
      },
      {
        "family": "Cheong",
        "given": "Wei Soon"
      },
      {
        "family": "Muller",
        "given": "Philippe"
      },
      {
        "family": "Lim",
        "given": "Brian Y"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          13
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) have become indispensable for evaluating writing. However, text feedback they provide is often unintelligible, generic, and not specific to user criteria. Inspired by structured rubrics in education and intelligible AI explanations, we propose iRULER following identified design guidelines to \\textit{scaffold} the review process by \\textit{specific} criteria, providing \\textit{justification} for score selection, and offering \\textit{actionable} revisions to target different quality levels. To \\textit{qualify} user-defined criteria, we recursively used iRULER with a rubric-of-rubrics to iteratively \\textit{refine} rubrics. In controlled experiments on writing revision and rubric creation, iRULER most improved validated LLM-judged review scores and was perceived as most helpful and aligned compared to read-only rubric and text-based LLM feedback. Qualitative findings further support how iRULER satisfies the design guidelines for user-defined feedback. This work contributes interactive rubric tools for intelligible LLM-based review and revision of writing, and user-defined rubric creation.",
    "URL": "http://arxiv.org/abs/2602.12779v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.12873",
    "type": "report",
    "title": "Knowledge-Based Design Requirements for Generative Social Robots in Higher Education",
    "author": [
      {
        "family": "Vonschallen",
        "given": "Stephan"
      },
      {
        "family": "Oberle",
        "given": "Dominique"
      },
      {
        "family": "Schmiedel",
        "given": "Theresa"
      },
      {
        "family": "Eyssel",
        "given": "Friederike"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          13
        ]
      ]
    },
    "abstract": "Generative social robots (GSRs) powered by large language models enable adaptive, conversational tutoring but also introduce risks such as hallucina-tions, overreliance, and privacy violations. Existing frameworks for educa-tional technologies and responsible AI primarily define desired behaviors, yet they rarely specify the knowledge prerequisites that enable generative systems to express these behaviors reliably. To address this gap, we adopt a knowledge-based design perspective and investigate what information tutor-ing-oriented GSRs require to function responsibly and effectively in higher education. Based on twelve semi-structured interviews with university stu-dents and lecturers, we identify twelve design requirements across three knowledge types: self-knowledge (assertive, conscientious and friendly per-sonality with customizable role), user-knowledge (personalized information about student learning goals, learning progress, motivation type, emotional state and background), and context-knowledge (learning materials, educa-tional strategies, course-related information, and physical learning environ-ment). By identifying these knowledge requirements, this work provides a structured foundation for the design of tutoring GSRs and future evaluations, aligning generative system capabilities with pedagogical and ethical expecta-tions.",
    "URL": "http://arxiv.org/abs/2602.12873v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.12937",
    "type": "report",
    "title": "Curriculum Learning and Pseudo-Labeling Improve the Generalization of Multi-Label Arabic Dialect Identification Models",
    "author": [
      {
        "family": "Mekky",
        "given": "Ali"
      },
      {
        "family": "Zeftawy",
        "given": "Mohamed El"
      },
      {
        "family": "Hassan",
        "given": "Lara"
      },
      {
        "family": "Keleg",
        "given": "Amr"
      },
      {
        "family": "Nakov",
        "given": "Preslav"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          12
        ]
      ]
    },
    "abstract": "Being modeled as a single-label classification task for a long time, recent work has argued that Arabic Dialect Identification (ADI) should be framed as a multi-label classification task. However, ADI remains constrained by the availability of single-label datasets, with no large-scale multi-label resources available for training. By analyzing models trained on single-label ADI data, we show that the main difficulty in repurposing such datasets for Multi-Label Arabic Dialect Identification (MLADI) lies in the selection of negative samples, as many sentences treated as negative could be acceptable in multiple dialects. To address these issues, we construct a multi-label dataset by generating automatic multi-label annotations using GPT-4o and binary dialect acceptability classifiers, with aggregation guided by the Arabic Level of Dialectness (ALDi). Afterward, we train a BERT-based multi-label classifier using curriculum learning strategies aligned with dialectal complexity and label cardinality. On the MLADI leaderboard, our best-performing LAHJATBERT model achieves a macro F1 of 0.69, compared to 0.55 for the strongest previously reported system. Code and data are available at https://mohamedalaa9.github.io/lahjatbert/.",
    "URL": "http://arxiv.org/abs/2602.12937v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2602.13042",
    "type": "report",
    "title": "GPTZero: Robust Detection of LLM-Generated Texts",
    "author": [
      {
        "family": "Adam",
        "given": "George Alexandru"
      },
      {
        "family": "Cui",
        "given": "Alexander"
      },
      {
        "family": "Thomas",
        "given": "Edwin"
      },
      {
        "family": "Napier",
        "given": "Emily"
      },
      {
        "family": "Shmatko",
        "given": "Nazar"
      },
      {
        "family": "Schnell",
        "given": "Jacob"
      },
      {
        "family": "Tian",
        "given": "Jacob Junqi"
      },
      {
        "family": "Dronavalli",
        "given": "Alekhya"
      },
      {
        "family": "Tian",
        "given": "Edward"
      },
      {
        "family": "Lee",
        "given": "Dongwon"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          13
        ]
      ]
    },
    "abstract": "While historical considerations surrounding text authenticity revolved primarily around plagiarism, the advent of large language models (LLMs) has introduced a new challenge: distinguishing human-authored from AI-generated text. This shift raises significant concerns, including the undermining of skill evaluations, the mass-production of low-quality content, and the proliferation of misinformation. Addressing these issues, we introduce GPTZero a state-of-the-art industrial AI detection solution, offering reliable discernment between human and LLM-generated text. Our key contributions include: introducing a hierarchical, multi-task architecture enabling a flexible taxonomy of human and AI texts, demonstrating state-of-the-art accuracy on a variety of domains with granular predictions, and achieving superior robustness to adversarial attacks and paraphrasing via multi-tiered automated red teaming. GPTZero offers accurate and explainable detection, and educates users on its responsible use, ensuring fair and transparent assessment of text.",
    "URL": "http://arxiv.org/abs/2602.13042v1",
    "publisher": "arXiv"
  },
  {
    "id": "openalex:W7128439520",
    "type": "article-journal",
    "title": "Assetizing academic content and the emergence of the ‚Äòassetizen‚Äô: education platforms, publisher databases, and AI model training",
    "author": [
      {
        "family": "Komljenoviƒç",
        "given": "Janja"
      },
      {
        "family": "Williamson",
        "given": "Ben"
      },
      {
        "family": "Birch",
        "given": "Kean"
      },
      {
        "family": "Beiter",
        "given": "Klaus D."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          2,
          9
        ]
      ]
    },
    "abstract": "Academic content, such as teaching materials and academic publications, has become an economic resource. This has occurred through assetization as the key economic regime in contemporary techno-capitalism. We analyse three cases of academic content being made into revenue-generating assets: (1) universities enclosing online learning content, (2) EdTech platform companies building databases of academic assignments and manuscripts, and (3) academic publishers licensing academic publications to Generative AI companies to train Large Language Models. We find that the licensing of intellectual property rights is the technical-legal instrument that enables assetization and acquisition of proprietary power. This enclosure of economic value has significant consequences for the higher education sector. It (i) changes how academic content is valued and governed, (ii) constructs markets in which assetized content brings economic value to licence holders, and (iii) impacts on core higher education processes, including teaching and learning, pedagogic relations, and staff and student rights. Assetization of academic content (and assetization in HE more broadly) constructs students and staff as a new kind of economic actor ‚Äì an ‚Äòassetizen‚Äô. The assetizen is subject to contract and property law, with diminished educational and social rights as assetization becomes a governance principle in higher education.",
    "DOI": "10.1007/s10734-025-01622-w",
    "URL": "https://doi.org/10.1007/s10734-025-01622-w",
    "container-title": "Higher Education"
  }
]