[
  {
    "id": "arxiv:2512.03501",
    "type": "report",
    "title": "SocraticAI: Transforming LLMs into Guided CS Tutors Through Scaffolded Interaction",
    "author": [
      {
        "family": "Sunil",
        "given": "Karthik"
      },
      {
        "family": "Thakkar",
        "given": "Aalok"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          3
        ]
      ]
    },
    "abstract": "We present SocraticAI, a scaffolded AI tutoring system that integrates large language models (LLMs) into undergraduate Computer Science education through structured constraints rather than prohibition. The system enforces well-formulated questions, reflective engagement, and daily usage limits while providing Socratic dialogue scaffolds. Unlike traditional AI bans, our approach cultivates responsible and strategic AI interaction skills through technical guardrails, including authentication, query validation, structured feedback, and RAG-based course grounding. Initial deployment demonstrates that students progress from vague help-seeking to sophisticated problem decomposition within 2-3 weeks, with over 75% producing substantive reflections and displaying emergent patterns of deliberate, strategic AI use.",
    "URL": "http://arxiv.org/abs/2512.03501v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.03671",
    "type": "report",
    "title": "Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context",
    "author": [
      {
        "family": "Savoldi",
        "given": "Beatrice"
      },
      {
        "family": "Attanasio",
        "given": "Giuseppe"
      },
      {
        "family": "Gorodetskaya",
        "given": "Olga"
      },
      {
        "family": "Manerba",
        "given": "Marta Marchiori"
      },
      {
        "family": "Bassignana",
        "given": "Elisa"
      },
      {
        "family": "Casola",
        "given": "Silvia"
      },
      {
        "family": "Negri",
        "given": "Matteo"
      },
      {
        "family": "Caselli",
        "given": "Tommaso"
      },
      {
        "family": "Bentivogli",
        "given": "Luisa"
      },
      {
        "family": "Ramponi",
        "given": "Alan"
      },
      {
        "family": "Muti",
        "given": "Arianna"
      },
      {
        "family": "Balbo",
        "given": "Nicoletta"
      },
      {
        "family": "Nozza",
        "given": "Debora"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          3
        ]
      ]
    },
    "abstract": "The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.",
    "URL": "http://arxiv.org/abs/2512.03671v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.03694",
    "type": "report",
    "title": "SRPG: Semantically Reconstructed Privacy Guard for Zero-Trust Privacy in Educational Multi-Agent Systems",
    "author": [
      {
        "family": "Guo",
        "given": "Shuang"
      },
      {
        "family": "Li",
        "given": "Zihui"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          3
        ]
      ]
    },
    "abstract": "Multi-Agent Systems (MAS) with large language models (LLMs) enable personalized education but risk leaking minors personally identifiable information (PII) via unstructured dialogue. Existing privacy methods struggle to balance security and utility: role-based access control fails on unstructured text, while naive masking destroys pedagogical context. We propose SRPG, a privacy guard for educational MAS, using a Dual-Stream Reconstruction Mechanism: a strict sanitization stream ensures zero PII leakage, and a context reconstruction stream (LLM driven) recovers mathematical logic. This decouples instructional content from private data, preserving teaching efficacy. Tests on MathDial show SRPG works across models; with GPT-4o, it achieves 0.0000 Attack Success Rate (ASR) (zero leakage) and 0.8267 Exact Match, far outperforming the zero trust Pure LLM baseline (0.2138). SRPG effectively protects minors privacy without sacrificing mathematical instructional quality.",
    "URL": "http://arxiv.org/abs/2512.03694v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.04256",
    "type": "report",
    "title": "On the Role and Impact of GenAI Tools in Software Engineering Education",
    "author": [
      {
        "family": "Qin",
        "given": "Qiaolin"
      },
      {
        "family": "Santos",
        "given": "Ronnie de Souza"
      },
      {
        "family": "Spinola",
        "given": "Rodrigo"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          3
        ]
      ]
    },
    "abstract": "Context. The rise of generative AI (GenAI) tools like ChatGPT and GitHub Copilot has transformed how software is learned and written. In software engineering (SE) education, these tools offer new opportunities for support, but also raise concerns about over-reliance, ethical use, and impacts on learning. Objective. This study investigates how undergraduate SE students use GenAI tools, focusing on the benefits, challenges, ethical concerns, and instructional expectations that shape their experiences. Method. We conducted a survey with 130 undergraduate students from two universities. The survey combined structured Likert-scale items and open-ended questions to investigate five dimensions: usage context, perceived benefits, challenges, ethical and instructional perceptions. Results. Students most often use GenAI for incremental learning and advanced implementation, reporting benefits such as brainstorming support and confidence-building. At the same time, they face challenges including unclear rationales and difficulty adapting outputs. Students highlight ethical concerns around fairness and misconduct, and call for clearer instructional guidance. Conclusion. GenAI is reshaping SE education in nuanced ways. Our findings underscore the need for scaffolding, ethical policies, and adaptive instructional strategies to ensure that GenAI supports equitable and effective learning.",
    "URL": "http://arxiv.org/abs/2512.04256v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.04359",
    "type": "report",
    "title": "Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning",
    "author": [
      {
        "family": "Cao",
        "given": "Hongye"
      },
      {
        "family": "Bai",
        "given": "Zhixin"
      },
      {
        "family": "Peng",
        "given": "Ziyue"
      },
      {
        "family": "Wang",
        "given": "Boyan"
      },
      {
        "family": "Yang",
        "given": "Tianpei"
      },
      {
        "family": "Huo",
        "given": "Jing"
      },
      {
        "family": "Zhang",
        "given": "Yuyao"
      },
      {
        "family": "Gao",
        "given": "Yang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.",
    "URL": "http://arxiv.org/abs/2512.04359v2",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.04555",
    "type": "report",
    "title": "ADAPT: Learning Task Mixtures for Budget-Constrained Instruction Tuning",
    "author": [
      {
        "family": "Kadasi",
        "given": "Pritam"
      },
      {
        "family": "Upperwal",
        "given": "Abhishek"
      },
      {
        "family": "SIngh",
        "given": "Mayank"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "We propose ADAPT, a meta-learning algorithm that \\emph{learns} task sampling proportions under an explicit token budget for multi-task instruction tuning. Instead of fixing task weights by hand, \\adapt{} maintains a continuous distribution over tasks and updates it via meta-gradients of a smooth worst-case validation objective, inducing an adaptive curriculum that allocates more tokens to useful tasks while avoiding collapse. We instantiate ADAPT on three $\\sim$1B-parameter open-weight LLMs (Gemma-3-1B, LLaMA-3.2-1B, Qwen-0.6B), training on 20 Natural Instructions task types under budgets of $1\\%$, $5\\%$, and $10\\%$ of the available supervised tokens, and compare against strong supervised fine-tuning baselines with uniform and size-proportional mixing. We conduct evaluations on 11 out-of-domain benchmarks spanning reasoning, reading comprehension, code generation, and instruction following, we find that ADAPT matches or slightly improves average downstream performance relative to the best static mixture, while using fewer effective training tokens and reallocating budget toward harder, benchmark-aligned tasks.",
    "URL": "http://arxiv.org/abs/2512.04555v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.04629",
    "type": "report",
    "title": "BioMedGPT-Mol: Multi-task Learning for Molecular Understanding and Generation",
    "author": [
      {
        "family": "Zuo",
        "given": "Chenyang"
      },
      {
        "family": "Fan",
        "given": "Siqi"
      },
      {
        "family": "Nie",
        "given": "Zaiqing"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "Molecules play a crucial role in biomedical research and discovery, particularly in the field of small molecule drug development. Given the rapid advancements in large language models, especially the recent emergence of reasoning models, it is natural to explore how a general-purpose language model can be efficiently adapted for molecular science applications. In this work, we introduce BioMedGPT-Mol, a molecular language model designed to support molecular understanding and generation tasks. By curating and unifying existing public instruction datasets, we have assembled a large-scale, comprehensive, and high-quality training dataset. The model is then fine-tuned through a meticulously designed multi-task learning framework. On a consolidated benchmark derived from LlaSMol, TOMG-Bench, and MuMOInstruct, BioMedGPT-Mol achieves remarkable performance. Our experimental results demonstrate that a general-purpose reasoning model can be effectively and efficiently post-trained into a professional molecular language model through a well-structured multi-task curriculum. Leveraging these capabilities, we further apply the model to multi-step retrosynthetic planning, achieving state-of-the-art performance on RetroBench and demonstrating its superior efficacy as an end-to-end retrosynthetic planner. We anticipate that our approach can be extended to other biomedical scientific domains.",
    "URL": "http://arxiv.org/abs/2512.04629v2",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.04630",
    "type": "report",
    "title": "Reflection-Satisfaction Tradeoff: Investigating Impact of Reflection on Student Engagement with AI-Generated Programming Hints",
    "author": [
      {
        "family": "Choi",
        "given": "Heeryung"
      },
      {
        "family": "Phung",
        "given": "Tung"
      },
      {
        "family": "Wu",
        "given": "Mengyan"
      },
      {
        "family": "Singla",
        "given": "Adish"
      },
      {
        "family": "Brooks",
        "given": "Christopher"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "Generative AI tools, such as AI-generated hints, are increasingly integrated into programming education to offer timely, personalized support. However, little is known about how to effectively leverage these hints while ensuring autonomous and meaningful learning. One promising approach involves pairing AI-generated hints with reflection prompts, asking students to review and analyze their learning, when they request hints. This study investigates the interplay between AI-generated hints and different designs of reflection prompts in an online introductory programming course. We conducted a two-trial field experiment. In Trial 1, students were randomly assigned to receive prompts either before or after receiving hints, or no prompt at all. Each prompt also targeted one of three SRL phases: planning, monitoring, and evaluation. In Trial 2, we examined two types of prompt guidance: directed (offering more explicit and structured guidance) and open (offering more general and less constrained guidance). Findings show that students in the before-hint (RQ1), planning (RQ2), and directed (RQ3) prompt groups produced higher-quality reflections but reported lower satisfaction with AI-generated hints than those in other conditions. Immediate performance did not differ across conditions. This negative relationship between reflection quality and hint satisfaction aligns with previous work on student mental effort and satisfaction. Our results highlight the need to reconsider how AI models are trained and evaluated for education, as prioritizing user satisfaction can undermine deeper learning.",
    "URL": "http://arxiv.org/abs/2512.04630v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.04834",
    "type": "report",
    "title": "Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case",
    "author": [
      {
        "family": "Kembu",
        "given": "Vignesh Kumar"
      },
      {
        "family": "Morandini",
        "given": "Pierandrea"
      },
      {
        "family": "Ranzini",
        "given": "Marta Bianca Maria"
      },
      {
        "family": "Nocera",
        "given": "Antonino"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.",
    "URL": "http://arxiv.org/abs/2512.04834v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.04869",
    "type": "report",
    "title": "Developing a General Personal Tutor for Education",
    "author": [
      {
        "family": "Aru",
        "given": "Jaan"
      },
      {
        "family": "Laak",
        "given": "Kristjan-Julius"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "The vision of a universal AI tutor has remained elusive, despite decades of effort. Could LLMs be the game-changer? We overview novel issues arising from developing a nationwide AI tutor. We highlight the practical questions that point to specific gaps in our scientific understanding of the learning process.",
    "DOI": "10.1016/j.tics.2025.09.010",
    "URL": "https://doi.org/10.1016/j.tics.2025.09.010",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.05024",
    "type": "report",
    "title": "Model-Free Assessment of Simulator Fidelity via Quantile Curves",
    "author": [
      {
        "family": "Iyengar",
        "given": "Garud"
      },
      {
        "family": "Lin",
        "given": "Yu-Shiou Willy"
      },
      {
        "family": "Wang",
        "given": "Kaizheng"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "Simulation of complex systems originated in manufacturing and queuing applications. It is now widely used for large-scale, ML-based systems in research, education, and consumer surveys. However, characterizing the discrepancy between simulators and ground truth remains challenging for increasingly complex, machine-learning-based systems. We propose a computationally tractable method to estimate the quantile function of the discrepancy between the simulated and ground-truth outcome distributions. Our approach focuses on output uncertainty and treats the simulator as a black box, imposing no modeling assumptions on its internals, and hence applies broadly across many parameter families, from Bernoulli and multinomial models to continuous, vector-valued settings. The resulting quantile curve supports confidence interval construction for unseen scenarios, risk-aware summaries of sim-to-real discrepancy (e.g., VaR/CVaR), and comparison of simulators' performance. We demonstrate our methodology in an application assessing LLM simulation fidelity on the WorldValueBench dataset spanning four LLMs.",
    "URL": "http://arxiv.org/abs/2512.05024v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.05167",
    "type": "report",
    "title": "Bridging Traditional Machine Learning and Large Language Models: A Two-Part Course Design for Modern AI Education",
    "author": [
      {
        "family": "Li",
        "given": "Fang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "This paper presents an innovative pedagogical approach for teaching artificial intelligence and data science that systematically bridges traditional machine learning techniques with modern Large Language Models (LLMs). We describe a course structured in two sequential and complementary parts: foundational machine learning concepts and contemporary LLM applications. This design enables students to develop a comprehensive understanding of AI evolution while building practical skills with both established and cutting-edge technologies. We detail the course architecture, implementation strategies, assessment methods, and learning outcomes from our summer course delivery spanning two seven-week terms. Our findings demonstrate that this integrated approach enhances student comprehension of the AI landscape and better prepares them for industry demands in the rapidly evolving field of artificial intelligence.",
    "URL": "http://arxiv.org/abs/2512.05167v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.05506",
    "type": "report",
    "title": "When Scaffolding Breaks: Investigating Student Interaction with LLM-Based Writing Support in Real-Time K-12 EFL Classrooms",
    "author": [
      {
        "family": "Myung",
        "given": "Junho"
      },
      {
        "family": "Lim",
        "given": "Hyunseung"
      },
      {
        "family": "Oh",
        "given": "Hana"
      },
      {
        "family": "Jin",
        "given": "Hyoungwook"
      },
      {
        "family": "Kang",
        "given": "Nayeon"
      },
      {
        "family": "Ahn",
        "given": "So-Yeon"
      },
      {
        "family": "Hong",
        "given": "Hwajung"
      },
      {
        "family": "Oh",
        "given": "Alice"
      },
      {
        "family": "Kim",
        "given": "Juho"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          5
        ]
      ]
    },
    "abstract": "Large language models (LLMs) are promising tools for scaffolding students' English writing skills, but their effectiveness in real-time K-12 classrooms remains underexplored. Addressing this gap, our study examines the benefits and limitations of using LLMs as real-time learning support, considering how classroom constraints, such as diverse proficiency levels and limited time, affect their effectiveness. We conducted a deployment study with 157 eighth-grade students in a South Korean middle school English class over six weeks. Our findings reveal that while scaffolding improved students' ability to compose grammatically correct sentences, this step-by-step approach demotivated lower-proficiency students and increased their system reliance. We also observed challenges to classroom dynamics, where extroverted students often dominated the teacher's attention, and the system's assistance made it difficult for teachers to identify struggling students. Based on these findings, we discuss design guidelines for integrating LLMs into real-time writing classes as inclusive educational tools.",
    "URL": "http://arxiv.org/abs/2512.05506v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.05671",
    "type": "report",
    "title": "MedTutor-R1: Socratic Personalized Medical Teaching with Multi-Agent Simulation",
    "author": [
      {
        "family": "He",
        "given": "Zhitao"
      },
      {
        "family": "Yang",
        "given": "Haolin"
      },
      {
        "family": "Qin",
        "given": "Zeyu"
      },
      {
        "family": "Fung",
        "given": "Yi R"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          5
        ]
      ]
    },
    "abstract": "The significant gap between rising demands for clinical training and the scarcity of expert instruction poses a major challenge to medical education. With powerful capabilities in personalized guidance, Large Language Models (LLMs) offer a promising solution to bridge this gap. However, current research focuses mainly on one-on-one knowledge instruction, overlooking collaborative reasoning, a key skill for students developed in teamwork like ward rounds. To this end, we develop ClinEdu, a multi-agent pedagogical simulator with personality-driven patients and diverse student cohorts, enabling controlled testing of complex pedagogical processes and scalable generation of teaching data. Based on ClinEdu, we construct ClinTeach, a large Socratic teaching dialogue dataset that captures the complexities of group instruction. We then train MedTutor-R1, the first multimodal Socratic tutor designed for one-to-many instruction in clinical medical education. MedTutor-R1 is first instruction-tuned on our ClinTeach dataset and then optimized with reinforcement learning, using rewards derived from a three-axis rubric, covering structural fidelity, analytical quality, and clinical safety, to refine its adaptive Socratic strategies. For authentic in-situ assessment, we use simulation-based interactive evaluation that redeploys the tutor back into ClinEdu. Experimental results demonstrate that our MedTutor-R1 outperforms the base model by over 20% in average pedagogical score and is comparable to o3, while also exhibiting high adaptability in handling a varying number of students. This promising performance underscores the effectiveness of our pedagogical simulator, ClinEdu.",
    "URL": "http://arxiv.org/abs/2512.05671v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.05967",
    "type": "report",
    "title": "Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms",
    "author": [
      {
        "family": "Granata",
        "given": "Francesco"
      },
      {
        "family": "Poggi",
        "given": "Francesco"
      },
      {
        "family": "Mongiov√¨",
        "given": "Misael"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          5
        ]
      ]
    },
    "abstract": "In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.",
    "URL": "http://arxiv.org/abs/2512.05967v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.06018",
    "type": "report",
    "title": "Uncovering Students' Inquiry Patterns in GenAI-Supported Clinical Practice: An Integration of Epistemic Network Analysis and Sequential Pattern Mining",
    "author": [
      {
        "family": "Wei",
        "given": "Jiameng"
      },
      {
        "family": "Dang",
        "given": "Dinh"
      },
      {
        "family": "Yang",
        "given": "Kaixun"
      },
      {
        "family": "Stokes",
        "given": "Emily"
      },
      {
        "family": "Mazeh",
        "given": "Amna"
      },
      {
        "family": "Lim",
        "given": "Angelina"
      },
      {
        "family": "Dai",
        "given": "David Wei"
      },
      {
        "family": "Moore",
        "given": "Joel"
      },
      {
        "family": "Fan",
        "given": "Yizhou"
      },
      {
        "family": "Gasevic",
        "given": "Danijela"
      },
      {
        "family": "Gasevic",
        "given": "Dragan"
      },
      {
        "family": "Chen",
        "given": "Guanliang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          4
        ]
      ]
    },
    "abstract": "Assessment of medication history-taking has traditionally relied on human observation, limiting scalability and detailed performance data. While Generative AI (GenAI) platforms enable extensive data collection and learning analytics provide powerful methods for analyzing educational traces, these approaches remain largely underexplored in pharmacy clinical training. This study addresses this gap by applying learning analytics to understand how students develop clinical communication competencies with GenAI-powered virtual patients -- a crucial endeavor given the diversity of student cohorts, varying language backgrounds, and the limited opportunities for individualized feedback in traditional training settings. We analyzed 323 students' interaction logs across Australian and Malaysian institutions, comprising 50,871 coded utterances from 1,487 student-GenAI dialogues. Combining Epistemic Network Analysis to model inquiry co-occurrences with Sequential Pattern Mining to capture temporal sequences, we found that high performers demonstrated strategic deployment of information recognition behaviors. Specifically, high performers centered inquiry on recognizing clinically relevant information, integrating rapport-building and structural organization, while low performers remained in routine question-verification loops. Demographic factors including first-language background, prior pharmacy work experience, and institutional context, also shaped distinct inquiry patterns. These findings reveal inquiry patterns that may indicate clinical reasoning development in GenAI-assisted contexts, providing methodological insights for health professions education assessment and informing adaptive GenAI system design that supports diverse learning pathways.",
    "URL": "http://arxiv.org/abs/2512.06018v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.06483",
    "type": "report",
    "title": "Classifying German Language Proficiency Levels Using Large Language Models",
    "author": [
      {
        "family": "Ahlers",
        "given": "Elias-Leander"
      },
      {
        "family": "Brunsmann",
        "given": "Witold"
      },
      {
        "family": "Schilling",
        "given": "Malte"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          6
        ]
      ]
    },
    "abstract": "Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.",
    "URL": "http://arxiv.org/abs/2512.06483v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.06617",
    "type": "report",
    "title": "Teaching large language models to see in radar: aspect-distributed prototypes for few-shot HRRP ATR",
    "author": [
      {
        "family": "Bi",
        "given": "De"
      },
      {
        "family": "Xu",
        "given": "Chengbai"
      },
      {
        "family": "Chen",
        "given": "Lingfeng"
      },
      {
        "family": "Hu",
        "given": "Panhe"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          7
        ]
      ]
    },
    "abstract": "High-resolution range profiles (HRRPs) play a critical role in automatic target recognition (ATR) due to their richinformationregarding target scattering centers (SCs), which encapsulate the geometric and electromagnetic characteristics of thetarget.Under few-shot circumstances, traditional learning-based methods often suffer from overfitting and struggle togeneralizeeffectively. The recently proposed HRRPLLM, which leverages the in-context learning (ICL) capabilities of largelanguagemodels (LLMs) for one-shot HRRP ATR, is limited in few-shot scenarios. This limitation arises because it primarilyutilizesthe distribution of SCs for recognition while neglecting the variance of the samples caused by aspect sensitivity. Thispaperproposes a straightforward yet effective Aspect-Distributed Prototype (ADP) strategy for LLM-based ATRunder few-shotconditions to enhance aspect robustness. Experiments conducted on both simulated and measured aircraft electromagneticdatasets demonstrate that the proposed method significantly outperforms current benchmarks.",
    "URL": "http://arxiv.org/abs/2512.06617v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.06999",
    "type": "report",
    "title": "Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model",
    "author": [
      {
        "family": "Wang",
        "given": "Zihao"
      },
      {
        "family": "Yuan",
        "given": "Ruibin"
      },
      {
        "family": "Geng",
        "given": "Ziqi"
      },
      {
        "family": "Li",
        "given": "Hengjia"
      },
      {
        "family": "Qu",
        "given": "Xingwei"
      },
      {
        "family": "Li",
        "given": "Xinyi"
      },
      {
        "family": "Chen",
        "given": "Songye"
      },
      {
        "family": "Fu",
        "given": "Haoying"
      },
      {
        "family": "Dannenberg",
        "given": "Roger B."
      },
      {
        "family": "Zhang",
        "given": "Kejun"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          7
        ]
      ]
    },
    "abstract": "Automated singing assessment is crucial for education and entertainment. However, existing systems face two fundamental limitations: reliance on reference tracks, which stifles creative expression, and the simplification of complex performances into non-diagnostic scores based solely on pitch and rhythm. We advocate for a shift from discriminative to descriptive evaluation, creating a complete ecosystem for reference-free, multi-dimensional assessment. First, we introduce Sing-MD, a large-scale dataset annotated by experts across four dimensions: breath control, timbre quality, emotional expression, and vocal technique. Our analysis reveals significant annotation inconsistencies among experts, challenging the validity of traditional accuracy-based metrics. Second, addressing the memory limitations of Multimodal Large Language Models (MLLMs) in analyzing full-length songs, we propose VocalVerse. This efficient hybrid architecture leverages a lightweight acoustic encoder to model global performance features and long-term dependencies. Third, to address automated metric shortcomings, we establish the H-TPR (Human-in-the-loop Tiered Perceptual Ranking) benchmark, which evaluates a model's ability to generate perceptually valid rankings rather than predicting noisy ground-truth scores.",
    "DOI": "10.1145/3746027.3758148",
    "URL": "https://doi.org/10.1145/3746027.3758148",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.07143",
    "type": "report",
    "title": "A Theoretical Framework of Student Agency in AI- Assisted Learning: A Grounded Theory Approach",
    "author": [
      {
        "family": "Dai",
        "given": "Yun"
      },
      {
        "family": "Lai",
        "given": "Sichen"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          8
        ]
      ]
    },
    "abstract": "Generative AI(GenAI) is a kind of AI model capable of producing human-like content in various modalities, including text, image, audio, video, and computer programming. Although GenAI offers great potential for education, its value often depends on students' ability to engage with it actively, responsibly, and critically - qualities central to student agency. Nevertheless, student agency has long been a complex and ambiguous concept in educational discourses, with few empirical studies clarifying its distinct nature and process in AI-assisted learning environments. To address this gap, the qualitative study presented in this article examines how higher education students exercise agency in AI-assisted learning and proposes a theoretical framework using a grounded theory approach. Guided by agentic engagement theory, this article analyzes the authentic experiences of 26 students using data from their GenAI conversation records and cognitive interviews that capture their thought processes and decision-making. The findings identify four key aspects of student agency: initiating and (re)directing, mindful adoption, external help-seeking, and reflective learning. Together, these aspects form an empirically developed framework that characterizes student agency in AI-assisted learning as a proactive, intentional, adaptive, reflective, and iterative process. Based on the empirical findings, theoretical and practical implications are discussed for researchers, educators, and policymakers.",
    "DOI": "10.1093/9780198945253.003.0009",
    "URL": "https://doi.org/10.1093/9780198945253.003.0009",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.07454",
    "type": "report",
    "title": "Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning",
    "author": [
      {
        "family": "Akhlaghi",
        "given": "Amir Mohammad"
      },
      {
        "family": "Shabani",
        "given": "Amirhossein"
      },
      {
        "family": "Abdolmaleki",
        "given": "Mostafa"
      },
      {
        "family": "Kheradpisheh",
        "given": "Saeed Reza"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          8
        ]
      ]
    },
    "abstract": "The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique \"warm-up\" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.",
    "URL": "http://arxiv.org/abs/2512.07454v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.08057",
    "type": "report",
    "title": "Large Language Models for Education and Research: An Empirical and User Survey-based Analysis",
    "author": [
      {
        "family": "Rahman",
        "given": "Md Mostafizer"
      },
      {
        "family": "Shiplu",
        "given": "Ariful Islam"
      },
      {
        "family": "Amin",
        "given": "Md Faizul Ibne"
      },
      {
        "family": "Watanobe",
        "given": "Yutaka"
      },
      {
        "family": "Peng",
        "given": "Lu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          8
        ]
      ]
    },
    "abstract": "Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency-focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.",
    "URL": "http://arxiv.org/abs/2512.08057v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.08545",
    "type": "report",
    "title": "Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks",
    "author": [
      {
        "family": "Kar",
        "given": "Indrajit"
      },
      {
        "family": "Kumar",
        "given": "Kalathur Chenchu Kishore"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          9
        ]
      ]
    },
    "abstract": "Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.",
    "URL": "http://arxiv.org/abs/2512.08545v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.08596",
    "type": "report",
    "title": "Examining Student Interactions with a Pedagogical AI-Assistant for Essay Writing and their Impact on Students Writing Quality",
    "author": [
      {
        "family": "Febriantoro",
        "given": "Wicaksono"
      },
      {
        "family": "Zhou",
        "given": "Qi"
      },
      {
        "family": "Suraworachet",
        "given": "Wannapon"
      },
      {
        "family": "Bulathwela",
        "given": "Sahan"
      },
      {
        "family": "Gauthier",
        "given": "Andrea"
      },
      {
        "family": "Millan",
        "given": "Eva"
      },
      {
        "family": "Cukurova",
        "given": "Mutlu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          9
        ]
      ]
    },
    "abstract": "The dynamic nature of interactions between students and GenAI, as well as their relationship to writing quality, remains underexplored. While most research has examined how general-purpose GenAI can support writing, fewer studies have investigated how students interact with pedagogically designed systems across different phases of the writing process. To address this gap, we evaluated a GenAI-driven essay-writing assistant (EWA) designed to support higher education students in argumentative writing. Drawing on 1,282 interaction logs from 32 undergraduates during a two-hour writing session, Sequential Pattern Mining and K-Means clustering were used to identify behavioral patterns. Two clusters emerged: Cluster 1 emphasized outline planning and essay structure, while Cluster 2 focused on content development. A Mann-Whitney U test revealed a moderate effect size (r = 0.36) in the essay Organization dimension, with Cluster 1 showing higher scores. Qualitative analysis indicated that students with better performance actively wrote and shared essay sections with EWA for feedback, rather than interacted passively by asking questions. These findings suggest implications for teaching and system design. Teachers can encourage active engagement, while future EWAs may integrate automatic labeling and monitoring to prompt students to move from questioning to writing, enabling fuller benefits from GenAI-supported learning.",
    "URL": "http://arxiv.org/abs/2512.08596v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.08713",
    "type": "report",
    "title": "Automatic Essay Scoring and Feedback Generation in Basque Language Learning",
    "author": [
      {
        "family": "Azurmendi",
        "given": "Ekhi"
      },
      {
        "family": "Arregi",
        "given": "Xabier"
      },
      {
        "family": "Lacalle",
        "given": "Oier Lopez de"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          9
        ]
      ]
    },
    "abstract": "This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.",
    "URL": "http://arxiv.org/abs/2512.08713v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.08998",
    "type": "report",
    "title": "DermETAS-SNA LLM: A Dermatology Focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM Assistant",
    "author": [
      {
        "family": "Oruganty",
        "given": "Nitya Phani Santosh"
      },
      {
        "family": "Murali",
        "given": "Keerthi Vemula"
      },
      {
        "family": "Ngan",
        "given": "Chun-Kit"
      },
      {
        "family": "Pinho",
        "given": "Paulo Bandeira"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          9
        ]
      ]
    },
    "abstract": "Our work introduces the DermETAS-SNA LLM Assistant that integrates Dermatology-focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM. The assistant dynamically learns skin-disease classifiers and provides medically informed descriptions to facilitate clinician-patient interpretation. Contributions include: (1) Developed an ETAS framework on the SKINCON dataset to optimize a Vision Transformer (ViT) tailored for dermatological feature representation and then fine-tuned binary classifiers for each of the 23 skin disease categories in the DermNet dataset to enhance classification performance; (2) Designed a StackNet architecture that integrates multiple fine-tuned binary ViT classifiers to enhance predictive robustness and mitigate class imbalance issues; (3) Implemented a RAG pipeline, termed Diagnostic Explanation and Retrieval Model for Dermatology, which harnesses the capabilities of the Google Gemini 2.5 Pro LLM architecture to generate personalized, contextually informed diagnostic descriptions and explanations for patients, leveraging a repository of verified dermatological materials; (4) Performed extensive experimental evaluations on 23 skin disease categories to demonstrate performance increase, achieving an overall F1-score of 56.30% that surpasses SkinGPT-4 (48.51%) by a considerable margin, representing a performance increase of 16.06%; (5) Conducted a domain-expert evaluation, with eight licensed medical doctors, of the clinical responses generated by our AI assistant for seven dermatological conditions. Our results show a 92% agreement rate with the assessments provided by our AI assistant (6) Created a proof-of-concept prototype that fully integrates our DermETAS-SNA LLM into our AI assistant to demonstrate its practical feasibility for real-world clinical and educational applications.",
    "URL": "http://arxiv.org/abs/2512.08998v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.10159",
    "type": "report",
    "title": "Enhancing Large Language Models for End-to-End Circuit Analysis Problem Solving",
    "author": [
      {
        "family": "Chen",
        "given": "Liangliang"
      },
      {
        "family": "Sun",
        "given": "Weiyu"
      },
      {
        "family": "Zhang",
        "given": "Ying"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          10
        ]
      ]
    },
    "abstract": "Large language models (LLMs) have shown strong performance in data-rich domains such as programming, but their reliability in engineering tasks remains limited. Circuit analysis -- requiring multimodal understanding and precise mathematical reasoning -- highlights these challenges. Although Gemini 2.5 Pro improves diagram interpretation and analog-circuit reasoning, it still struggles to consistently produce correct solutions when given both text and circuit diagrams. At the same time, engineering education needs scalable AI tools capable of generating accurate solutions for tasks such as automated homework feedback and question-answering. This paper presents an enhanced, end-to-end circuit problem solver built on Gemini 2.5 Pro. We first benchmark Gemini on a representative set of undergraduate circuit problems and identify two major failure modes: 1) circuit-recognition hallucinations, particularly incorrect source polarity detection, and 2) reasoning-process hallucinations, such as incorrect current directions. To address recognition errors, we integrate a fine-tuned YOLO detector and OpenCV processing to isolate voltage and current sources, enabling Gemini to re-identify source polarities from cropped images with near-perfect accuracy. To reduce reasoning errors, we introduce an ngspice-based verification loop in which Gemini generates a .cir file, ngspice simulates the circuit, and discrepancies trigger iterative regeneration with optional human-in-the-loop review. Across 83 problems, the proposed pipeline achieves a 97.59% success rate (81 correct solutions), substantially outperforming Gemini 2.5 Pro's original 79.52% accuracy. This system extends LLM capabilities for multimodal engineering problem-solving and supports the creation of high-quality educational datasets and AI-powered instructional tools.",
    "URL": "http://arxiv.org/abs/2512.10159v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.10441",
    "type": "report",
    "title": "Decoding Student Minds: Leveraging Conversational Agents for Psychological and Learning Analysis",
    "author": [
      {
        "family": "Chaabene",
        "given": "Nour El Houda Ben"
      },
      {
        "family": "Hammami",
        "given": "Hamza"
      },
      {
        "family": "Kahloul",
        "given": "Laid"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          11
        ]
      ]
    },
    "abstract": "This paper presents a psychologically-aware conversational agent designed to enhance both learning performance and emotional well-being in educational settings. The system combines Large Language Models (LLMs), a knowledge graph-enhanced BERT (KG-BERT), and a bidirectional Long Short-Term Memory (LSTM) with attention to classify students' cognitive and affective states in real time. Unlike prior chatbots limited to either tutoring or affective support, our approach leverages multimodal data-including textual semantics, prosodic speech features, and temporal behavioral trends-to infer engagement, stress, and conceptual understanding. A pilot study with university students demonstrated improved motivation, reduced stress, and moderate academic gains compared to baseline methods. These results underline the promise of integrating semantic reasoning, multimodal fusion, and temporal modeling to support adaptive, student-centered educational interventions.",
    "URL": "http://arxiv.org/abs/2512.10441v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.10487",
    "type": "report",
    "title": "LLM-Assisted AHP for Explainable Cyber Range Evaluation",
    "author": [
      {
        "family": "Kampourakis",
        "given": "Vyron"
      },
      {
        "family": "Kavallieratos",
        "given": "Georgios"
      },
      {
        "family": "Spathoulas",
        "given": "Georgios"
      },
      {
        "family": "Gkioulos",
        "given": "Vasileios"
      },
      {
        "family": "Katsikas",
        "given": "Sokratis"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          11
        ]
      ]
    },
    "abstract": "Cyber Ranges (CRs) have emerged as prominent platforms for cybersecurity training and education, especially for Critical Infrastructure (CI) sectors that face rising cyber threats. One way to address these threats is through hands-on exercises that bridge IT and OT domains to improve defensive readiness. However, consistently evaluating whether a CR platform is suitable and effective remains a challenge. This paper proposes an evaluation framework for CRs, emphasizing mission-critical settings by using a multi-criteria decision-making approach. We define a set of evaluation criteria that capture technical fidelity, training and assessment capabilities, scalability, usability, and other relevant factors. To weight and aggregate these criteria, we employ the Analytic Hierarchy Process (AHP), supported by a simulated panel of multidisciplinary experts implemented through a Large Language Model (LLM). This LLM-assisted expert reasoning enables consistent and reproducible pairwise comparisons across criteria without requiring direct expert convening. The framework's output equals quantitative scores that facilitate objective comparison of CR platforms and highlight areas for improvement. Overall, this work lays the foundation for a standardized and explainable evaluation methodology to guide both providers and end-users of CRs.",
    "URL": "http://arxiv.org/abs/2512.10487v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.10758",
    "type": "report",
    "title": "Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework",
    "author": [
      {
        "family": "Ding",
        "given": "Kaihua"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          11
        ]
      ]
    },
    "abstract": "The rapid adoption of generative AI has undermined traditional modular assessments in computing education, creating a disconnect between academic evaluation and industry practice. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and multi-year empirical validation.\n  We make three contributions. First, we establish two theoretical results: (1) assessments composed of interconnected problems, where outputs feed into subsequent stages, are more AI-resilient than modular assessments because current language models struggle with sustained multi-step reasoning and context; and (2) semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution patterns. These results challenge common policy and institutional guidance that promotes open-ended assessments as the primary safeguard for academic integrity.\n  Second, we validate these results using data from four university data science courses (N = 138). While students achieve near-perfect scores on AI-assisted modular homework, performance drops by roughly 30 percentage points on proctored exams, indicating substantial AI score inflation. Interconnected projects remain strongly correlated with modular assessments, suggesting they measure the same underlying skills while resisting AI misuse. Proctored exams show weaker alignment, implying they may assess test-taking ability rather than intended learning outcomes.\n  Third, we translate these findings into a practical assessment design framework. The proposed approach enables educators to create assessments that promote integrative thinking, reflect real-world AI-augmented workflows, and naturally resist trivial delegation to generative AI, thereby helping restore academic integrity.",
    "URL": "http://arxiv.org/abs/2512.10758v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.10777",
    "type": "report",
    "title": "Opportunities and Challenges in Harnessing Digital Technology for Effective Teaching and Learning",
    "author": [
      {
        "family": "Chen",
        "given": "Zhongzhou"
      },
      {
        "family": "Singh",
        "given": "Chandralekha"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          11
        ]
      ]
    },
    "abstract": "Most of today's educators are in no shortage of digital and online learning technologies available at their fingertips, ranging from Learning Management Systems such as Canvas, Blackboard, or Moodle, online meeting tools, online homework, and tutoring systems, exam proctoring platforms, computer simulations, and even virtual reality/augmented reality technologies. Furthermore, with the rapid development and wide availability of generative artificial intelligence (GenAI) services such as ChatGPT, we are just at the beginning of harnessing their potential to transform higher education. Yet, facing the large number of available options provided by cutting-edge technology, an imminent question on the mind of most educators is the following: how should I choose the technologies and integrate them into my teaching process so that they would best support student learning? We contemplate over these types of important and timely questions and share our reflections on evidence-based approaches to harnessing digital learning tools using a Self-regulated Engaged Learning Framework we have employed in our research in physics education that can be valuable for educators in other disciplines.",
    "DOI": "10.3390/higheredu4010006",
    "URL": "https://doi.org/10.3390/higheredu4010006",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.10785",
    "type": "report",
    "title": "Developing and Evaluating a Large Language Model-Based Automated Feedback System Grounded in Evidence-Centered Design for Supporting Physics Problem Solving",
    "author": [
      {
        "family": "Maus",
        "given": "Holger"
      },
      {
        "family": "Tschisgale",
        "given": "Paul"
      },
      {
        "family": "Kieser",
        "given": "Fabian"
      },
      {
        "family": "Petersen",
        "given": "Stefan"
      },
      {
        "family": "Wulff",
        "given": "Peter"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          11
        ]
      ]
    },
    "abstract": "Generative AI offers new opportunities for individualized and adaptive learning, particularly through large language model (LLM)-based feedback systems. While LLMs can produce effective feedback for relatively straightforward conceptual tasks, delivering high-quality feedback for tasks that require advanced domain expertise, such as physics problem solving, remains a substantial challenge. This study presents the design of an LLM-based feedback system for physics problem solving grounded in evidence-centered design (ECD) and evaluates its performance within the German Physics Olympiad. Participants assessed the usefulness and accuracy of the generated feedback, which was generally perceived as useful and highly accurate. However, an in-depth analysis revealed that the feedback contained factual errors in 20% of cases; errors that often went unnoticed by the students. We discuss the risks associated with uncritical reliance on LLM-based feedback systems and outline potential directions for generating more adaptive and reliable LLM-based feedback in the future.",
    "URL": "http://arxiv.org/abs/2512.10785v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.11882",
    "type": "report",
    "title": "An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education",
    "author": [
      {
        "family": "Happe",
        "given": "Lucia"
      },
      {
        "family": "Fuch√ü",
        "given": "Dominik"
      },
      {
        "family": "H√ºttner",
        "given": "Luca"
      },
      {
        "family": "Marquardt",
        "given": "Kai"
      },
      {
        "family": "Koziolek",
        "given": "Anne"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          8
        ]
      ]
    },
    "abstract": "The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn.",
    "URL": "http://arxiv.org/abs/2512.11882v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.11930",
    "type": "report",
    "title": "Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction",
    "author": [
      {
        "family": "Jiang",
        "given": "Mei"
      },
      {
        "family": "Shen",
        "given": "Haihai"
      },
      {
        "family": "Luo",
        "given": "Zhuo"
      },
      {
        "family": "Li",
        "given": "Bingdong"
      },
      {
        "family": "Hong",
        "given": "Wenjing"
      },
      {
        "family": "Tang",
        "given": "Ke"
      },
      {
        "family": "Zhou",
        "given": "Aimin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          12
        ]
      ]
    },
    "abstract": "Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent.",
    "URL": "http://arxiv.org/abs/2512.11930v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.11934",
    "type": "report",
    "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching",
    "author": [
      {
        "family": "Mazaherian",
        "given": "Adeleh"
      },
      {
        "family": "Nourbakhsh",
        "given": "Erfan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          12
        ]
      ]
    },
    "abstract": "The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent).",
    "URL": "http://arxiv.org/abs/2512.11934v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.12045",
    "type": "report",
    "title": "AI as a Teaching Partner: Early Lessons from Classroom Codesign with Secondary Teachers",
    "author": [
      {
        "family": "Liu",
        "given": "Alex"
      },
      {
        "family": "Esbenshade",
        "given": "Lief"
      },
      {
        "family": "Sarkar",
        "given": "Shawon"
      },
      {
        "family": "Tian",
        "given": "Zewei"
      },
      {
        "family": "Sun",
        "given": "Min"
      },
      {
        "family": "Zhang",
        "given": "Zachary"
      },
      {
        "family": "Han",
        "given": "Thomas"
      },
      {
        "family": "Lapicus",
        "given": "Yulia"
      },
      {
        "family": "He",
        "given": "Kevin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          12
        ]
      ]
    },
    "abstract": "This report presents a comprehensive account of the Colleague AI Classroom pilot, a collaborative design (co-design) study that brought generative AI technology directly into real classrooms. In this study, AI functioned as a third agent, an active participant that mediated feedback, supported inquiry, and extended teachers' instructional reach while preserving human judgment and teacher authority.\n  Over seven weeks in spring 2025, 21 in-service teachers from four Washington State public school districts and one independent school integrated four AI-powered features of the Colleague AI Classroom into their instruction: Teaching Aide, Assessment and AI Grading, AI Tutor, and Student Growth Insights. More than 600 students in grades 6-12 used the platform in class at the direction of their teachers, who designed and facilitated the AI activities.\n  During the Classroom pilot, teachers were co-design partners: they planned activities, implemented them with students, and provided weekly reflections on AI's role in classroom settings. The teachers' feedback guided iterative improvements for Colleague AI. The research team captured rich data through surveys, planning and reflection forms, group meetings, one-on-one interviews, and platform usage logs to understand where AI adds instructional value and where it requires refinement.",
    "URL": "http://arxiv.org/abs/2512.12045v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.12306",
    "type": "report",
    "title": "From Co-Design to Metacognitive Laziness: Evaluating Generative AI in Vocational Education",
    "author": [
      {
        "family": "Yunus",
        "given": "Amir"
      },
      {
        "family": "Gay",
        "given": "Peng Rend"
      },
      {
        "family": "Lee",
        "given": "Oon Teng"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          13
        ]
      ]
    },
    "abstract": "This study examines the development and deployment of a Generative AI proof-of-concept (POC) designed to support lecturers in a vocational education setting in Singapore. Employing a user-centred, mixed-methods design process, we co-developed an AI chatbot with lecturers to address recurring instructional challenges during exam preparation, specifically managing repetitive questions and scaling feedback delivery. The POC achieved its primary operational goals: lecturers reported streamlined workflows, reduced cognitive load, and observed improved student confidence in navigating course content. However, the deployment yielded unexpected insights into student learning behaviours. Despite enhanced teaching processes, performance data revealed no significant improvement in overall student assessment outcomes. Deep analysis of interaction logs identified concerning patterns, including self-efficacy-driven dependency, \"metacognitive laziness\" (cognitive offloading), and divergent usage strategies. While high-ability students leveraged the tool for strategic verification, low-ability students frequently used it to bypass cognitive effort, potentially exacerbating performance gaps. These findings suggest that Generative AI's educational influence extends beyond instructional efficiency to shape cognitive engagement, self-regulation, and learner equity. The study raises consequential design questions regarding how AI tools can be engineered to minimise dependency, scaffold metacognitive development, and calibrate support across varying ability levels. We conclude that while Generative AI can substantially enhance the teaching experience, achieving meaningful learning gains requires rigorous attention to learner behaviour and the equitable design of AI-supported environments.",
    "URL": "http://arxiv.org/abs/2512.12306v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.12503",
    "type": "report",
    "title": "KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs",
    "author": [
      {
        "family": "Ye",
        "given": "Mingrui"
      },
      {
        "family": "Zheng",
        "given": "Chanjin"
      },
      {
        "family": "Yu",
        "given": "Zengyi"
      },
      {
        "family": "Xiang",
        "given": "Chenyu"
      },
      {
        "family": "Zhao",
        "given": "Zhixue"
      },
      {
        "family": "Yuan",
        "given": "Zheng"
      },
      {
        "family": "Yannakoudakis",
        "given": "Helen"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          14
        ]
      ]
    },
    "abstract": "Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.",
    "URL": "http://arxiv.org/abs/2512.12503v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.12633",
    "type": "report",
    "title": "DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model",
    "author": [
      {
        "family": "Tao",
        "given": "Zhou"
      },
      {
        "family": "Wang",
        "given": "Shida"
      },
      {
        "family": "Hua",
        "given": "Yongxiang"
      },
      {
        "family": "Cao",
        "given": "Haoyu"
      },
      {
        "family": "Xu",
        "given": "Linli"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          14
        ]
      ]
    },
    "abstract": "Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.",
    "URL": "http://arxiv.org/abs/2512.12633v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.12770",
    "type": "report",
    "title": "Curi√≥-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining",
    "author": [
      {
        "family": "Almeida",
        "given": "Thales Sales"
      },
      {
        "family": "Nogueira",
        "given": "Rodrigo"
      },
      {
        "family": "Pedrini",
        "given": "H√©lio"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          14
        ]
      ]
    },
    "abstract": "Continued pretraining extends a language model's capabilities by further exposing it to additional data, often tailored to a specific linguistic or domain context. This strategy has emerged as an efficient alternative to full retraining when adapting general-purpose models to new settings. In this work, we investigate this paradigm through Curi√≥ 7B, a 7-billion-parameter model derived from LLaMA-2 and trained on 100 billion Portuguese tokens from the ClassiCC-PT corpus - the most extensive Portuguese-specific continued-pretraining effort above the three-billion-parameter scale to date. Beyond scale, we investigate whether quantity alone suffices or whether data quality plays a decisive role in linguistic adaptation. To this end, we introduce Curi√≥-Edu 7B, a variant trained exclusively on the educational and STEM-filtered subset of the same corpus, totaling just 10 billion tokens. Despite using only 10% of the data and 20% of the computation, Curi√≥-Edu 7B surpasses the full-corpus model in our evaluations, demonstrating that data selection can be fundamental even when adapting models with limited prior exposure to the target language. The developed models are available at https://huggingface.co/collections/ClassiCC-Corpus/curio-edu",
    "URL": "http://arxiv.org/abs/2512.12770v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.12775",
    "type": "report",
    "title": "Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions",
    "author": [
      {
        "family": "Araujo",
        "given": "Pedro Henrique Luz de"
      },
      {
        "family": "Hedderich",
        "given": "Michael A."
      },
      {
        "family": "Modarressi",
        "given": "Ali"
      },
      {
        "family": "Schuetze",
        "given": "Hinrich"
      },
      {
        "family": "Roth",
        "given": "Benjamin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          14
        ]
      ]
    },
    "abstract": "Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.",
    "URL": "http://arxiv.org/abs/2512.12775v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.13102",
    "type": "report",
    "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions",
    "author": [
      {
        "family": "Ambati",
        "given": "Rajeev Bhatt"
      },
      {
        "family": "Niu",
        "given": "Tianyi"
      },
      {
        "family": "Singh",
        "given": "Aashu"
      },
      {
        "family": "Mishra",
        "given": "Shlok"
      },
      {
        "family": "Srivastava",
        "given": "Shashank"
      },
      {
        "family": "Chaturvedi",
        "given": "Snigdha"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          15
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.",
    "URL": "http://arxiv.org/abs/2512.13102v2",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.13526",
    "type": "report",
    "title": "Async Control: Stress-testing Asynchronous Control Measures for LLM Agents",
    "author": [
      {
        "family": "Stickland",
        "given": "Asa Cooper"
      },
      {
        "family": "Michelfeit",
        "given": "Jan"
      },
      {
        "family": "Mani",
        "given": "Arathi"
      },
      {
        "family": "Griffin",
        "given": "Charlie"
      },
      {
        "family": "Matthews",
        "given": "Ollie"
      },
      {
        "family": "Korbak",
        "given": "Tomek"
      },
      {
        "family": "Inglis",
        "given": "Rogan"
      },
      {
        "family": "Makins",
        "given": "Oliver"
      },
      {
        "family": "Cooney",
        "given": "Alan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          15
        ]
      ]
    },
    "abstract": "LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.",
    "URL": "http://arxiv.org/abs/2512.13526v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.13658",
    "type": "report",
    "title": "Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance",
    "author": [
      {
        "family": "Molavi",
        "given": "Mohammadreza"
      },
      {
        "family": "Moein",
        "given": "Mohammad"
      },
      {
        "family": "Tavakoli",
        "given": "Mohammadreza"
      },
      {
        "family": "Faraji",
        "given": "Abdolali"
      },
      {
        "family": "Mol",
        "given": "Stefan T."
      },
      {
        "family": "Kismih√≥k",
        "given": "G√°bor"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          15
        ]
      ]
    },
    "abstract": "As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.",
    "URL": "http://arxiv.org/abs/2512.13658v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.13730",
    "type": "report",
    "title": "Exploring the Modular Integration of \"AI + Architecture\" Pedagogy in Undergraduate Design Education: A Case Study of Architectural Design III/IV Courses at Zhejiang University",
    "author": [
      {
        "family": "Jiaqi",
        "given": "Wang"
      },
      {
        "family": "Yi",
        "given": "Lan"
      },
      {
        "family": "Xiang",
        "given": "Chen"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          14
        ]
      ]
    },
    "abstract": "This study investigates AI integration in architectural education through a teaching experiment in Zhejiang University's 2024-25 grade three undergraduate design studio. Adopting a dual-module framework (20-hour AI training + embedded ethics discussions), the course introduced deep learning models, LLMs, AIGC, LoRA, and ComfyUI while maintaining the original curriculum structure, supported by dedicated technical instructors. Findings demonstrate the effectiveness of phased guidance, balanced technical-ethical approaches, and institutional support. The model improved students' digital skills and strategic cognition while addressing AI ethics, providing a replicable approach combining technical and critical learning in design education.",
    "URL": "http://arxiv.org/abs/2512.13730v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.13914",
    "type": "report",
    "title": "Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming",
    "author": [
      {
        "family": "Nanjundappa",
        "given": "Bhargav Chickmagalur"
      },
      {
        "family": "Maaheshwari",
        "given": "Spandan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          15
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context.\n  We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.",
    "URL": "http://arxiv.org/abs/2512.13914v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.13978",
    "type": "report",
    "title": "Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms",
    "author": [
      {
        "family": "Cao",
        "given": "Yang"
      },
      {
        "family": "Chen",
        "given": "Yubin"
      },
      {
        "family": "Guo",
        "given": "Xuyang"
      },
      {
        "family": "Song",
        "given": "Zhao"
      },
      {
        "family": "Yue",
        "given": "Song"
      },
      {
        "family": "Zhang",
        "given": "Jiahao"
      },
      {
        "family": "Zhao",
        "given": "Jiale"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          16
        ]
      ]
    },
    "abstract": "The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${√≥}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].\n  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.",
    "URL": "http://arxiv.org/abs/2512.13978v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.14220",
    "type": "report",
    "title": "Estimating problem difficulty without ground truth using Large Language Model comparisons",
    "author": [
      {
        "family": "Ballon",
        "given": "Marthe"
      },
      {
        "family": "Algaba",
        "given": "Andres"
      },
      {
        "family": "Verbeken",
        "given": "Brecht"
      },
      {
        "family": "Ginis",
        "given": "Vincent"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          16
        ]
      ]
    },
    "abstract": "Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \\geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\\%$ degradation in Pearson correlation for $10\\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.",
    "URL": "http://arxiv.org/abs/2512.14220v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.14673",
    "type": "report",
    "title": "Reconsidering Conversational Norms in LLM Chatbots for Sustainable AI",
    "author": [
      {
        "family": "Santos",
        "given": "Ronnie de Souza"
      },
      {
        "family": "Magalh√£es",
        "given": "Cleyton"
      },
      {
        "family": "Santos",
        "given": "Italo"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          16
        ]
      ]
    },
    "abstract": "LLM based chatbots have become central interfaces in technical, educational, and analytical domains, supporting tasks such as code reasoning, problem solving, and information exploration. As these systems scale, sustainability concerns have intensified, with most assessments focusing on model architecture, hardware efficiency, and deployment infrastructure. However, existing mitigation efforts largely overlook how user interaction practices themselves shape the energy profile of LLM based systems. In this vision paper, we argue that interaction level behavior appears to be an underexamined factor shaping the environmental impact of LLM based systems, and we present this issue across four dimensions. First, extended conversational patterns increase token production and raise the computational cost of inference. Second, expectations of instant responses limit opportunities for energy aware scheduling and workload consolidation. Third, everyday user habits contribute to cumulative operational demand in ways that are rarely quantified. Fourth, the accumulation of context affects memory requirements and reduces the efficiency of long running dialogues. Addressing these challenges requires rethinking how chatbot interactions are designed and conceptualized, and adopting perspectives that recognize sustainability as partly dependent on the conversational norms through which users engage with LLM based systems.",
    "URL": "http://arxiv.org/abs/2512.14673v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.14944",
    "type": "report",
    "title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning",
    "author": [
      {
        "family": "Jeddi",
        "given": "Ahmadreza"
      },
      {
        "family": "Karaimer",
        "given": "Hakki Can"
      },
      {
        "family": "Nguyen",
        "given": "Hue"
      },
      {
        "family": "Wang",
        "given": "Zhongling"
      },
      {
        "family": "Zhao",
        "given": "Ke"
      },
      {
        "family": "Rajabi",
        "given": "Javad"
      },
      {
        "family": "Zhang",
        "given": "Ran"
      },
      {
        "family": "Goyal",
        "given": "Raghav"
      },
      {
        "family": "Taati",
        "given": "Babak"
      },
      {
        "family": "Grzeszczuk",
        "given": "Radek"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          16
        ]
      ]
    },
    "abstract": "Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.",
    "URL": "http://arxiv.org/abs/2512.14944v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.15183",
    "type": "report",
    "title": "From NLG Evaluation to Modern Student Assessment in the Era of ChatGPT: The Great Misalignment Problem and Pedagogical Multi-Factor Assessment (P-MFA)",
    "author": [
      {
        "family": "H√§m√§l√§inen",
        "given": "Mika"
      },
      {
        "family": "Leivisk√§",
        "given": "Kimmo"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          17
        ]
      ]
    },
    "abstract": "This paper explores the growing epistemic parallel between NLG evaluation and grading of students in a Finnish University. We argue that both domains are experiencing a Great Misalignment Problem. As students increasingly use tools like ChatGPT to produce sophisticated outputs, traditional assessment methods that focus on final products rather than learning processes have lost their validity. To address this, we introduce the Pedagogical Multi-Factor Assessment (P-MFA) model, a process-based, multi-evidence framework inspired by the logic of multi-factor authentication.",
    "URL": "http://arxiv.org/abs/2512.15183v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.15298",
    "type": "report",
    "title": "ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I",
    "author": [
      {
        "family": "Ga",
        "given": "Seok-Hyun"
      },
      {
        "family": "Chang",
        "given": "Chun-Yen"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          17
        ]
      ]
    },
    "abstract": "The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that \"Perception Errors\" were dominant, highlighting a \"Perception-Cognition Gap\" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a \"Calculation-Conceptualization Discrepancy,\" successfully performing calculations while failing to apply the underlying scientific concepts, and \"Process Hallucination,\" where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing \"AI-resistant questions\" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.",
    "URL": "http://arxiv.org/abs/2512.15298v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.15397",
    "type": "report",
    "title": "ORACLE: Time-Dependent Recursive Summary Graphs for Foresight on News Data Using LLMs",
    "author": [
      {
        "family": "Kharlashkin",
        "given": "Lev"
      },
      {
        "family": "Morooka",
        "given": "Eiaki"
      },
      {
        "family": "Tereshchenko",
        "given": "Yehor"
      },
      {
        "family": "H√§m√§l√§inen",
        "given": "Mika"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          17
        ]
      ]
    },
    "abstract": "ORACLE turns daily news into week-over-week, decision-ready insights for one of the Finnish University of Applied Sciences. The platform crawls and versions news, applies University-specific relevance filtering, embeds content, classifies items into PESTEL dimensions and builds a concise Time-Dependent Recursive Summary Graph (TRSG): two clustering layers summarized by an LLM and recomputed weekly. A lightweight change detector highlights what is new, removed or changed, then groups differences into themes for PESTEL-aware analysis. We detail the pipeline, discuss concrete design choices that make the system stable in production and present a curriculum-intelligence use case with an evaluation plan.",
    "URL": "http://arxiv.org/abs/2512.15397v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.16036",
    "type": "report",
    "title": "Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education",
    "author": [
      {
        "family": "Woodbridge",
        "given": "Diane Myung-kyung"
      },
      {
        "family": "Seba",
        "given": "Allyson"
      },
      {
        "family": "Seba",
        "given": "Freddie"
      },
      {
        "family": "Schwartz",
        "given": "Aydin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          17
        ]
      ]
    },
    "abstract": "As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.",
    "URL": "http://arxiv.org/abs/2512.16036v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.16428",
    "type": "report",
    "title": "Preparing Future-Ready Learners: K12 Skills Shift and GenAI EdTech Innovation Direction",
    "author": [
      {
        "family": "Miao",
        "given": "Xin"
      },
      {
        "family": "Mishra",
        "given": "Pawan Kumar"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          18
        ]
      ]
    },
    "abstract": "Since Generative AI came out it has quickly embedded itself in our social fabric, triggering lots of discussions, predictions, and efforts from research, industry, government and capital market to experiment and embrace the technology. The question for the global K12 education is, what and how should our children learn in this fast changing world to be prepared for the changing labor market and live a happy and balanced life? Three key aspects will be discussed: 1) Skills; 2) Evaluation of Learning; 3) Strategic GenAI-powered EdTech innovation for long term educational impact.",
    "URL": "http://arxiv.org/abs/2512.16428v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.16649",
    "type": "report",
    "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
    "author": [
      {
        "family": "He",
        "given": "Bingxiang"
      },
      {
        "family": "Qu",
        "given": "Zekai"
      },
      {
        "family": "Liu",
        "given": "Zeyuan"
      },
      {
        "family": "Chen",
        "given": "Yinghao"
      },
      {
        "family": "Zuo",
        "given": "Yuxin"
      },
      {
        "family": "Qian",
        "given": "Cheng"
      },
      {
        "family": "Zhang",
        "given": "Kaiyan"
      },
      {
        "family": "Chen",
        "given": "Weize"
      },
      {
        "family": "Xiao",
        "given": "Chaojun"
      },
      {
        "family": "Cui",
        "given": "Ganqu"
      },
      {
        "family": "Ding",
        "given": "Ning"
      },
      {
        "family": "Liu",
        "given": "Zhiyuan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          18
        ]
      ]
    },
    "abstract": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \\textbf{Is this complexity necessary?} We present \\textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2$\\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.",
    "URL": "http://arxiv.org/abs/2512.16649v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.16701",
    "type": "report",
    "title": "Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences",
    "author": [
      {
        "family": "Adorni",
        "given": "Giovanni"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          18
        ]
      ]
    },
    "abstract": "Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\\-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.\n  We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.",
    "URL": "http://arxiv.org/abs/2512.16701v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.17060",
    "type": "report",
    "title": "On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues",
    "author": [
      {
        "family": "Zamojska",
        "given": "Monika"
      },
      {
        "family": "Chudziak",
        "given": "Jaros≈Çaw A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          18
        ]
      ]
    },
    "abstract": "LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.",
    "URL": "http://arxiv.org/abs/2512.17060v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.18306",
    "type": "report",
    "title": "Leveraging Peer, Self, and Teacher Assessments for Generative AI-Enhanced Feedback",
    "author": [
      {
        "family": "Becerra",
        "given": "Alvaro"
      },
      {
        "family": "Cobos",
        "given": "Ruth"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          20
        ]
      ]
    },
    "abstract": "Providing timely and meaningful feedback remains a persistent challenge in higher education, especially in large courses where teachers must balance formative depth with scalability. Recent advances in Generative Artificial Intelligence (GenAI) offer new opportunities to support feedback processes while maintaining human oversight. This paper presents an study conducted within the AICoFe (AI-based Collaborative Feedback) system, which integrates teacher, peer, and self-assessments of engineering students' oral presentations. Using a validated rubric, 46 evaluation sets were analyzed to examine agreement, correlation, and bias across evaluators. The analyses revealed consistent overall alignment among sources but also systematic variations in scoring behavior, reflecting distinct evaluative perspectives. These findings informed the proposal of an enhanced GenAI model within AICoFe system, designed to integrate human assessments through weighted input aggregation, bias detection, and context-aware feedback generation. The study contributes empirical evidence and design principles for developing GenAI-based feedback systems that combine data-based efficiency with pedagogical validity and transparency.",
    "URL": "http://arxiv.org/abs/2512.18306v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.19512",
    "type": "report",
    "title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
    "author": [
      {
        "family": "Song",
        "given": "Ziyang"
      },
      {
        "family": "Zang",
        "given": "Zelin"
      },
      {
        "family": "Chen",
        "given": "Zuyao"
      },
      {
        "family": "Liang",
        "given": "Xusheng"
      },
      {
        "family": "Yi",
        "given": "Dong"
      },
      {
        "family": "Wu",
        "given": "Jinlin"
      },
      {
        "family": "Liu",
        "given": "Hongbin"
      },
      {
        "family": "Luo",
        "given": "Jiebo"
      },
      {
        "family": "Lei",
        "given": "Zhen."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          22
        ]
      ]
    },
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1",
    "URL": "http://arxiv.org/abs/2512.19512v2",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.19682",
    "type": "report",
    "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
    "author": [
      {
        "family": "Guo",
        "given": "Jiacheng"
      },
      {
        "family": "Yang",
        "given": "Ling"
      },
      {
        "family": "Chen",
        "given": "Peter"
      },
      {
        "family": "Xiao",
        "given": "Qixin"
      },
      {
        "family": "Wang",
        "given": "Yinjie"
      },
      {
        "family": "Juan",
        "given": "Xinzhe"
      },
      {
        "family": "Qiu",
        "given": "Jiahao"
      },
      {
        "family": "Shen",
        "given": "Ke"
      },
      {
        "family": "Wang",
        "given": "Mengdi"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          22
        ]
      ]
    },
    "abstract": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $Œ±$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
    "URL": "http://arxiv.org/abs/2512.19682v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.19903",
    "type": "report",
    "title": "How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse",
    "author": [
      {
        "family": "Vanacore",
        "given": "Kirk"
      },
      {
        "family": "Kizilcec",
        "given": "Rene F."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          22
        ]
      ]
    },
    "abstract": "Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.",
    "URL": "http://arxiv.org/abs/2512.19903v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.20714",
    "type": "report",
    "title": "From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education",
    "author": [
      {
        "family": "Reihanian",
        "given": "Iman"
      },
      {
        "family": "Hou",
        "given": "Yunfei"
      },
      {
        "family": "Sun",
        "given": "Qingquan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          23
        ]
      ]
    },
    "abstract": "Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.",
    "DOI": "10.3390/ai7010006",
    "URL": "https://doi.org/10.3390/ai7010006",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.20732",
    "type": "report",
    "title": "FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs",
    "author": [
      {
        "family": "Mohammadzadeh",
        "given": "Saeed"
      },
      {
        "family": "Hamdi",
        "given": "Erfan"
      },
      {
        "family": "Shor",
        "given": "Joel"
      },
      {
        "family": "Lejeune",
        "given": "Emma"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          23
        ]
      ]
    },
    "abstract": "As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification. The discipline requires constructing explicit models of physical systems and reasoning about geometry, spatial relationships, and material behavior, connecting directly to emerging AI goals in physical reasoning and world modeling. We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code. FEM-Bench 2025 contains a suite of introductory but nontrivial tasks aligned with material from a first graduate course on computational mechanics. These tasks capture essential numerical and physical modeling challenges while representing only a small fraction of the complexity present in the discipline. Despite their simplicity, state-of-the-art LLMs do not reliably solve all of them. In a five attempt run, the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least once and 26/33 tasks all five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%. Other popular models showed broad performance variation. FEM-Bench establishes a structured foundation for evaluating AI-generated scientific code, and future iterations will incorporate increasingly sophisticated tasks to track progress as models evolve.",
    "URL": "http://arxiv.org/abs/2512.20732v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.20780",
    "type": "report",
    "title": "Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles",
    "author": [
      {
        "family": "Abdulsalam",
        "given": "Ramatu Oiza"
      },
      {
        "family": "Aroyehun",
        "given": "Segun"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          23
        ]
      ]
    },
    "abstract": "Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.",
    "URL": "http://arxiv.org/abs/2512.20780v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.21041",
    "type": "report",
    "title": "When LLMs fall short in Deductive Coding: Model Comparison and Human AI Collaboration Workflow Design",
    "author": [
      {
        "family": "Li",
        "given": "Zijian"
      },
      {
        "family": "Tang",
        "given": "Luzhen"
      },
      {
        "family": "Xia",
        "given": "Mengyu"
      },
      {
        "family": "Li",
        "given": "Xinyu"
      },
      {
        "family": "Chen",
        "given": "Naping"
      },
      {
        "family": "Ga≈°eviƒá",
        "given": "Dragan"
      },
      {
        "family": "Fan",
        "given": "Yizhou"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          24
        ]
      ]
    },
    "abstract": "With generative artificial intelligence driving the growth of dialogic data in education, automated coding is a promising direction for learning analytics to improve efficiency. This surge highlights the need to understand the nuances of student-AI interactions, especially those rare yet crucial. However, automated coding may struggle to capture these rare codes due to imbalanced data, while human coding remains time-consuming and labour-intensive. The current study examined the potential of large language models (LLMs) to approximate or replace humans in deductive, theory-driven coding, while also exploring how human-AI collaboration might support such coding tasks at scale. We compared the coding performance of small transformer classifiers (e.g., BERT) and LLMs in two datasets, with particular attention to imbalanced head-tail distributions in dialogue codes. Our results showed that LLMs did not outperform BERT-based models and exhibited systematic errors and biases in deductive coding tasks. We designed and evaluated a human-AI collaborative workflow that improved coding efficiency while maintaining coding reliability. Our findings reveal both the limitations of LLMs -- especially their difficulties with semantic similarity and theoretical interpretations and the indispensable role of human judgment -- while demonstrating the practical promise of human-AI collaborative workflows for coding.",
    "URL": "http://arxiv.org/abs/2512.21041v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.21055",
    "type": "report",
    "title": "Making AI Work: An Autoethnography of a Workaround in Higher Education",
    "author": [
      {
        "family": "Lee",
        "given": "Shang Chieh"
      },
      {
        "family": "Narayan",
        "given": "Bhuva"
      },
      {
        "family": "Shum",
        "given": "Simon Buckingham"
      },
      {
        "family": "Ng",
        "given": "Stella"
      },
      {
        "family": "Kocaballi",
        "given": "A. Baki"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          24
        ]
      ]
    },
    "abstract": "Research on the implementation of Generative Artificial Intelligence (GenAI) in higher education often focuses on strategic goals, overlooking the hidden, and often politically charged, labour required to make it functional. This paper provides an insider's account of the sociotechnical friction that arises when an institutional goal of empowering non-technical staff conflicts with the technical limitations of enterprise Large Language Models (LLMs). Through analytic autoethnography, this study examines a GenAI project pushed to an impasse, focusing on a workaround developed to navigate not only technical constraints but also the combined challenge of organisational territoriality and assertions of positional power. Drawing upon Alter's (2014) theory of workarounds, the analysis interprets \"articulation work\" as a form of \"invisible labour\". By engaging with the Information Systems (IS) domains of user innovation and technology-in-practice, this study argues that such user-driven workarounds should be understood not as deviations, but as integral acts of sociotechnical integration. This integration, however, highlights the central paradoxes of modern GenAI where such workarounds for \"unfinished\" systems can simultaneously create unofficial \"shadow\" systems and obscure the crucial, yet invisible, sociotechnical labour involved. The findings suggest that the invisible labour required to integrate GenAI within complex organisational politics is an important, rather than peripheral, component of how it becomes functional in practice.",
    "URL": "http://arxiv.org/abs/2512.21055v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.21238",
    "type": "report",
    "title": "Assessing the Software Security Comprehension of Large Language Models",
    "author": [
      {
        "family": "Siddiq",
        "given": "Mohammed Latif"
      },
      {
        "family": "Sekerak",
        "given": "Natalie"
      },
      {
        "family": "Karam",
        "given": "Antonio"
      },
      {
        "family": "Leal",
        "given": "Maria"
      },
      {
        "family": "Islam-Gomes",
        "given": "Arvin"
      },
      {
        "family": "Santos",
        "given": "Joanna C. S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          24
        ]
      ]
    },
    "abstract": "Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.",
    "URL": "http://arxiv.org/abs/2512.21238v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.21347",
    "type": "report",
    "title": "Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey",
    "author": [
      {
        "family": "Brito",
        "given": "V√≠tor Mateus de"
      },
      {
        "family": "Farias",
        "given": "Kleinner"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          19
        ]
      ]
    },
    "abstract": "The rapid advancement of Large Language Models (LLMs) is reshaping software engineering by profoundly influencing coding, documentation, and system maintenance practices. As these tools become deeply embedded in developers' daily workflows, understanding how they are used has become essential. This paper reports an empirical study of LLM adoption in software engineering, based on a survey of 46 industry professionals with diverse educational backgrounds and levels of experience. The results reveal positive perceptions of LLMs, particularly regarding faster resolution of technical questions, improved documentation support, and enhanced source code standardization. However, respondents also expressed concerns about cognitive dependence, security risks, and the potential erosion of technical autonomy. These findings underscore the need for critical and supervised use of LLM-based tools. By grounding the discussion in empirical evidence from industry practice, this study bridges the gap between academic discourse and real-world software development. The results provide actionable insights for developers and researchers seeking to adopt and evolve LLM-based technologies in a more effective, responsible, and secure manner, while also motivating future research on their cognitive, ethical, and organizational implications.",
    "URL": "http://arxiv.org/abs/2512.21347v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.21422",
    "type": "report",
    "title": "Teaching People LLM's Errors and Getting it Right",
    "author": [
      {
        "family": "Stringham",
        "given": "Nathan"
      },
      {
        "family": "Chaleshtori",
        "given": "Fateme Hashemi"
      },
      {
        "family": "Yan",
        "given": "Xinyuan"
      },
      {
        "family": "Xu",
        "given": "Zhichao"
      },
      {
        "family": "Wang",
        "given": "Bei"
      },
      {
        "family": "Marasoviƒá",
        "given": "Ana"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          24
        ]
      ]
    },
    "abstract": "People use large language models (LLMs) when they should not. This is partly because they see LLMs compose poems and answer intricate questions, so they understandably, but incorrectly, assume LLMs won't stumble on basic tasks like simple arithmetic. Prior work has tried to address this by clustering instance embeddings into regions where an LLM is likely to fail and automatically describing patterns in these regions. The found failure patterns are taught to users to mitigate their overreliance. Yet, this approach has not fully succeeded. In this analysis paper, we aim to understand why.\n  We first examine whether the negative result stems from the absence of failure patterns. We group instances in two datasets by their meta-labels and evaluate an LLM's predictions on these groups. We then define criteria to flag groups that are sizable and where the LLM is error-prone, and find meta-label groups that meet these criteria. Their meta-labels are the LLM's failure patterns that could be taught to users, so they do exist. We next test whether prompting and embedding-based approaches can surface these known failures. Without this, users cannot be taught about them to reduce their overreliance. We find mixed results across methods, which could explain the negative result. Finally, we revisit the final metric that measures teaching effectiveness. We propose to assess a user's ability to effectively use the given failure patterns to anticipate when an LLM is error-prone. A user study shows a positive effect from teaching with this metric, unlike the human-AI team accuracy. Our findings show that teaching failure patterns could be a viable approach to mitigating overreliance, but success depends on better automated failure-discovery methods and using metrics like ours.",
    "URL": "http://arxiv.org/abs/2512.21422v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.22404",
    "type": "report",
    "title": "Mining the Gold: Student-AI Chat Logs as Rich Sources for Automated Knowledge Gap Detection",
    "author": [
      {
        "family": "Fu",
        "given": "Quanzhi"
      },
      {
        "family": "Wu",
        "given": "Qiyu"
      },
      {
        "family": "Williams",
        "given": "Dan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          26
        ]
      ]
    },
    "abstract": "With the significant increase in enrollment in computing-related programs over the past 20 years, lecture sizes have grown correspondingly. In large lectures, instructors face challenges on identifying students' knowledge gaps timely, which is critical for effective teaching. Existing classroom response systems rely on instructor-initiated interactions, which limits their ability to capture the spontaneous knowledge gaps that naturally emerge during lectures. With the widespread adoption of LLMs among students, we recognize these student-AI dialogues as a valuable, student-centered data source for identifying knowledge gaps. In this idea paper, we propose QueryQuilt, a multi-agent LLM framework that automatically detects common knowledge gaps in large-scale lectures by analyzing students' chat logs with AI assistants. QueryQuilt consists of two key components: (1) a Dialogue Agent that responds to student questions while employing probing questions to reveal underlying knowledge gaps, and (2) a Knowledge Gap Identification Agent that systematically analyzes these dialogues to identify knowledge gaps across the student population. By generating frequency distributions of identified gaps, instructors can gain comprehensive insights into class-wide understanding. Our evaluation demonstrates promising results, with QueryQuilt achieving 100% accuracy in identifying knowledge gaps among simulated students and 95% completeness when tested on real student-AI dialogue data. These initial findings indicate the system's potential for facilitate teaching in authentic learning environments. We plan to deploy QueryQuilt in actual classroom settings for comprehensive evaluation, measuring its detection accuracy and impact on instruction.",
    "URL": "http://arxiv.org/abs/2512.22404v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.22496",
    "type": "report",
    "title": "Hierarchical Pedagogical Oversight: A Multi-Agent Adversarial Framework for Reliable AI Tutoring",
    "author": [
      {
        "family": "Sadhu",
        "given": "Saisab"
      },
      {
        "family": "Dhor",
        "given": "Ashim"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          27
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) are increasingly deployed as automated tutors to address educator shortages; however, they often fail at pedagogical reasoning, frequently validating incorrect student solutions (sycophancy) or providing overly direct answers that hinder learning. We introduce Hierarchical Pedagogical Oversight (HPO), a framework that adapts structured adversarial synthesis to educational assessment. Unlike cooperative multi-agent systems that often drift toward superficial consensus, HPO enforces a dialectical separation of concerns: specialist agents first distill dialogue context, which then grounds a moderated, five-act debate between opposing pedagogical critics. We evaluate this framework on the MRBench dataset of 1,214 middle-school mathematics dialogues. Our 8B-parameter model achieves a Macro F1 of 0.845, outperforming GPT-4o (0.812) by 3.3% while using 20 times fewer parameters. These results establish adversarial reasoning as a critical mechanism for deploying reliable, low-compute pedagogical oversight in resource-constrained environments.",
    "URL": "http://arxiv.org/abs/2512.22496v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.22508",
    "type": "report",
    "title": "Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals",
    "author": [
      {
        "family": "Susanto",
        "given": "Lucky"
      },
      {
        "family": "Pranawijayana",
        "given": "Anasta"
      },
      {
        "family": "Sukotjo",
        "given": "Cortino"
      },
      {
        "family": "Prasad",
        "given": "Soni"
      },
      {
        "family": "Wijaya",
        "given": "Derry"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          27
        ]
      ]
    },
    "abstract": "Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models' internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.",
    "URL": "http://arxiv.org/abs/2512.22508v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.23036",
    "type": "report",
    "title": "Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education",
    "author": [
      {
        "family": "Hooshyar",
        "given": "Danial"
      },
      {
        "family": "Yang",
        "given": "Yeongwook"
      },
      {
        "family": "≈†√≠≈ô",
        "given": "Gustav"
      },
      {
        "family": "K√§rkk√§inen",
        "given": "Tommi"
      },
      {
        "family": "H√§m√§l√§inen",
        "given": "Raija"
      },
      {
        "family": "Cukurova",
        "given": "Mutlu"
      },
      {
        "family": "Azevedo",
        "given": "Roger"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          28
        ]
      ]
    },
    "abstract": "The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\\% over the zero-shot baseline, it remains 6\\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.",
    "URL": "http://arxiv.org/abs/2512.23036v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.23053",
    "type": "report",
    "title": "LLteacher: A Tool for the Integration of Generative AI into Statistics Assignments",
    "author": [
      {
        "family": "Furfaro",
        "given": "Emanuela"
      },
      {
        "family": "Mosciatti",
        "given": "Simone"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          28
        ]
      ]
    },
    "abstract": "As generative AI becomes increasingly embedded in everyday life, the thoughtful and intentional integration of AI-based tools into statistics education has become essential. We address this need with a focus on homework assignments and we propose the use of LLMs as a companion to complete homework by developing an open-source tool named LLteacher. This LLM-based tool preserves learning processes and it guides students to engage with AI in ways that support their learning, while ensuring alignment with course content and equitable access. We illustrate LLteacher's design and functionality with examples from an undergraduate Statistical Computing course in R, showing how it supports two distinct pedagogical goals: recalling prior knowledge and discovering new concepts. While this is an initial version, LLteacher demonstrates one possible pathway for integrating generative AI into statistics courses, with strong potential for adaptation to other types of classes and assignments.",
    "URL": "http://arxiv.org/abs/2512.23053v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.23136",
    "type": "report",
    "title": "Understanding EFL Learners' Code-Switching and Teachers' Pedagogical Approaches in LLM-Supported Speaking Practice",
    "author": [
      {
        "family": "Park",
        "given": "Junyeong"
      },
      {
        "family": "Han",
        "given": "Jieun"
      },
      {
        "family": "Park",
        "given": "Yeon Su"
      },
      {
        "family": "Lee",
        "given": "Youngbin"
      },
      {
        "family": "Kim",
        "given": "Suin"
      },
      {
        "family": "Kim",
        "given": "Juho"
      },
      {
        "family": "Oh",
        "given": "Alice"
      },
      {
        "family": "Ahn",
        "given": "So-Yeon"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          29
        ]
      ]
    },
    "abstract": "For English as a Foreign Language (EFL) learners, code-switching (CSW), or alternating between their native language and the target language (English), can lower anxiety and ease communication barriers. Large language models (LLMs), with their multilingual abilities, offer new opportunities to support CSW in speaking practice. Yet, the pedagogical design of LLM-based tutors remains underexplored. To this end, we conducted a six-week study of LLM-mediated speaking practice with 20 Korean EFL learners, alongside a qualitative study with nine English teachers who designed and refined responses to learner CSW. Findings show that learners used CSW not only to bridge lexical gaps but also to express cultural and emotional nuance, prompting teachers to employ selective interventions and dynamic scaffolding strategies. We conclude with design implications for bilingual LLM-powered tutors that leverage teachers' expertise to transform CSW into meaningful learning opportunities.",
    "URL": "http://arxiv.org/abs/2512.23136v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.23587",
    "type": "report",
    "title": "Can AI Recognize Its Own Reflection? Self-Detection Performance of LLMs in Computing Education",
    "author": [
      {
        "family": "Burger",
        "given": "Christopher"
      },
      {
        "family": "Talley",
        "given": "Karmece"
      },
      {
        "family": "Trotter",
        "given": "Christina"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          29
        ]
      ]
    },
    "abstract": "The rapid advancement of Large Language Models (LLMs) presents a significant challenge to academic integrity within computing education. As educators seek reliable detection methods, this paper evaluates the capacity of three prominent LLMs (GPT-4, Claude, and Gemini) to identify AI-generated text in computing-specific contexts. We test their performance under both standard and 'deceptive' prompt conditions, where the models were instructed to evade detection. Our findings reveal a significant instability: while default AI-generated text was easily identified, all models struggled to correctly classify human-written work (with error rates up to 32%). Furthermore, the models were highly susceptible to deceptive prompts, with Gemini's output completely fooling GPT-4. Given that simple prompt alterations significantly degrade detection efficacy, our results demonstrate that these LLMs are currently too unreliable for making high-stakes academic misconduct judgments.",
    "URL": "http://arxiv.org/abs/2512.23587v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.23601",
    "type": "report",
    "title": "Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation",
    "author": [
      {
        "family": "Nguyen",
        "given": "Manh Hung"
      },
      {
        "family": "Singla",
        "given": "Adish"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          29
        ]
      ]
    },
    "abstract": "Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.",
    "URL": "http://arxiv.org/abs/2512.23601v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.23633",
    "type": "report",
    "title": "AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms",
    "author": [
      {
        "family": "Team",
        "given": "LearnLM"
      },
      {
        "literal": "Eedi"
      },
      {
        "literal": ":"
      },
      {
        "family": "Wang",
        "given": "Albert"
      },
      {
        "family": "Rysbek",
        "given": "Aliya"
      },
      {
        "family": "Huber",
        "given": "Andrea"
      },
      {
        "family": "Nambiar",
        "given": "Anjali"
      },
      {
        "family": "Kenolty",
        "given": "Anna"
      },
      {
        "family": "Caulfield",
        "given": "Ben"
      },
      {
        "family": "Lilley-Draper",
        "given": "Beth"
      },
      {
        "family": "Groot",
        "given": "Bibi"
      },
      {
        "family": "Veprek",
        "given": "Brian"
      },
      {
        "family": "Burdett",
        "given": "Chelsea"
      },
      {
        "family": "Willis",
        "given": "Claire"
      },
      {
        "family": "Barton",
        "given": "Craig"
      },
      {
        "family": "Smith",
        "given": "Digory"
      },
      {
        "family": "Mu",
        "given": "George"
      },
      {
        "family": "Walters",
        "given": "Harriet"
      },
      {
        "family": "Jurenka",
        "given": "Irina"
      },
      {
        "family": "Hulls",
        "given": "Iris"
      },
      {
        "family": "Stalley-Moores",
        "given": "James"
      },
      {
        "family": "Caton",
        "given": "Jonathan"
      },
      {
        "family": "Wilkowski",
        "given": "Julia"
      },
      {
        "family": "Alarakyia",
        "given": "Kaiz"
      },
      {
        "family": "McKee",
        "given": "Kevin R."
      },
      {
        "family": "McCafferty",
        "given": "Liam"
      },
      {
        "family": "Dalton",
        "given": "Lucy"
      },
      {
        "family": "Kunesch",
        "given": "Markus"
      },
      {
        "family": "Malubay",
        "given": "Pauline"
      },
      {
        "family": "Kidson",
        "given": "Rachel"
      },
      {
        "family": "Wells",
        "given": "Rich"
      },
      {
        "family": "Wheeler",
        "given": "Sam"
      },
      {
        "family": "Wiltberger",
        "given": "Sara"
      },
      {
        "family": "Mohamed",
        "given": "Shakir"
      },
      {
        "family": "Woodhead",
        "given": "Simon"
      },
      {
        "family": "Braz√£o",
        "given": "Vasco"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          29
        ]
      ]
    },
    "abstract": "One-to-one tutoring is widely considered the gold standard for personalized education, yet it remains prohibitively expensive to scale. To evaluate whether generative AI might help expand access to this resource, we conducted an exploratory randomized controlled trial (RCT) with $N = 165$ students across five UK secondary schools. We integrated LearnLM -- a generative AI model fine-tuned for pedagogy -- into chat-based tutoring sessions on the Eedi mathematics platform. In the RCT, expert tutors directly supervised LearnLM, with the remit to revise each message it drafted until they would be satisfied sending it themselves. LearnLM proved to be a reliable source of pedagogical instruction, with supervising tutors approving 76.4% of its drafted messages making zero or minimal edits (i.e., changing only one or two characters). This translated into effective tutoring support: students guided by LearnLM performed at least as well as students chatting with human tutors on each learning outcome we measured. In fact, students who received support from LearnLM were 5.5 percentage points more likely to solve novel problems on subsequent topics (with a success rate of 66.2%) than those who received tutoring from human tutors alone (rate of 60.7%). In interviews, tutors highlighted LearnLM's strength at drafting Socratic questions that encouraged deeper reflection from students, with multiple tutors even reporting that they learned new pedagogical practices from the model. Overall, our results suggest that pedagogically fine-tuned AI tutoring systems may play a promising role in delivering effective, individualized learning support at scale.",
    "URL": "http://arxiv.org/abs/2512.23633v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.23982",
    "type": "report",
    "title": "Coding With AI: From a Reflection on Industrial Practices to Future Computer Science and Software Engineering Education",
    "author": [
      {
        "family": "Chang",
        "given": "Hung-Fu"
      },
      {
        "family": "Shirazi",
        "given": "MohammadShokrolah"
      },
      {
        "family": "Cao",
        "given": "Lizhou"
      },
      {
        "family": "Mobasser",
        "given": "Supannika Koolmanojwong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          30
        ]
      ]
    },
    "abstract": "Recent advances in large language models (LLMs) have introduced new paradigms in software development, including vibe coding, AI-assisted coding, and agentic coding, fundamentally reshaping how software is designed, implemented, and maintained. Prior research has primarily examined AI-based coding at the individual level or in educational settings, leaving industrial practitioners' perspectives underexplored. This paper addresses this gap by investigating how LLM coding tools are used in professional practice, the associated concerns and risks, and the resulting transformations in development workflows, with particular attention to implications for computing education. We conducted a qualitative analysis of 57 curated YouTube videos published between late 2024 and 2025, capturing reflections and experiences shared by practitioners. Following a filtering and quality assessment process, the selected sources were analyzed to compare LLM-based and traditional programming, identify emerging risks, and characterize evolving workflows. Our findings reveal definitions of AI-based coding practices, notable productivity gains, and lowered barriers to entry. Practitioners also report a shift in development bottlenecks toward code review and concerns regarding code quality, maintainability, security vulnerabilities, ethical issues, erosion of foundational problem-solving skills, and insufficient preparation of entry-level engineers. Building on these insights, we discuss implications for computer science and software engineering education and argue for curricular shifts toward problem-solving, architectural thinking, code review, and early project-based learning that integrates LLM tools. This study offers an industry-grounded perspective on AI-based coding and provides guidance for aligning educational practices with rapidly evolving professional realities.",
    "URL": "http://arxiv.org/abs/2512.23982v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.24618",
    "type": "report",
    "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
    "author": [
      {
        "family": "Lu",
        "given": "Junru"
      },
      {
        "family": "Qin",
        "given": "Jiarui"
      },
      {
        "family": "Qiao",
        "given": "Lingfeng"
      },
      {
        "family": "Li",
        "given": "Yinghui"
      },
      {
        "family": "Dai",
        "given": "Xinyi"
      },
      {
        "family": "Ke",
        "given": "Bo"
      },
      {
        "family": "He",
        "given": "Jianfeng"
      },
      {
        "family": "Qiao",
        "given": "Ruizhi"
      },
      {
        "family": "Yin",
        "given": "Di"
      },
      {
        "family": "Sun",
        "given": "Xing"
      },
      {
        "family": "Wu",
        "given": "Yunsheng"
      },
      {
        "family": "Liu",
        "given": "Yinsong"
      },
      {
        "family": "Liu",
        "given": "Shuangyin"
      },
      {
        "family": "Tang",
        "given": "Mingkong"
      },
      {
        "family": "Lin",
        "given": "Haodong"
      },
      {
        "family": "Kuang",
        "given": "Jiayi"
      },
      {
        "family": "Meng",
        "given": "Fanxu"
      },
      {
        "family": "Tang",
        "given": "Xiaojuan"
      },
      {
        "family": "Xi",
        "given": "Yunjia"
      },
      {
        "family": "Huang",
        "given": "Junjie"
      },
      {
        "family": "Yang",
        "given": "Haotong"
      },
      {
        "family": "Shen",
        "given": "Zhenyi"
      },
      {
        "family": "Li",
        "given": "Yangning"
      },
      {
        "family": "Zhang",
        "given": "Qianwen"
      },
      {
        "family": "Yu",
        "given": "Yifei"
      },
      {
        "family": "An",
        "given": "Siyu"
      },
      {
        "family": "Dong",
        "given": "Junnan"
      },
      {
        "family": "Wang",
        "given": "Qiufeng"
      },
      {
        "family": "Wang",
        "given": "Jie"
      },
      {
        "family": "Chen",
        "given": "Keyu"
      },
      {
        "family": "Wen",
        "given": "Wei"
      },
      {
        "family": "Guo",
        "given": "Taian"
      },
      {
        "family": "Shen",
        "given": "Zhifeng"
      },
      {
        "family": "Yu",
        "given": "Daohai"
      },
      {
        "family": "Li",
        "given": "Jiahao"
      },
      {
        "family": "Li",
        "given": "Ke"
      },
      {
        "family": "Li",
        "given": "Zongyi"
      },
      {
        "family": "Tan",
        "given": "Xiaoyu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          31
        ]
      ]
    },
    "abstract": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.",
    "URL": "http://arxiv.org/abs/2512.24618v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2512.24969",
    "type": "report",
    "title": "Large language models and the entropy of English",
    "author": [
      {
        "family": "Scheibner",
        "given": "Colin"
      },
      {
        "family": "Smith",
        "given": "Lindsay M."
      },
      {
        "family": "Bialek",
        "given": "William"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          31
        ]
      ]
    },
    "abstract": "We use large language models (LLMs) to uncover long-ranged structure in English texts from a variety of sources. The conditional entropy or code length in many cases continues to decrease with context length at least to $N\\sim 10^4$ characters, implying that there are direct dependencies or interactions across these distances. A corollary is that there are small but significant correlations between characters at these separations, as we show from the data independent of models. The distribution of code lengths reveals an emergent certainty about an increasing fraction of characters at large $N$. Over the course of model training, we observe different dynamics at long and short context lengths, suggesting that long-ranged structure is learned only gradually. Our results constrain efforts to build statistical physics models of LLMs or language itself.",
    "URL": "http://arxiv.org/abs/2512.24969v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.00493",
    "type": "report",
    "title": "Measuring University Students Satisfaction with Traditional Search Engines and Generative AI Tools as Information Sources",
    "author": [
      {
        "family": "Lund",
        "given": "Brady D."
      },
      {
        "family": "Warren",
        "given": "Scott J."
      },
      {
        "family": "Teel",
        "given": "Zoe A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          1
        ]
      ]
    },
    "abstract": "This study examines university students levels of satisfaction with generative artificial intelligence (AI) tools and traditional search engines as academic information sources. An electronic survey was distributed to students at U.S. universities in late fall 2025, with 236 valid responses received. In addition to demographic information about respondents, frequency of use and levels of satisfaction with both generative AI and traditional search engines were measured. Principal components analysis identified distinct constructs of satisfaction for each information source, while k-means cluster analysis revealed two primary student groups: those highly satisfied with search engines but dissatisfied with AI, and those moderately to highly satisfied with both. Regression analysis showed that frequency of use strongly predicts satisfaction, with international and undergraduate students reporting significantly higher satisfaction with AI tools than domestic and graduate students. Students generally expressed higher levels of satisfaction with traditional search engines over generative AI tools. Those who did prefer AI tools appear to see them more as a complementary source of information rather than a replacement for other sources. These findings stress evolving patterns of student information seeking and use behavior and offer meaningful insights for evaluating and integrating both traditional and AI-driven information sources within higher education.",
    "URL": "http://arxiv.org/abs/2601.00493v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.00497",
    "type": "report",
    "title": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications",
    "author": [
      {
        "family": "Sorokin",
        "given": "Lev"
      },
      {
        "family": "Vasilev",
        "given": "Ivan"
      },
      {
        "family": "Friedl",
        "given": "Ken E."
      },
      {
        "family": "Stocco",
        "given": "Andrea"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          1
        ]
      ]
    },
    "abstract": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.",
    "URL": "http://arxiv.org/abs/2601.00497v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.00570",
    "type": "report",
    "title": "User Perceptions of an LLM-Based Chatbot for Cognitive Reappraisal of Stress: Feasibility Study",
    "author": [
      {
        "family": "Bhattacharjee",
        "given": "Ananya"
      },
      {
        "family": "Suh",
        "given": "Jina"
      },
      {
        "family": "Chandra",
        "given": "Mohit"
      },
      {
        "family": "Hernandez",
        "given": "Javier"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          2
        ]
      ]
    },
    "abstract": "Cognitive reappraisal is a well-studied emotion regulation strategy that helps individuals reinterpret stressful situations to reduce their impact. Many digital mental health tools struggle to support this process because rigid scripts fail to accommodate how users naturally describe stressors. This study examined the feasibility of an LLM-based single-session intervention (SSI) for workplace stress reappraisal. We assessed short-term changes in stress-related outcomes and examined design tensions during use. We conducted a feasibility study with 100 employees at a large technology company who completed a structured cognitive reappraisal session delivered by a GPT-4o-based chatbot. Pre-post measures included perceived stress intensity, stress mindset, perceived demand, and perceived resources. These outcomes were analyzed using paired Wilcoxon signed-rank tests with correction for multiple comparisons. We also examined sentiment and stress trajectories across conversation quartiles using two RoBERTa-based classifiers and an LLM-based stress rater. Open-ended responses were analyzed using thematic analysis. Results showed significant reductions in perceived stress intensity and significant improvements in stress mindset. Changes in perceived resources and perceived demand trended in expected directions but were not statistically significant. Automated analyses indicated consistent declines in negative sentiment and stress over the course of the interaction. Qualitative findings suggested that participants valued the structured prompts for organizing thoughts, gaining perspective, and feeling acknowledged. Participants also reported tensions around scriptedness, preferred interaction length, and reactions to AI-driven empathy. These findings highlight both the promise and the design constraints of integrating LLMs into DMH interventions for workplace settings.",
    "URL": "http://arxiv.org/abs/2601.00570v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.00730",
    "type": "report",
    "title": "Grading Handwritten Engineering Exams with Multimodal Large Language Models",
    "author": [
      {
        "family": "Per≈°",
        "given": "Janez"
      },
      {
        "family": "Muhoviƒç",
        "given": "Jon"
      },
      {
        "family": "Ko≈°ir",
        "given": "Andrej"
      },
      {
        "family": "Murovec",
        "given": "Bo≈°tjan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          2
        ]
      ]
    },
    "abstract": "Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\\approx$17% at $D_{\\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.",
    "URL": "http://arxiv.org/abs/2601.00730v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.00943",
    "type": "report",
    "title": "PhyEduVideo: A Benchmark for Evaluating Text-to-Video Models for Physics Education",
    "author": [
      {
        "family": "M",
        "given": "Megha Mariam K."
      },
      {
        "family": "Arun",
        "given": "Aditya"
      },
      {
        "family": "Laskar",
        "given": "Zakaria"
      },
      {
        "family": "Jawahar",
        "given": "C. V."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          2
        ]
      ]
    },
    "abstract": "Generative AI models, particularly Text-to-Video (T2V) systems, offer a promising avenue for transforming science education by automating the creation of engaging and intuitive visual explanations. In this work, we take a first step toward evaluating their potential in physics education by introducing a dedicated benchmark for explanatory video generation. The benchmark is designed to assess how well T2V models can convey core physics concepts through visual illustrations. Each physics concept in our benchmark is decomposed into granular teaching points, with each point accompanied by a carefully crafted prompt intended for visual explanation of the teaching point. T2V models are evaluated on their ability to generate accurate videos in response to these prompts. Our aim is to systematically explore the feasibility of using T2V models to generate high-quality, curriculum-aligned educational content-paving the way toward scalable, accessible, and personalized learning experiences powered by AI. Our evaluation reveals that current models produce visually coherent videos with smooth motion and minimal flickering, yet their conceptual accuracy is less reliable. Performance in areas such as mechanics, fluids, and optics is encouraging, but models struggle with electromagnetism and thermodynamics, where abstract interactions are harder to depict. These findings underscore the gap between visual quality and conceptual correctness in educational video generation. We hope this benchmark helps the community close that gap and move toward T2V systems that can deliver accurate, curriculum-aligned physics content at scale. The benchmark and accompanying codebase are publicly available at https://github.com/meghamariamkm/PhyEduVideo.",
    "URL": "http://arxiv.org/abs/2601.00943v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.01196",
    "type": "report",
    "title": "EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners",
    "author": [
      {
        "family": "Lu",
        "given": "Shenqi"
      },
      {
        "family": "Zhang",
        "given": "Liangwei"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          3
        ]
      ]
    },
    "abstract": "In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an important challenge in the rapid development of human-robot interaction and intelligent automation industries. This challenge hinders intuitive human control over complex robotic systems, limiting their educational and practical accessibility. To address this, we present the EduSim-LLM, an educational platform that integrates LLMs with robot simulation and constructs a language-drive control model that translates natural language instructions into executable robot behavior sequences in CoppeliaSim. We design two human-robot interaction models: direct control and autonomous control, conduct systematic simulations based on multiple language models, and evaluate multi-robot collaboration, motion planning, and manipulation capabilities. Experiential results show that LLMs can reliably convert natural language into structured robot actions; after applying prompt-engineering templates instruction-parsing accuracy improves significantly; as task complexity increases, overall accuracy rate exceeds 88.9% in the highest complexity tests.",
    "URL": "http://arxiv.org/abs/2601.01196v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.01366",
    "type": "report",
    "title": "KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models",
    "author": [
      {
        "family": "Liu",
        "given": "Zixian"
      },
      {
        "family": "Liu",
        "given": "Sihao"
      },
      {
        "family": "Zhao",
        "given": "Yuqi"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          4
        ]
      ]
    },
    "abstract": "With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.",
    "URL": "http://arxiv.org/abs/2601.01366v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.01684",
    "type": "report",
    "title": "LACONIC: Dense-Level Effectiveness for Scalable Sparse Retrieval via a Two-Phase Training Curriculum",
    "author": [
      {
        "family": "Xu",
        "given": "Zhichao"
      },
      {
        "family": "Zhuang",
        "given": "Shengyao"
      },
      {
        "family": "Zhang",
        "given": "Crystina"
      },
      {
        "family": "Ma",
        "given": "Xueguang"
      },
      {
        "family": "Tian",
        "given": "Yijun"
      },
      {
        "family": "Mehta",
        "given": "Maitrey"
      },
      {
        "family": "Lin",
        "given": "Jimmy"
      },
      {
        "family": "Srikumar",
        "given": "Vivek"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          4
        ]
      ]
    },
    "abstract": "While dense retrieval models have become the standard for state-of-the-art information retrieval, their deployment is often constrained by high memory requirements and reliance on GPU accelerators for vector similarity search. Learned sparse retrieval offers a compelling alternative by enabling efficient search via inverted indices, yet it has historically received less attention than dense approaches. In this report, we introduce LACONIC, a family of learned sparse retrievers based on the Llama-3 architecture (1B, 3B, and 8B). We propose a streamlined two-phase training curriculum consisting of (1) weakly supervised pre-finetuning to adapt causal LLMs for bidirectional contextualization and (2) high-signal finetuning using curated hard negatives. Our results demonstrate that LACONIC effectively bridges the performance gap with dense models: the 8B variant achieves a state-of-the-art 60.2 nDCG on the MTEB Retrieval benchmark, ranking 15th on the leaderboard as of January 1, 2026, while utilizing 71\\% less index memory than an equivalent dense model. By delivering high retrieval effectiveness on commodity CPU hardware with a fraction of the compute budget required by competing models, LACONIC provides a scalable and efficient solution for real-world search applications.",
    "URL": "http://arxiv.org/abs/2601.01684v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.02404",
    "type": "report",
    "title": "PCEval: A Benchmark for Evaluating Physical Computing Capabilities of Large Language Models",
    "author": [
      {
        "family": "Song",
        "given": "Inpyo"
      },
      {
        "family": "Jeon",
        "given": "Eunji"
      },
      {
        "family": "Lee",
        "given": "Jangwon"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          31
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including software development, education, and technical assistance. Among these, software development is one of the key areas where LLMs are increasingly adopted. However, when hardware constraints are considered-for instance, in physical computing, where software must interact with and control physical hardware -their effectiveness has not been fully explored. To address this gap, we introduce \\textsc{PCEval} (Physical Computing Evaluation), the first benchmark in physical computing that enables a fully automatic evaluation of the capabilities of LLM in both the logical and physical aspects of the projects, without requiring human assessment. Our evaluation framework assesses LLMs in generating circuits and producing compatible code across varying levels of project complexity. Through comprehensive testing of 13 leading models, \\textsc{PCEval} provides the first reproducible and automatically validated empirical assessment of LLMs' ability to reason about fundamental hardware implementation constraints within a simulation environment. Our findings reveal that while LLMs perform well in code generation and logical circuit design, they struggle significantly with physical breadboard layout creation, particularly in managing proper pin connections and avoiding circuit errors. \\textsc{PCEval} advances our understanding of AI assistance in hardware-dependent computing environments and establishes a foundation for developing more effective tools to support physical computing education.",
    "URL": "http://arxiv.org/abs/2601.02404v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.02554",
    "type": "report",
    "title": "AI-exposed jobs deteriorated before ChatGPT",
    "author": [
      {
        "family": "Frank",
        "given": "Morgan R."
      },
      {
        "family": "Sabet",
        "given": "Alireza Javadian"
      },
      {
        "family": "Simon",
        "given": "Lisa"
      },
      {
        "family": "Bana",
        "given": "Sarah H."
      },
      {
        "family": "Yu",
        "given": "Renzhe"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          5
        ]
      ]
    },
    "abstract": "Public debate links worsening job prospects for AI-exposed occupations to the release of ChatGPT in late 2022. Using monthly U.S. unemployment insurance records, we measure occupation- and location-specific unemployment risk and find that risk rose in AI-exposed occupations beginning in early 2022, months before ChatGPT. Analyzing millions of LinkedIn profiles, we show that graduate cohorts from 2021 onward entered AI-exposed jobs at lower rates than earlier cohorts, with gaps opening before late 2022. Finally, from millions of university syllabi, we find that graduates taking more AI-exposed curricula had higher first-job pay and shorter job searches after ChatGPT. Together, these results point to forces pre-dating generative AI and to the ongoing value of LLM-relevant education.",
    "URL": "http://arxiv.org/abs/2601.02554v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.02902",
    "type": "report",
    "title": "Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning",
    "author": [
      {
        "family": "Zhang",
        "given": "Xinglang"
      },
      {
        "family": "Zhang",
        "given": "Yunyao"
      },
      {
        "family": "Chen",
        "given": "ZeLiang"
      },
      {
        "family": "Yu",
        "given": "Junqing"
      },
      {
        "family": "Yang",
        "given": "Wei"
      },
      {
        "family": "Song",
        "given": "Zikai"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          6
        ]
      ]
    },
    "abstract": "Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.",
    "URL": "http://arxiv.org/abs/2601.02902v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.03285",
    "type": "report",
    "title": "Feedback Indices to Evaluate LLM Responses to Rebuttals for Multiple Choice Type Questions",
    "author": [
      {
        "family": "Dunlap",
        "given": "Justin C."
      },
      {
        "family": "Parent",
        "given": "Anne-Simone"
      },
      {
        "family": "Widenhorn",
        "given": "Ralf"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          2
        ]
      ]
    },
    "abstract": "We present a systematic framework of indices designed to characterize Large Language Model (LLM) responses when challenged with rebuttals during a chat. Assessing how LLMs respond to user dissent is crucial for understanding their reliability and behavior patterns, yet the complexity of human-LLM interactions makes systematic evaluation challenging. Our approach employs a fictitious-response rebuttal method that quantifies LLM behavior when presented with multiple-choice questions followed by deliberate challenges to their fictitious previous response. The indices are specifically designed to detect and measure what could be characterized as sycophantic behavior (excessive agreement with user challenges) or stubborn responses (rigid adherence to the fictitious response in the chat history) from LLMs. These metrics allow investigation of the relationships between sycophancy, stubbornness, and the model's actual mastery of the subject matter. We demonstrate the utility of these indices using two physics problems as test scenarios with various OpenAI models. The framework is intentionally generalizable to any multiple-choice format question, including on topics without universally accepted correct answers. Our results reveal measurable differences across OpenAI model generations, with trends indicating that newer models and those employing greater \"Reasoning Effort\" exhibit reduced sycophantic behavior. The FR pairing method combined with our proposed indices provides a practical, adaptable toolkit for systematically comparing LLM dialogue behaviors across different models and contexts.",
    "URL": "http://arxiv.org/abs/2601.03285v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.03432",
    "type": "report",
    "title": "CodeEval: A pedagogical approach for targeted evaluation of code-trained Large Language Models",
    "author": [
      {
        "family": "Brahman",
        "given": "Danny"
      },
      {
        "family": "Mahoor",
        "given": "Mohammad"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          6
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) are predominantly assessed based on their common sense reasoning, language comprehension, and logical reasoning abilities. While models trained in specialized domains like mathematics or coding have demonstrated remarkable advancements in logical reasoning, there remains a significant gap in evaluating their code generation capabilities. Existing benchmark datasets fall short in pinpointing specific strengths and weaknesses, impeding targeted enhancements in models' reasoning abilities to synthesize code. To bridge this gap, our paper introduces an innovative, pedagogical benchmarking method that mirrors the evaluation processes encountered in academic programming courses. We introduce CodeEval, a multi-dimensional benchmark dataset designed to rigorously evaluate LLMs across 24 distinct aspects of Python programming. The dataset covers three proficiency levels - beginner, intermediate, and advanced - and includes both class-based and function-based problem types with detailed problem specifications and comprehensive test suites. To facilitate widespread adoption, we also developed RunCodeEval, an open-source execution framework that provides researchers with a ready-to-use evaluation pipeline for CodeEval. RunCodeEval handles test execution, context setup, and metrics generation, enabling researchers to quickly obtain detailed insights into model strengths and weaknesses across complexity levels, problem types, and programming categories. This combination enables targeted evaluation and guides improvements in LLMs' programming proficiencies.",
    "URL": "http://arxiv.org/abs/2601.03432v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.03458",
    "type": "report",
    "title": "Automated Feedback Generation for Undergraduate Mathematics: Development and Evaluation of an AI Teaching Assistant",
    "author": [
      {
        "family": "Gohr",
        "given": "Aron"
      },
      {
        "family": "Lawn",
        "given": "Marie-Amelie"
      },
      {
        "family": "Gao",
        "given": "Kevin"
      },
      {
        "family": "Serjeant",
        "given": "Inigo"
      },
      {
        "family": "Heslip",
        "given": "Stephen"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          6
        ]
      ]
    },
    "abstract": "Intelligent tutoring systems have long enabled automated immediate feedback on student work when it is presented in a tightly structured format and when problems are very constrained, but reliably assessing free-form mathematical reasoning remains challenging.\n  We present a system that processes free-form natural language input, handles a wide range of edge cases, and comments competently not only on the technical correctness of submitted proofs, but also on style and presentation issues. We discuss the advantages and disadvantages of various approaches to the evaluation of such a system, and show that by the metrics we evaluate, the quality of the feedback generated is comparable to that produced by human experts when assessing early undergraduate homework. We stress-test our system with a small set of more advanced and unusual questions, and report both significant gaps and encouraging successes in that more challenging setting.\n  Our system uses large language models in a modular workflow. The workflow configuration is human-readable and editable without programming knowledge, and allows some intermediate steps to be precomputed or injected by the instructor.\n  A version of our tool is deployed on the Imperial mathematics homework platform Lambdafeedback. We report also on the integration of our tool into this platform.",
    "URL": "http://arxiv.org/abs/2601.03458v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.03645",
    "type": "report",
    "title": "LLM-MC-Affect: LLM-Based Monte Carlo Modeling of Affective Trajectories and Latent Ambiguity for Interpersonal Dynamic Insight",
    "author": [
      {
        "family": "Lin",
        "given": "Yu-Zheng"
      },
      {
        "family": "Shih",
        "given": "Bono Po-Jen"
      },
      {
        "family": "Encinas",
        "given": "John Paul Martin"
      },
      {
        "family": "Achom",
        "given": "Elizabeth Victoria Abraham"
      },
      {
        "family": "Patel",
        "given": "Karan Himanshu"
      },
      {
        "family": "Pacheco",
        "given": "Jesus Horacio"
      },
      {
        "family": "Shao",
        "given": "Sicong"
      },
      {
        "family": "Dass",
        "given": "Jyotikrishna"
      },
      {
        "family": "Salehi",
        "given": "Soheil"
      },
      {
        "family": "Satam",
        "given": "Pratik"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "Emotional coordination is a core property of human interaction that shapes how relational meaning is constructed in real time. While text-based affect inference has become increasingly feasible, prior approaches often treat sentiment as a deterministic point estimate for individual speakers, failing to capture the inherent subjectivity, latent ambiguity, and sequential coupling found in mutual exchanges. We introduce LLM-MC-Affect, a probabilistic framework that characterizes emotion not as a static label, but as a continuous latent probability distribution defined over an affective space. By leveraging stochastic LLM decoding and Monte Carlo estimation, the methodology approximates these distributions to derive high-fidelity sentiment trajectories that explicitly quantify both central affective tendencies and perceptual ambiguity. These trajectories enable a structured analysis of interpersonal coupling through sequential cross-correlation and slope-based indicators, identifying leading or lagging influences between interlocutors. To validate the interpretive capacity of this approach, we utilize teacher-student instructional dialogues as a representative case study, where our quantitative indicators successfully distill high-level interaction insights such as effective scaffolding. This work establishes a scalable and deployable pathway for understanding interpersonal dynamics, offering a generalizable solution that extends beyond education to broader social and behavioral research.",
    "URL": "http://arxiv.org/abs/2601.03645v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.03725",
    "type": "report",
    "title": "EDCO: Dynamic Curriculum Orchestration for Domain-specific Large Language Model Fine-tuning",
    "author": [
      {
        "family": "Pang",
        "given": "Jing-Cheng"
      },
      {
        "family": "Sun",
        "given": "Liu"
      },
      {
        "family": "Zhou",
        "given": "Chang"
      },
      {
        "family": "Tang",
        "given": "Xian"
      },
      {
        "family": "Ma",
        "given": "Haichuan"
      },
      {
        "family": "Jiang",
        "given": "Kun"
      },
      {
        "family": "Wang",
        "given": "Jianlong"
      },
      {
        "family": "Zhang",
        "given": "Kai"
      },
      {
        "family": "Wu",
        "given": "Sijie"
      },
      {
        "family": "Cai",
        "given": "Haoran"
      },
      {
        "family": "Wu",
        "given": "Chenwei"
      },
      {
        "family": "Li",
        "given": "Xubin"
      },
      {
        "family": "Chen",
        "given": "Xin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "Domain-specific large language models (LLMs), typically developed by fine-tuning a pre-trained general-purpose LLM on specialized datasets, represent a significant advancement in applied AI. A common strategy in LLM fine-tuning is curriculum learning, which pre-orders training samples based on metrics like difficulty to improve learning efficiency compared to a random sampling strategy. However, most existing methods for LLM fine-tuning rely on a static curriculum, designed prior to training, which lacks adaptability to the model's evolving needs during fine-tuning. To address this, we propose EDCO, a novel framework based on two key concepts: inference entropy and dynamic curriculum orchestration. Inspired by recent findings that maintaining high answer entropy benefits long-term reasoning gains, EDCO prioritizes samples with high inference entropy in a continuously adapted curriculum. EDCO integrates three core components: an efficient entropy estimator that uses prefix tokens to approximate full-sequence entropy, an entropy-based curriculum generator that selects data points with the highest inference entropy, and an LLM trainer that optimizes the model on the selected curriculum. Comprehensive experiments in communication, medicine and law domains, EDCO outperforms traditional curriculum strategies for fine-tuning Qwen3-4B and Llama3.2-3B models under supervised and reinforcement learning settings. Furthermore, the proposed efficient entropy estimation reduces computational time by 83.5% while maintaining high accuracy.",
    "URL": "http://arxiv.org/abs/2601.03725v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.03858",
    "type": "report",
    "title": "What Does Loss Optimization Actually Teach, If Anything? Knowledge Dynamics in Continual Pre-training of LLMs",
    "author": [
      {
        "family": "Mousavi",
        "given": "Seyed Mahed"
      },
      {
        "family": "Alghisi",
        "given": "Simone"
      },
      {
        "family": "Riccardi",
        "given": "Giuseppe"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "Continual Pre-Training (CPT) is widely used for acquiring and updating factual knowledge in LLMs. This practice treats loss as a proxy for knowledge learning, while offering no grounding into how it changes during training. We study CPT as a knowledge learning process rather than a solely optimization problem. We construct a controlled, distribution-matched benchmark of factual documents and interleave diagnostic probes directly into the CPT loop, enabling epoch-level measurement of knowledge acquisition dynamics and changes in Out-Of-Domain (OOD) general skills (e.g., math). We further analyze how CPT reshapes knowledge circuits during training. Across three instruction-tuned LLMs and multiple CPT strategies, optimization and learning systematically diverge as loss decreases monotonically while factual learning is unstable and non-monotonic. Acquired facts are rarely consolidated, learning is strongly conditioned on prior exposure, and OOD performance degrades from early epochs. Circuit analysis reveals rapid reconfiguration of knowledge pathways across epochs, providing an explanation for narrow acquisition windows and systematic forgetting. These results show that loss optimization is misaligned with learning progress in CPT and motivate evaluation of stopping criteria based on task-level learning dynamics.",
    "URL": "http://arxiv.org/abs/2601.03858v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.03880",
    "type": "report",
    "title": "Women Worry, Men Adopt: How Gendered Perceptions Shape the Use of Generative AI",
    "author": [
      {
        "family": "Stephany",
        "given": "Fabian"
      },
      {
        "family": "Duszynski",
        "given": "Jedrzej"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "Generative artificial intelligence (GenAI) is diffusing rapidly, yet its adoption is strikingly unequal. Using nationally representative UK survey data from 2023 to 2024, we show that women adopt GenAI substantially less often than men because they perceive its societal risks differently. We construct a composite index capturing concerns about mental health, privacy, climate impact, and labor market disruption. This index explains between 9 and 18 percent of the variation in GenAI adoption and ranks among the strongest predictors for women across all age groups, surpassing digital literacy and education for young women. Intersectional analyses show that the largest disparities arise among younger, digitally fluent individuals with high societal risk concerns, where gender gaps in personal use exceed 45 percentage points. Using a synthetic twin panel design, we show that increased optimism about AI's societal impact raises GenAI use among young women from 13 percent to 33 percent, substantially narrowing the gender divide. These findings indicate that gendered perceptions of AI's social and ethical consequences, rather than access or capability, are the primary drivers of unequal GenAI adoption, with implications for productivity, skill formation, and economic inequality in an AI enabled economy.",
    "URL": "http://arxiv.org/abs/2601.03880v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04025",
    "type": "report",
    "title": "Simulated Students in Tutoring Dialogues: Substance or Illusion?",
    "author": [
      {
        "family": "Scarlatos",
        "given": "Alexander"
      },
      {
        "family": "Lee",
        "given": "Jaewook"
      },
      {
        "family": "Woodhead",
        "given": "Simon"
      },
      {
        "family": "Lan",
        "given": "Andrew"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "Advances in large language models (LLMs) enable many new innovations in education. However, evaluating the effectiveness of new technology requires real students, which is time-consuming and hard to scale up. Therefore, many recent works on LLM-powered tutoring solutions have used simulated students for both training and evaluation, often via simple prompting. Surprisingly, little work has been done to ensure or even measure the quality of simulated students. In this work, we formally define the student simulation task, propose a set of evaluation metrics that span linguistic, behavioral, and cognitive aspects, and benchmark a wide range of student simulation methods on these metrics. We experiment on a real-world math tutoring dialogue dataset, where both automated and human evaluation results show that prompting strategies for student simulation perform poorly; supervised fine-tuning and preference optimization yield much better but still limited performance, motivating future work on this challenging task.",
    "URL": "http://arxiv.org/abs/2601.04025v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04219",
    "type": "report",
    "title": "AgentTutor: Empowering Personalized Learning with Multi-Turn Interactive Teaching in Intelligent Education Systems",
    "author": [
      {
        "family": "Liu",
        "given": "Yuxin"
      },
      {
        "family": "Song",
        "given": "Zeqing"
      },
      {
        "family": "Lou",
        "given": "Jiong"
      },
      {
        "family": "Wu",
        "given": "Chentao"
      },
      {
        "family": "Li",
        "given": "Jie"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          24
        ]
      ]
    },
    "abstract": "The rapid advancement of large-scale language models (LLMs) has shown their potential to transform intelligent education systems (IESs) through automated teaching and learning support applications. However, current IESs often rely on single-turn static question-answering, which fails to assess learners' cognitive levels, cannot adjust teaching strategies based on real-time feedback, and is limited to providing simple one-off responses. To address these issues, we introduce AgentTutor, a multi-turn interactive intelligent education system to empower personalized learning. It features an LLM-powered generative multi-agent system and a learner-specific personalized learning profile environment that dynamically optimizes and delivers teaching strategies based on learners' learning status, personalized goals, learning preferences, and multimodal study materials. It includes five key modules: curriculum decomposition, learner assessment, dynamic strategy, teaching reflection, and knowledge & experience memory. We conducted extensive experiments on multiple benchmark datasets, AgentTutor significantly enhances learners' performance while demonstrating strong effectiveness in multi-turn interactions and competitiveness in teaching quality among other baselines.",
    "URL": "http://arxiv.org/abs/2601.04219v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04225",
    "type": "report",
    "title": "Can Consumer Chatbots Reason? A Student-Led Field Experiment Embedded in an \"AI-for-All\" Undergraduate Course",
    "author": [
      {
        "family": "Shehu",
        "given": "Amarda"
      },
      {
        "family": "Ababu",
        "given": "Adonyas"
      },
      {
        "family": "Akbary",
        "given": "Asma"
      },
      {
        "family": "Allen",
        "given": "Griffin"
      },
      {
        "family": "Baig",
        "given": "Aroush"
      },
      {
        "family": "Battle",
        "given": "Tereana"
      },
      {
        "family": "Beall",
        "given": "Elias"
      },
      {
        "family": "Byrom",
        "given": "Christopher"
      },
      {
        "family": "Dean",
        "given": "Matt"
      },
      {
        "family": "Demarco",
        "given": "Kate"
      },
      {
        "family": "Douglass",
        "given": "Ethan"
      },
      {
        "family": "Granados",
        "given": "Luis"
      },
      {
        "family": "Hantush",
        "given": "Layla"
      },
      {
        "family": "Hay",
        "given": "Andy"
      },
      {
        "family": "Hay",
        "given": "Eleanor"
      },
      {
        "family": "Jackson",
        "given": "Caleb"
      },
      {
        "family": "Jang",
        "given": "Jaewon"
      },
      {
        "family": "Jones",
        "given": "Carter"
      },
      {
        "family": "Li",
        "given": "Quanyang"
      },
      {
        "family": "Lopez",
        "given": "Adrian"
      },
      {
        "family": "Massimo",
        "given": "Logan"
      },
      {
        "family": "McMullin",
        "given": "Garrett"
      },
      {
        "family": "Maldonado",
        "given": "Ariana Mendoza"
      },
      {
        "family": "Mirza",
        "given": "Eman"
      },
      {
        "family": "Muddasar",
        "given": "Hadiya"
      },
      {
        "family": "Nuwayhid",
        "given": "Sara"
      },
      {
        "family": "Pak",
        "given": "Brandon"
      },
      {
        "family": "Petty",
        "given": "Ashley"
      },
      {
        "family": "Rancourt",
        "given": "Dryden"
      },
      {
        "family": "Rodriguez",
        "given": "Lily"
      },
      {
        "family": "Rogers",
        "given": "Corbin"
      },
      {
        "family": "Schiek",
        "given": "Jacob"
      },
      {
        "family": "Seok",
        "given": "Taeseo"
      },
      {
        "family": "Sethi",
        "given": "Aarav"
      },
      {
        "family": "Vitela",
        "given": "Giovanni"
      },
      {
        "family": "Williams",
        "given": "Winston"
      },
      {
        "family": "Yetukuri",
        "given": "Jagan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          28
        ]
      ]
    },
    "abstract": "Claims about whether large language model (LLM) chatbots \"reason\" are typically debated using curated benchmarks and laboratory-style evaluation protocols. This paper offers a complementary perspective: a student-led field experiment embedded as a midterm project in UNIV 182 (AI4All) at George Mason University, a Mason Core course designed for undergraduates across disciplines with no expected prior STEM exposure. Student teams designed their own reasoning tasks, ran them on widely used consumer chatbots representative of current capabilities, and evaluated both (i) answer correctness and (ii) the validity of the chatbot's stated reasoning (for example, cases where an answer is correct but the explanation is not, or vice versa). Across eight teams that reported standardized scores, students contributed 80 original reasoning prompts spanning six categories: pattern completion, transformation rules, spatial/visual reasoning, quantitative reasoning, relational/logic reasoning, and analogical reasoning. These prompts yielded 320 model responses plus follow-up explanations. Aggregating team-level results, OpenAI GPT-5 and Claude 4.5 achieved the highest mean answer accuracy (86.2% and 83.8%), followed by Grok 4 (82.5%) and Perplexity (73.1%); explanation validity showed a similar ordering (81.2%, 80.0%, 77.5%, 66.2%). Qualitatively, teams converged on a consistent error signature: strong performance on short, structured math and pattern items but reduced reliability on spatial/visual reasoning and multi-step transformations, with frequent \"sound right but reason wrong\" explanations. The assignment's primary contribution is pedagogical: it operationalizes AI literacy as experimental practice (prompt design, measurement, rater disagreement, and interpretability/grounding) while producing a reusable, student-generated corpus of reasoning probes grounded in authentic end-user interaction.",
    "URL": "http://arxiv.org/abs/2601.04225v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04336",
    "type": "report",
    "title": "Pilot Study on Student Public Opinion Regarding GAI",
    "author": [
      {
        "family": "Lamberti",
        "given": "William Franz"
      },
      {
        "family": "Kim",
        "given": "Sunbin"
      },
      {
        "family": "Lawrence",
        "given": "Samantha Rose"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "The emergence of generative AI (GAI) has sparked diverse opinions regarding its appropriate use across various domains, including education. This pilot study investigates university students' perceptions of GAI in higher education classrooms, aiming to lay the groundwork for understanding these attitudes. With a participation rate of approximately 4.4%, the study highlights the challenges of engaging students in GAI-related research and underscores the need for larger sample sizes in future studies. By gaining insights into student perspectives, instructors can better prepare to integrate discussions of GAI into their classrooms, fostering informed and critical engagement with this transformative technology.",
    "URL": "http://arxiv.org/abs/2601.04336v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04574",
    "type": "report",
    "title": "FeedEval: Pedagogically Aligned Evaluation of LLM-Generated Essay Feedback",
    "author": [
      {
        "family": "Chu",
        "given": "Seongyeub"
      },
      {
        "family": "Kim",
        "given": "Jongwoo"
      },
      {
        "family": "Yi",
        "given": "Munyong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          8
        ]
      ]
    },
    "abstract": "Going beyond the prediction of numerical scores, recent research in automated essay scoring has increasingly emphasized the generation of high-quality feedback that provides justification and actionable guidance. To mitigate the high cost of expert annotation, prior work has commonly relied on LLM-generated feedback to train essay assessment models. However, such feedback is often incorporated without explicit quality validation, resulting in the propagation of noise in downstream applications. To address this limitation, we propose FeedEval, an LLM-based framework for evaluating LLM-generated essay feedback along three pedagogically grounded dimensions: specificity, helpfulness, and validity. FeedEval employs dimension-specialized LLM evaluators trained on datasets curated in this study to assess multiple feedback candidates and select high-quality feedback for downstream use. Experiments on the ASAP++ benchmark show that FeedEval closely aligns with human expert judgments and that essay scoring models trained with FeedEval-filtered high-quality feedback achieve superior scoring performance. Furthermore, revision experiments using small LLMs show that the high-quality feedback identified by FeedEval leads to more effective essay revisions. We will release our code and curated datasets upon accepted.",
    "URL": "http://arxiv.org/abs/2601.04574v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04654",
    "type": "report",
    "title": "LLMs-Integrated Automatic Hate Speech Recognition Using Controllable Text Generation Models",
    "author": [
      {
        "family": "Oshima",
        "given": "Ryutaro"
      },
      {
        "family": "Hosoda",
        "given": "Yuya"
      },
      {
        "family": "Iiguni",
        "given": "Youji"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          8
        ]
      ]
    },
    "abstract": "This paper proposes an automatic speech recognition (ASR) model for hate speech using large language models (LLMs). The proposed method integrates the encoder of the ASR model with the decoder of the LLMs, enabling simultaneous transcription and censorship tasks to prevent the exposure of harmful content. Instruction tuning of the LLM to mask hate-related words with specific tokens requires an annotated hate speech dataset, which is limited. We generate text samples using an LLM with the Chain-of-Thought (CoT) prompting technique guided by cultural context and examples and then convert them into speech samples using a text-to-speech (TTS) system. However, some of them contain non-hate speech samples with hate-related words, which degrades the censorship performance. This paper filters the samples which text classification models correctly label as hate content. By adjusting the threshold for the number of correct answer models, we can control the level of hate in the generated dataset, allowing us to train the LLMs through curriculum learning in a gradual manner. Experimental results show that the proposed method achieves a masking accuracy of 58.6\\% for hate-related words, surpassing previous baselines. We also confirm that the curriculum training contributes to the efficiency of both transcription and censorship tasks.",
    "URL": "http://arxiv.org/abs/2601.04654v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04919",
    "type": "report",
    "title": "What Students Ask, How a Generative AI Assistant Responds: Exploring Higher Education Students' Dialogues on Learning Analytics Feedback",
    "author": [
      {
        "family": "Uzun",
        "given": "Yildiz"
      },
      {
        "family": "Gauthier",
        "given": "Andrea"
      },
      {
        "family": "Cukurova",
        "given": "Mutlu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          8
        ]
      ]
    },
    "abstract": "Learning analytics dashboards (LADs) aim to support students' regulation of learning by translating complex data into feedback. Yet students, especially those with lower self-regulated learning (SRL) competence, often struggle to engage with and interpret analytics feedback. Conversational generative artificial intelligence (GenAI) assistants have shown potential to scaffold this process through real-time, personalised, dialogue-based support. Further advancing this potential, we explored authentic dialogues between students and GenAI assistant integrated into LAD during a 10-week semester. The analysis focused on questions students with different SRL levels posed, the relevance and quality of the assistant's answers, and how students perceived the assistant's role in their learning. Findings revealed distinct query patterns. While low SRL students sought clarification and reassurance, high SRL students queried technical aspects and requested personalised strategies. The assistant provided clear and reliable explanations but limited in personalisation, handling emotionally charged queries, and integrating multiple data points for tailored responses. Findings further extend that GenAI interventions can be especially valuable for low SRL students, offering scaffolding that supports engagement with feedback and narrows gaps with their higher SRL peers. At the same time, students' reflections underscored the importance of trust, need for greater adaptivity, context-awareness, and technical refinement in future systems.",
    "URL": "http://arxiv.org/abs/2601.04919v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.04940",
    "type": "report",
    "title": "CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs",
    "author": [
      {
        "family": "Nijdam",
        "given": "Arthur"
      },
      {
        "family": "K√§hk√∂nen",
        "given": "Harri"
      },
      {
        "family": "Niemi",
        "given": "Valtteri"
      },
      {
        "family": "Wagner",
        "given": "Paul Stankovski"
      },
      {
        "family": "Ramezanian",
        "given": "Sara"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          8
        ]
      ]
    },
    "abstract": "The cybersecurity landscape is constantly evolving, driven by increased digitalization and new cybersecurity threats. Cybersecurity programs often fail to equip graduates with skills demanded by the workforce, particularly concerning recent developments in cybersecurity, as curriculum design is costly and labor-intensive. To address this misalignment, we present a novel Large Language Model (LLM)-based framework for automated design and analysis of cybersecurity curricula, called CurricuLLM. Our approach provides three key contributions: (1) automation of personalized curriculum design, (2) a data-driven pipeline aligned with industry demands, and (3) a comprehensive methodology for leveraging fine-tuned LLMs in curriculum development.\n  CurricuLLM utilizes a two-tier approach consisting of PreprocessLM, which standardizes input data, and ClassifyLM, which assigns course content to nine Knowledge Areas in cybersecurity. We systematically evaluated multiple Natural Language Processing (NLP) architectures and fine-tuning strategies, ultimately selecting the Bidirectional Encoder Representations from Transformers (BERT) model as ClassifyLM, fine-tuned on foundational cybersecurity concepts and workforce competencies.\n  We are the first to validate our method with human experts who analyzed real-world cybersecurity curricula and frameworks, motivating that CurricuLLM is an efficient solution to replace labor-intensive curriculum analysis. Moreover, once course content has been classified, it can be integrated with established cybersecurity role-based weights, enabling alignment of the educational program with specific job roles, workforce categories, or general market needs. This lays the foundation for personalized, workforce-aligned cybersecurity curricula that prepare students for the evolving demands in cybersecurity.",
    "URL": "http://arxiv.org/abs/2601.04940v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.05187",
    "type": "report",
    "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
    "author": [
      {
        "family": "Liang",
        "given": "Yanchang"
      },
      {
        "family": "Zhao",
        "given": "Xiaowei"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          8
        ]
      ]
    },
    "abstract": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
    "URL": "http://arxiv.org/abs/2601.05187v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.05414",
    "type": "report",
    "title": "Large Language Models Are Bad Dice Players: LLMs Struggle to Generate Random Numbers from Statistical Distributions",
    "author": [
      {
        "family": "Zhao",
        "given": "Minda"
      },
      {
        "family": "Du",
        "given": "Yilun"
      },
      {
        "family": "Wang",
        "given": "Mengyu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          8
        ]
      ]
    },
    "abstract": "As large language models (LLMs) transition from chat interfaces to integral components of stochastic pipelines across domains like educational assessment and synthetic data construction, the ability to faithfully sample from specified probability distributions has become a functional requirement rather than a theoretical curiosity. We present the first large-scale, statistically powered audit of native probabilistic sampling in frontier LLMs, benchmarking 11 models across 15 distributions. To disentangle failure modes, we employ a dual-protocol design: Batch Generation, where a model produces N=1000 samples within one response, and Independent Requests, comprising $N=1000$ stateless calls. We observe a sharp protocol asymmetry: batch generation achieves only modest statistical validity, with a 13% median pass rate, while independent requests collapse almost entirely, with 10 of 11 models passing none of the distributions. Beyond this asymmetry, we reveal that sampling fidelity degrades monotonically with distributional complexity and aggravates as the requested sampling horizon N increases. Finally, we demonstrate the propagation of these failures into downstream tasks: models fail to enforce uniform answer-position constraints in MCQ generation and systematically violate demographic targets in attribute-constrained text-to-image prompt synthesis. These findings indicate that current LLMs lack a functional internal sampler, necessitating the use of external tools for applications requiring statistical guarantees.",
    "URL": "http://arxiv.org/abs/2601.05414v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.05473",
    "type": "report",
    "title": "Towards Valid Student Simulation with Large Language Models",
    "author": [
      {
        "family": "Yuan",
        "given": "Zhihao"
      },
      {
        "family": "Xiao",
        "given": "Yunze"
      },
      {
        "family": "Li",
        "given": "Ming"
      },
      {
        "family": "Xuan",
        "given": "Weihao"
      },
      {
        "family": "Tong",
        "given": "Richard"
      },
      {
        "family": "Diab",
        "given": "Mona"
      },
      {
        "family": "Mitchell",
        "given": "Tom"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          9
        ]
      ]
    },
    "abstract": "This paper presents a conceptual and methodological framework for large language model (LLM) based student simulation in educational settings. The authors identify a core failure mode, termed the \"competence paradox\" in which broadly capable LLMs are asked to emulate partially knowledgeable learners, leading to unrealistic error patterns and learning dynamics. To address this, the paper reframes student simulation as a constrained generation problem governed by an explicit Epistemic State Specification (ESS), which defines what a simulated learner can access, how errors are structured, and how learner state evolves over time. The work further introduces a Goal-by-Environment framework to situate simulated student systems according to behavioral objectives and deployment contexts. Rather than proposing a new system or benchmark, the paper synthesizes prior literature, formalizes key design dimensions, and articulates open challenges related to validity, evaluation, and ethical risks. Overall, the paper argues for epistemic fidelity over surface realism as a prerequisite for using LLM-based simulated students as reliable scientific and pedagogical instruments.",
    "URL": "http://arxiv.org/abs/2601.05473v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.05485",
    "type": "report",
    "title": "Readability-Robust Code Summarization via Meta Curriculum Learning",
    "author": [
      {
        "family": "Zeng",
        "given": "Wenhao"
      },
      {
        "family": "Chai",
        "given": "Yitian"
      },
      {
        "family": "Zhou",
        "given": "Hao"
      },
      {
        "family": "Meng",
        "given": "Fandong"
      },
      {
        "family": "Zhou",
        "given": "Jie"
      },
      {
        "family": "Gu",
        "given": "Xiaodong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          9
        ]
      ]
    },
    "abstract": "Code summarization has emerged as a fundamental technique in the field of program comprehension. While code language models have shown significant advancements, the current models and benchmarks are confined to high-readability code, which contains sufficient semantic cues such as function and variable names. In the real world, however, code is often poorly structured or obfuscated, significantly degrading model performance. In this paper, we first empirically evaluate the robustness of state-of-the-art language models on poor-readability code for the task of code summarization, focusing on (1) their effectiveness, (2) the impact of prompt engineering, and (3) the robustness of different variants. Experimental results reveal that state-of-the-art models-including GPT-4o and DeepSeek-V3 experience a substantial performance drop when faced with poorly readable code, and that prompt engineering and reasoning-enhanced models offer limited improvements. Motivated by these findings, we propose RoFTCodeSum, a novel fine-tuning method that enhances the robustness of code summarization against poorly readable code. RoFTCodeSum marries the concepts of curriculum learning and meta-learning: based on the original dataset for fine-tuning, it creates curricular training sets, e.g., obfuscating function names and identifiers from the code, respectively, that have progressive difficulty in code comprehension. In each training step, the approach meta-updates the gradients using these progressively challenging datasets, thereby optimizing both accuracy and readability robustness simultaneously. Experimental results demonstrate that RoFTCodeSum exhibits increased robustness against semantic perturbation while enhancing performance on the original code.",
    "URL": "http://arxiv.org/abs/2601.05485v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.05858",
    "type": "report",
    "title": "CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning",
    "author": [
      {
        "family": "Dragomir",
        "given": "Alexandra"
      },
      {
        "family": "Brad",
        "given": "Florin"
      },
      {
        "family": "Ionescu",
        "given": "Radu Tudor"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          9
        ]
      ]
    },
    "abstract": "Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given during training. We address this topic by integrating curriculum learning into various state-of-the-art preference optimization algorithms to boost MT performance. We introduce a novel curriculum learning strategy with restarts (CLewR), which reiterates easy-to-hard curriculum multiple times during training to effectively mitigate the catastrophic forgetting of easy examples. We demonstrate consistent gains across several model families (Gemma2, Qwen2.5, Llama3.1) and preference optimization techniques. We publicly release our code at https://github.com/alexandra-dragomir/CLewR.",
    "URL": "http://arxiv.org/abs/2601.05858v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06092",
    "type": "report",
    "title": "Islamic Chatbots in the Age of Large Language Models",
    "author": [
      {
        "family": "Ahmad",
        "given": "Muhammad Aurangzeb"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          31
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) are rapidly transforming how communities access, interpret, and circulate knowledge, and religious communities are no exception. Chatbots powered by LLMs are beginning to reshape authority, pedagogy, and everyday religious practice in Muslim communities. We analyze the landscape of LLM powered Islamic chatbots and how they are transforming Islamic religious practices e.g., democratizing access to religious knowledge but also running the risk of erosion of authority. We discuss what kind of challenges do these systems raise for Muslim communities and explore recommendations for the responsible design of these systems.",
    "URL": "http://arxiv.org/abs/2601.06092v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06121",
    "type": "report",
    "title": "Prompt Engineering for Responsible Generative AI Use in African Education: A Report from a Three-Day Training Series",
    "author": [
      {
        "family": "Quarshie",
        "given": "Benjamin"
      },
      {
        "family": "Willemse",
        "given": "Vanessa"
      },
      {
        "family": "Nabang",
        "given": "Macharious"
      },
      {
        "family": "Akanzire",
        "given": "Bismark Nyaaba"
      },
      {
        "family": "Kyeremeh",
        "given": "Patrick"
      },
      {
        "family": "Maigari",
        "given": "Saeed"
      },
      {
        "family": "Adomina",
        "given": "Dorcas"
      },
      {
        "family": "Kwarteng",
        "given": "Ellen"
      },
      {
        "family": "Majialuwe",
        "given": "Eric Kojo"
      },
      {
        "family": "Gibbs",
        "given": "Craig"
      },
      {
        "family": "Kudaya",
        "given": "Jerry Etornam"
      },
      {
        "family": "Koma",
        "given": "Sechaba"
      },
      {
        "family": "Nyaaba",
        "given": "Matthew Nyaaba Matthew"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          4
        ]
      ]
    },
    "abstract": "Generative artificial intelligence (GenAI) tools are increasingly adopted in education, yet many educators lack structured guidance on responsible and context sensitive prompt engineering, particularly in African and other resource constrained settings. This case report documents a three day online professional development programme organised by Generative AI for Education and Research in Africa (GenAI-ERA), designed to strengthen educators and researchers capacity to apply prompt engineering ethically for academic writing, teaching, and research. The programme engaged 468 participants across multiple African countries, including university educators, postgraduate students, and researchers. The training followed a scaffolded progression from foundational prompt design to applied and ethical strategies, including persona guided interactions. Data sources comprised registration surveys, webinar interaction records, facilitator observations, and session transcripts, analysed using descriptive statistics and computationally supported qualitative techniques. Findings indicate that participants increasingly conceptualised prompt engineering as a form of AI literacy requiring ethical awareness, contextual sensitivity, and pedagogical judgement rather than technical skill alone. The case highlights persistent challenges related to access, locally relevant training materials, and institutional support. The report recommends sustained professional development and the integration of prompt literacy into curricula to support responsible GenAI use in African education systems.",
    "URL": "http://arxiv.org/abs/2601.06121v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06141",
    "type": "report",
    "title": "An LLM -Powered Assessment Retrieval-Augmented Generation (RAG) For Higher Education",
    "author": [
      {
        "family": "Barenji",
        "given": "Reza Vatankhah"
      },
      {
        "family": "Salimi",
        "given": "Nazila"
      },
      {
        "family": "Khoshgoftar",
        "given": "Sina"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          5
        ]
      ]
    },
    "abstract": "Providing timely, consistent, and high-quality feedback in large-scale higher education courses remains a persistent challenge, often constrained by instructor workload and resource limitations. This study presents an LLM-powered, agentic assessment system built on a Retrieval-Augmented Generation (RAG) architecture to address these challenges. The system integrates a large language model with a structured retrieval mechanism that accesses rubric criteria, exemplar essays, and instructor feedback to generate contextually grounded grades and formative comments. A mixed-methods evaluation was conducted using 701 student essays, combining quantitative analyses of inter-rater reliability, scoring alignment, and consistency with instructor assessments, alongside qualitative evaluation of feedback quality, pedagogical relevance, and student support. Results demonstrate that the RAG system can produce reliable, rubric-aligned feedback at scale, achieving 94--99% agreement with human evaluators, while also enhancing students' opportunities for self-regulated learning and engagement with assessment criteria. The discussion highlights both pedagogical limitations, including potential constraints on originality and feedback dialogue, and the transformative potential of RAG systems to augment instructors' capabilities, streamline assessment workflows, and support scalable, adaptive learning environments. This research contributes empirical evidence for the application of agentic AI in higher education, offering a scalable and pedagogically informed model for enhancing feedback accessibility, consistency, and quality.",
    "URL": "http://arxiv.org/abs/2601.06141v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06171",
    "type": "report",
    "title": "From Individual Prompts to Collective Intelligence: Mainstreaming Generative AI in the Classroom",
    "author": [
      {
        "family": "Qadir",
        "given": "Junaid"
      },
      {
        "family": "Khan",
        "given": "Muhammad Salman"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "Engineering classrooms are increasingly experimenting with generative AI (GenAI), but most uses remain confined to individual prompting and isolated assistance. This narrow framing risks reinforcing equity gaps and only rewarding the already privileged or motivated students. We argue instead for a shift toward collective intelligence (CI)-focused pedagogy, where GenAI acts as a catalyst for peer-to-peer learning. We implemented Generative CI (GCI) activities in two undergraduate engineering courses, engaging 140 students through thinking routines -- short, repeatable scaffolds developed by Harvard Project Zero to make thinking visible and support collaborative sense-making. Using routines such as Question Sorts and Peel the Fruit, combined with strategic AI consultation, we enabled students to externalize their reasoning, compare interpretations, and iteratively refine ideas. Our dual-pronged approach synthesizes literature from learning sciences, CI, embodied cognition, and philosophy of technology, while also empirically learning through student surveys and engagement observations. Results demonstrate that students value the combination of human collaboration with strategic AI support, recognizing risks of over-reliance while appreciating AI's role in expanding perspectives. Students identified that group work fosters deeper understanding and creative problem-solving than AI alone, with the timing of AI consultation significantly affecting learning outcomes. We offer practical implementation pathways for mainstreaming CI-focused pedagogy that cultivates deeper engagement, resilient problem-solving, and shared ownership of knowledge.",
    "URL": "http://arxiv.org/abs/2601.06171v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06172",
    "type": "report",
    "title": "The Psychology of Learning from Machines: Anthropomorphic AI and the Paradox of Automation in Education",
    "author": [
      {
        "family": "Qadir",
        "given": "Junaid"
      },
      {
        "family": "Mumtaz",
        "given": "Muhammad"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          7
        ]
      ]
    },
    "abstract": "As AI tutors enter classrooms at unprecedented speed, their deployment increasingly outpaces our grasp of the psychological and social consequences of such technology. Yet decades of research in automation psychology, human factors, and human-computer interaction provide crucial insights that remain underutilized in educational AI design. This work synthesizes four research traditions -- automation psychology, human factors engineering, HCI, and philosophy of technology -- to establish a comprehensive framework for understanding how learners psychologically relate to anthropomorphic AI tutors. We identify three persistent challenges intensified by Generative AI's conversational fluency. First, learners exhibit dual trust calibration failures -- automation bias (uncritical acceptance) and algorithm aversion (excessive rejection after errors) -- with an expertise paradox where novices overrely while experts underrely. Second, while anthropomorphic design enhances engagement, it can distract from learning and foster harmful emotional attachment. Third, automation ironies persist: systems meant to aid cognition introduce designer errors, degrade skills through disuse, and create monitoring burdens humans perform poorly. We ground this theoretical synthesis through comparative analysis of over 104,984 YouTube comments across AI-generated philosophical debates and human-created engineering tutorials, revealing domain-dependent trust patterns and strong anthropomorphic projection despite minimal cues. For engineering education, our synthesis mandates differentiated approaches: AI tutoring for technical foundations where automation bias is manageable through proper scaffolding, but human facilitation for design, ethics, and professional judgment where tacit knowledge transmission proves irreplaceable.",
    "URL": "http://arxiv.org/abs/2601.06172v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06225",
    "type": "report",
    "title": "Classroom AI: Large Language Models as Grade-Specific Teachers",
    "author": [
      {
        "family": "Oh",
        "given": "Jio"
      },
      {
        "family": "Whang",
        "given": "Steven Euijong"
      },
      {
        "family": "Evans",
        "given": "James"
      },
      {
        "family": "Wang",
        "given": "Jindong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          9
        ]
      ]
    },
    "abstract": "Large Language Models (LLMs) offer a promising solution to complement traditional teaching and address global teacher shortages that affect hundreds of millions of children, but they fail to provide grade-appropriate responses for students at different educational levels. We introduce a framework for finetuning LLMs to generate age-appropriate educational content across six grade levels, from lower elementary to adult education. Our framework successfully adapts explanations to match students' comprehension capacities without sacrificing factual correctness. This approach integrates seven established readability metrics through a clustering method and builds a comprehensive dataset for grade-specific content generation. Evaluations across multiple datasets with 208 human participants demonstrate substantial improvements in grade-level alignment, achieving a 35.64 percentage point increase compared to prompt-based methods while maintaining response accuracy. AI-assisted learning tailored to different grade levels has the potential to advance educational engagement and equity.",
    "URL": "http://arxiv.org/abs/2601.06225v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06394",
    "type": "report",
    "title": "Context Matters: Peer-Aware Student Behavioral Engagement Measurement via VLM Action Parsing and LLM Sequence Classification",
    "author": [
      {
        "family": "Abdelkawy",
        "given": "Ahmed"
      },
      {
        "family": "Elsayed",
        "given": "Ahmed"
      },
      {
        "family": "Ali",
        "given": "Asem"
      },
      {
        "family": "Farag",
        "given": "Aly"
      },
      {
        "family": "Tretter",
        "given": "Thomas"
      },
      {
        "family": "McIntyre",
        "given": "Michael"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          10
        ]
      ]
    },
    "abstract": "Understanding student behavior in the classroom is essential to improve both pedagogical quality and student engagement. Existing methods for predicting student engagement typically require substantial annotated data to model the diversity of student behaviors, yet privacy concerns often restrict researchers to their own proprietary datasets. Moreover, the classroom context, represented in peers' actions, is ignored. To address the aforementioned limitation, we propose a novel three-stage framework for video-based student engagement measurement. First, we explore the few-shot adaptation of the vision-language model for student action recognition, which is fine-tuned to distinguish among action categories with a few training samples. Second, to handle continuous and unpredictable student actions, we utilize the sliding temporal window technique to divide each student's 2-minute-long video into non-overlapping segments. Each segment is assigned an action category via the fine-tuned VLM model, generating a sequence of action predictions. Finally, we leverage the large language model to classify this entire sequence of actions, together with the classroom context, as belonging to an engaged or disengaged student. The experimental results demonstrate the effectiveness of the proposed approach in identifying student engagement.",
    "URL": "http://arxiv.org/abs/2601.06394v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06536",
    "type": "report",
    "title": "Expos√≠a: Academic Writing Assessment of Expos√©s and Peer Feedback",
    "author": [
      {
        "family": "Zyska",
        "given": "Dennis"
      },
      {
        "family": "Rozovskaya",
        "given": "Alla"
      },
      {
        "family": "Kuznetsov",
        "given": "Ilia"
      },
      {
        "family": "Gurevych",
        "given": "Iryna"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          10
        ]
      ]
    },
    "abstract": "We present Expos√≠a, the first public dataset that connects writing and feedback assessment in higher education, enabling research on educationally grounded approaches to academic writing evaluation. Expos√≠a includes student research project proposals and peer and instructor feedback consisting of comments and free-text reviews. The dataset was collected in the \"Introduction to Scientific Work\" course of the Computer Science undergraduate program that focuses on teaching academic writing skills and providing peer feedback on academic writing. Expos√≠a reflects the multi-stage nature of the academic writing process that includes drafting, providing and receiving feedback, and revising the writing based on the feedback received. Both the project proposals and peer feedback are accompanied by human assessment scores based on a fine-grained, pedagogically-grounded schema for writing and feedback assessment that we develop.\n  We use Expos√≠a to benchmark state-of-the-art open-source large language models (LLMs) for two tasks: automated scoring of (1) the proposals and (2) the student reviews. The strongest LLMs attain high agreement on scoring aspects that require little domain knowledge but degrade on dimensions evaluating content, in line with human agreement values. We find that LLMs align better with the human instructors giving high scores. Finally, we establish that a prompting strategy that scores multiple aspects of the writing together is the most effective, an important finding for classroom deployment.",
    "URL": "http://arxiv.org/abs/2601.06536v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06543",
    "type": "report",
    "title": "SimLLM: Fine-Tuning Code LLMs for SimPy-Based Queueing System Simulation",
    "author": [
      {
        "family": "Chen",
        "given": "Jun-Qi"
      },
      {
        "family": "Zhang",
        "given": "Kun"
      },
      {
        "family": "Zheng",
        "given": "Rui"
      },
      {
        "family": "Zhong",
        "given": "Ying"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          10
        ]
      ]
    },
    "abstract": "The Python package SimPy is widely used for modeling queueing systems due to its flexibility, simplicity, and smooth integration with modern data analysis and optimization frameworks. Recent advances in large language models (LLMs) have shown strong ability in generating clear and executable code, making them powerful and suitable tools for writing SimPy queueing simulation code. However, directly employing closed-source models like GPT-4o to generate such code may lead to high computational costs and raise data privacy concerns. To address this, we fine-tune two open-source LLMs, Qwen-Coder-7B and DeepSeek-Coder-6.7B, on curated SimPy queueing data, which enhances their code-generating performance in executability, output-format compliance, and instruction-code consistency. Particularly, we proposed a multi-stage fine-tuning framework comprising two stages of supervised fine-tuning (SFT) and one stage of direct preference optimization (DPO), progressively enhancing the model's ability in SimPy-based queueing simulation code generation. Extensive evaluations demonstrate that both fine-tuned models achieve substantial improvements in executability, output-format compliance, and instruct consistency. These results confirm that domain-specific fine-tuning can effectively transform compact open-source code models into reliable SimPy simulation generators which provide a practical alternative to closed-source LLMs for education, research, and operational decision support.",
    "URL": "http://arxiv.org/abs/2601.06543v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06586",
    "type": "report",
    "title": "Detecting LLM-Generated Text with Performance Guarantees",
    "author": [
      {
        "family": "Zhou",
        "given": "Hongyi"
      },
      {
        "family": "Zhu",
        "given": "Jin"
      },
      {
        "family": "Yang",
        "given": "Ying"
      },
      {
        "family": "Shi",
        "given": "Chengchun"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          10
        ]
      ]
    },
    "abstract": "Large language models (LLMs) such as GPT, Claude, Gemini, and Grok have been deeply integrated into our daily life. They now support a wide range of tasks -- from dialogue and email drafting to assisting with teaching and coding, serving as search engines, and much more. However, their ability to produce highly human-like text raises serious concerns, including the spread of fake news, the generation of misleading governmental reports, and academic misconduct. To address this practical problem, we train a classifier to determine whether a piece of text is authored by an LLM or a human. Our detector is deployed on an online CPU-based platform https://huggingface.co/spaces/stats-powered-ai/StatDetectLLM, and contains three novelties over existing detectors: (i) it does not rely on auxiliary information, such as watermarks or knowledge of the specific LLM used to generate the text; (ii) it more effectively distinguishes between human- and LLM-authored text; and (iii) it enables statistical inference, which is largely absent in the current literature. Empirically, our classifier achieves higher classification accuracy compared to existing detectors, while maintaining type-I error control, high statistical power, and computational efficiency.",
    "URL": "http://arxiv.org/abs/2601.06586v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06633",
    "type": "report",
    "title": "KASER: Knowledge-Aligned Student Error Simulator for Open-Ended Coding Tasks",
    "author": [
      {
        "family": "Duan",
        "given": "Zhangqi"
      },
      {
        "family": "Fernandez",
        "given": "Nigel"
      },
      {
        "family": "Lan",
        "given": "Andrew"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          10
        ]
      ]
    },
    "abstract": "Open-ended tasks, such as coding problems that are common in computer science education, provide detailed insights into student knowledge. However, training large language models (LLMs) to simulate and predict possible student errors in their responses to these problems can be challenging: they often suffer from mode collapse and fail to fully capture the diversity in syntax, style, and solution approach in student responses. In this work, we present KASER (Knowledge-Aligned Student Error Simulator), a novel approach that aligns errors with student knowledge. We propose a training method based on reinforcement learning using a hybrid reward that reflects three aspects of student code prediction: i) code similarity to the ground-truth, ii) error matching, and iii) code prediction diversity. On two real-world datasets, we perform two levels of evaluation and show that: At the per-student-problem pair level, our method outperforms baselines on code and error prediction; at the per-problem level, our method outperforms baselines on error coverage and simulated code diversity.",
    "URL": "http://arxiv.org/abs/2601.06633v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06767",
    "type": "report",
    "title": "GanitLLM: Difficulty-Aware Bengali Mathematical Reasoning through Curriculum-GRPO",
    "author": [
      {
        "family": "Dipta",
        "given": "Shubhashis Roy"
      },
      {
        "family": "Mahbub",
        "given": "Khairul"
      },
      {
        "family": "Najjar",
        "given": "Nadia"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          11
        ]
      ]
    },
    "abstract": "We present a Bengali mathematical reasoning model called GanitLLM (named after the Bangla word for mathematics, \"Ganit\"), together with a new difficulty-aware Bengali math corpus and a curriculum-based GRPO pipeline. Bengali is one of the world's most widely spoken languages, yet existing LLMs either reason in English and then translate, or simply fail on multi-step Bengali math, in part because reinforcement learning recipes are tuned for high-resource languages and collapse under reward sparsity in low-resource settings. To address this, we construct Ganit, a rigorously filtered and decontaminated Bengali math dataset with automatic difficulty tags derived from the pass@k of a strong evaluator model. Building on this dataset, we propose Curriculum-GRPO, which combines multi-stage training (SFT + GRPO) with difficulty-aware sampling and verifiable rewards for format, numerical correctness, and Bengali reasoning. On Bn-MGSM and Bn-MSVAMP, GanitLLM-4B improves over its Qwen3-4B base by +8 and +7 accuracy points, respectively, while increasing the percentage of Bengali reasoning tokens from 14% to over 88% and reducing average solution length from 943 to 193 words.",
    "URL": "http://arxiv.org/abs/2601.06767v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06774",
    "type": "report",
    "title": "ImmuniFraug: A Metacognitive Intervention Anti-Fraud Approach to Enhance Undergraduate Students' Cyber Fraud Awareness",
    "author": [
      {
        "family": "Yuan",
        "given": "Xiangzhe"
      },
      {
        "family": "Wang",
        "given": "Jiajun"
      },
      {
        "family": "Wang",
        "given": "Huanchen"
      },
      {
        "family": "Wan",
        "given": "Qian"
      },
      {
        "family": "Hu",
        "given": "Siying"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          11
        ]
      ]
    },
    "abstract": "Cyber fraud now constitutes over half of criminal cases in China, with undergraduate students experiencing a disproportionate rise in victimization. Traditional anti-fraud training remains predominantly passive, yielding limited engagement and retention. This paper introduces ImmuniFraug, a Large Language Model (LLM)-based metacognitive intervention that delivers immersive, multimodal fraud simulations integrating text, voice, and visual avatars across ten prevalent fraud types. Each scenario is designed to replicate real-world persuasion tactics and psychological pressure, while post-interaction debriefs provide grounded feedback in protection motivation theory and reflective prompts to reinforce learning. In a controlled study with 846 Chinese undergraduates, ImmuniFraug was compared to official text-based materials. Linear Mixed-Effects Modeling (LMEM) reveals that the interactive intervention significantly improved fraud awareness (p = 0.026), successfully providing incremental learning value even when controlling for participants' extensive prior exposure to anti-fraud education, alongside high narrative immersion (M = 56.95/77). Thematic analysis of interviews revealed key effectiveness factors: perceived realism, adaptive deception, enforced time pressure, emotional manipulation awareness, and enhanced self-efficacy. Findings demonstrate that by shifting the focus from passive knowledge acquisition to active metacognitive engagement, LLM-based simulations offer a scalable and ecologically valid new paradigm for anti-fraud training and fostering fraud resilience.",
    "URL": "http://arxiv.org/abs/2601.06774v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06780",
    "type": "report",
    "title": "Multi-Stage Evolutionary Model Merging with Meta Data Driven Curriculum Learning for Sentiment-Specialized Large Language Modeling",
    "author": [
      {
        "family": "Inoshita",
        "given": "Keito"
      },
      {
        "family": "Zhou",
        "given": "Xiaokang"
      },
      {
        "family": "Kawai",
        "given": "Akira"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          11
        ]
      ]
    },
    "abstract": "The emergence of large language models (LLMs) has significantly transformed natural language processing (NLP), enabling more generalized models to perform various tasks with minimal training. However, traditional sentiment analysis methods, which focus on individual tasks such as sentiment classification or aspect-based analysis, are not practical for real-world applications that usually require handling multiple tasks. While offering flexibility, LLMs in sentiment-specific tasks often fall short of the required accuracy. Techniques like fine-tuning and evolutionary model merging help integrate models into a unified framework, which can improve the learning performance while reducing computational costs. The use of task meta-data and curriculum learning to optimize learning processes remains underexplored, while sentiment analysis is a critical task in NLP that requires high accuracy and scalability across multiple subtasks. In this study, we propose a hybrid learning model called Multi-stage Evolutionary Model Merging with Meta data driven Curriculum Learning (MEM-MCL), to enhance the sentiment analysis in large language modeling. In particular, expert models are created through instruction tuning for specific sentiment tasks and then merged using evolutionary algorithms to form a unified model. The merging process is optimized with weak data to enhance performance across tasks. The curriculum learning is incorporated to provide a learning sequence based on task difficulty, improving knowledge extraction from LLMs. Experiment results demonstrate that the proposed MEM-MCL model outperforms conventional LLMs in a majority of sentiment analysis tasks, achieving superior results across various subtasks.",
    "URL": "http://arxiv.org/abs/2601.06780v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.06979",
    "type": "report",
    "title": "MedTutor: A Retrieval-Augmented LLM System for Case-Based Medical Education",
    "author": [
      {
        "family": "Jang",
        "given": "Dongsuk"
      },
      {
        "family": "Shangguan",
        "given": "Ziyao"
      },
      {
        "family": "Tegtmeyer",
        "given": "Kyle"
      },
      {
        "family": "Gupta",
        "given": "Anurag"
      },
      {
        "family": "Czerminski",
        "given": "Jan"
      },
      {
        "family": "Chheang",
        "given": "Sophie"
      },
      {
        "family": "Cohan",
        "given": "Arman"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          11
        ]
      ]
    },
    "abstract": "The learning process for medical residents presents significant challenges, demanding both the ability to interpret complex case reports and the rapid acquisition of accurate medical knowledge from reliable sources. Residents typically study case reports and engage in discussions with peers and mentors, but finding relevant educational materials and evidence to support their learning from these cases is often time-consuming and challenging. To address this, we introduce MedTutor, a novel system designed to augment resident training by automatically generating evidence-based educational content and multiple-choice questions from clinical case reports. MedTutor leverages a Retrieval-Augmented Generation (RAG) pipeline that takes clinical case reports as input and produces targeted educational materials. The system's architecture features a hybrid retrieval mechanism that synergistically queries a local knowledge base of medical textbooks and academic literature (using PubMed, Semantic Scholar APIs) for the latest related research, ensuring the generated content is both foundationally sound and current. The retrieved evidence is filtered and ordered using a state-of-the-art reranking model and then an LLM generates the final long-form output describing the main educational content regarding the case-report. We conduct a rigorous evaluation of the system. First, three radiologists assessed the quality of outputs, finding them to be of high clinical and educational value. Second, we perform a large scale evaluation using an LLM-as-a Judge to understand if LLMs can be used to evaluate the output of the system. Our analysis using correlation between LLMs outputs and human expert judgments reveals a moderate alignment and highlights the continued necessity of expert oversight.",
    "DOI": "10.18653/v1/2025.emnlp-demos.24",
    "URL": "https://doi.org/10.18653/v1/2025.emnlp-demos.24",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.07354",
    "type": "report",
    "title": "Semantic Compression of LLM Instructions via Symbolic Metalanguages",
    "author": [
      {
        "family": "Gassen",
        "given": "Ernst van"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          12
        ]
      ]
    },
    "abstract": "We introduce MetaGlyph, a symbolic language for compressing prompts by encoding instructions as mathematical symbols rather than prose. Unlike systems requiring explicit decoding rules, MetaGlyph uses symbols like $\\in$ (membership) and $\\Rightarrow$ (implication) that models already understand from their training data. We test whether these symbols work as ''instruction shortcuts'' that models can interpret without additional teaching.\n  We evaluate eight models across two dimensions relevant to practitioners: scale (3B-1T parameters) and accessibility (open-source for local deployment vs. proprietary APIs). MetaGlyph achieves 62-81% token reduction across all task types. For API-based deployments, this translates directly to cost savings; for local deployments, it reduces latency and memory pressure.\n  Results vary by model. Gemini 2.5 Flash achieves 75% semantic equivalence between symbolic and prose instructions on selection tasks, with 49.9% membership operator fidelity. Kimi K2 reaches 98.1% fidelity for implication ($\\Rightarrow$) and achieves perfect (100%) accuracy on selection tasks with symbolic prompts. GPT-5.2 Chat shows the highest membership fidelity observed (91.3%), though with variable parse success across task types. Claude Haiku 4.5 achieves 100% parse success with 26% membership fidelity. Among mid-sized models, Qwen 2.5 7B shows 62% equivalence on extraction tasks. Mid-sized open-source models (7B-12B) show near-zero operator fidelity, suggesting a U-shaped relationship where sufficient scale overcomes instruction-tuning biases.",
    "URL": "http://arxiv.org/abs/2601.07354v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.08035",
    "type": "report",
    "title": "From Tool to Teacher: Rethinking Search Systems as Instructive Interfaces",
    "author": [
      {
        "family": "Elsweiler",
        "given": "David"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          12
        ]
      ]
    },
    "abstract": "Information access systems such as search engines and generative AI are central to how people seek, evaluate, and interpret information. Yet most systems are designed to optimise retrieval rather than to help users develop better search strategies or critical awareness. This paper introduces a pedagogical perspective on information access, conceptualising search and conversational systems as instructive interfaces that can teach, guide, and scaffold users' learning. We draw on seven didactic frameworks from education and behavioural science to analyse how existing and emerging system features, including query suggestions, source labels, and conversational or agentic AI, support or limit user learning. Using two illustrative search tasks, we demonstrate how different design choices promote skills such as critical evaluation, metacognitive reflection, and strategy transfer. The paper contributes a conceptual lens for evaluating the instructional value of information access systems and outlines design implications for technologies that foster more effective, reflective, and resilient information seekers.",
    "URL": "http://arxiv.org/abs/2601.08035v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.08402",
    "type": "report",
    "title": "PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors",
    "author": [
      {
        "family": "Rooein",
        "given": "Donya"
      },
      {
        "family": "Chowdhury",
        "given": "Sankalan Pal"
      },
      {
        "family": "Eremeeva",
        "given": "Mariia"
      },
      {
        "family": "Qin",
        "given": "Yuan"
      },
      {
        "family": "Nozza",
        "given": "Debora"
      },
      {
        "family": "Sachan",
        "given": "Mrinmaya"
      },
      {
        "family": "Hovy",
        "given": "Dirk"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          13
        ]
      ]
    },
    "abstract": "Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. Despite this, current LLM tutoring systems do not take into account student personality traits. To address this problem, we first construct a taxonomy that links pedagogical methods to personality profiles, based on pedagogical literature. We simulate student-teacher conversations and use our framework to let the LLM tutor adjust its strategy to the simulated student personality. We evaluate the scenario with human teachers and find that they consistently prefer our approach over two baselines. Our method also increases the use of less common, high-impact strategies such as role-playing, which human and LLM annotators prefer significantly. Our findings pave the way for developing more personalized and effective LLM use in educational applications.",
    "URL": "http://arxiv.org/abs/2601.08402v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.08697",
    "type": "report",
    "title": "Auditing Student-AI Collaboration: A Case Study of Online Graduate CS Students",
    "author": [
      {
        "family": "Dan",
        "given": "Nifu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          13
        ]
      ]
    },
    "abstract": "As generative AI becomes embedded in higher education, it increasingly shapes how students complete academic tasks. While these systems offer efficiency and support, concerns persist regarding over-automation, diminished student agency, and the potential for unreliable or hallucinated outputs. This study conducts a mixed-methods audit of student-AI collaboration preferences by examining the alignment between current AI capabilities and students' desired levels of automation in academic work. Using two sequential and complementary surveys, we capture students' perceived benefits, risks, and preferred boundaries when using AI. The first survey employs an existing task-based framework to assess preferences for and actual usage of AI across 12 academic tasks, alongside primary concerns and reasons for use. The second survey, informed by the first, explores how AI systems could be designed to address these concerns through open-ended questions. This study aims to identify gaps between existing AI affordances and students' normative expectations of collaboration, informing the development of more effective and trustworthy AI systems for education.",
    "URL": "http://arxiv.org/abs/2601.08697v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.08857",
    "type": "report",
    "title": "Revisiting Software Engineering Education in the Era of Large Language Models: A Curriculum Adaptation and Academic Integrity Framework",
    "author": [
      {
        "family": "Degerli",
        "given": "Mustafa"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          6
        ]
      ]
    },
    "abstract": "The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into professional workflows is increasingly reshaping software engineering practices. These tools have lowered the cost of code generation, explanation, and testing, while introducing new forms of automation into routine development tasks. In contrast, most of the software engineering and computer engineering curricula remain closely aligned with pedagogical models that equate manual syntax production with technical competence. This growing misalignment raises concerns regarding assessment validity, learning outcomes, and the development of foundational skills. Adopting a conceptual research approach, this paper proposes a theoretical framework for analyzing how generative AI alters core software engineering competencies and introduces a pedagogical design model for LLM-integrated education. Attention is given to computer engineering programs in Turkey, where centralized regulation, large class sizes, and exam-oriented assessment practices amplify these challenges. The framework delineates how problem analysis, design, implementation, and testing increasingly shift from construction toward critique, validation, and human-AI stewardship. In addition, the paper argues that traditional plagiarism-centric integrity mechanisms are becoming insufficient, motivating a transition toward a process transparency model. While this work provides a structured proposal for curriculum adaptation, it remains a theoretical contribution; the paper concludes by outlining the need for longitudinal empirical studies to evaluate these interventions and their long-term impacts on learning.",
    "URL": "http://arxiv.org/abs/2601.08857v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.08950",
    "type": "report",
    "title": "ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue",
    "author": [
      {
        "family": "Sharma",
        "given": "Mayank"
      },
      {
        "family": "Pea",
        "given": "Roy"
      },
      {
        "family": "Subramonyam",
        "given": "Hari"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          13
        ]
      ]
    },
    "abstract": "In educational applications, LLMs exhibit several fundamental pedagogical limitations, such as their tendency to reveal solutions rather than support dialogic learning. We introduce ConvoLearn (https://huggingface.co/datasets/masharma/convolearn ), a dataset grounded in knowledge building theory that operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. We construct a semi-synthetic dataset of 1250 tutor-student dialogues (20 turns each) in middle school Earth Science through controlled interactions between human teachers and a simulated student. Using QLoRA, we demonstrate that training on this dataset meaningfully shifts LLM behavior toward knowledge-building strategies. Human evaluation by 31 teachers shows our fine-tuned Mistral 7B (M = 4.10, SD = 1.03) significantly outperforms both its base version (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29) overall. This work establishes a potential framework to guide future development and evaluation of constructivist AI tutors.",
    "URL": "http://arxiv.org/abs/2601.08950v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.09120",
    "type": "report",
    "title": "Adaptive Multi-Stage Patent Claim Generation with Unified Quality Assessment",
    "author": [
      {
        "family": "Liang",
        "given": "Chen-Wei"
      },
      {
        "family": "Guo",
        "given": "Bin"
      },
      {
        "family": "Wei",
        "given": "Zhen-Yuan"
      },
      {
        "family": "Wang",
        "given": "Mu-Jiang-Shan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          14
        ]
      ]
    },
    "abstract": "Current patent claim generation systems face three fundamental limitations: poor cross-jurisdictional generalization, inadequate semantic relationship modeling between claims and prior art, and unreliable quality assessment. We introduce a novel three-stage framework that addresses these challenges through relationship-aware similarity analysis, domain-adaptive claim generation, and unified quality assessment. Our approach employs multi-head attention with eight specialized heads for explicit relationship modeling, integrates curriculum learning with dynamic LoRA adapter selection across five patent domains, and implements cross-attention mechanisms between evaluation aspects for comprehensive quality assessment. Extensive experiments on USPTO HUPD dataset, EPO patent collections, and Patent-CE benchmark demonstrate substantial improvements: 7.6-point ROUGE-L gain over GPT-4o, 8.3\\% BERTScore enhancement over Llama-3.1-8B, and 0.847 correlation with human experts compared to 0.623 for separate evaluation models. Our method maintains 89.4\\% cross-jurisdictional performance retention versus 76.2\\% for baselines, establishing a comprehensive solution for automated patent prosecution workflows.",
    "URL": "http://arxiv.org/abs/2601.09120v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.09182",
    "type": "report",
    "title": "Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback",
    "author": [
      {
        "family": "Yun",
        "given": "JungMin"
      },
      {
        "family": "Kwon",
        "given": "JuneHyoung"
      },
      {
        "family": "Kim",
        "given": "MiHyeon"
      },
      {
        "family": "Kim",
        "given": "YoungBin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          14
        ]
      ]
    },
    "abstract": "The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.",
    "URL": "http://arxiv.org/abs/2601.09182v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.09470",
    "type": "report",
    "title": "Personalized Multimodal Feedback Using Multiple External Representations: Strategy Profiles and Learning in High School Physics",
    "author": [
      {
        "family": "Revenga-Lozano",
        "given": "Natalia"
      },
      {
        "family": "Avila",
        "given": "Karina E."
      },
      {
        "family": "Steinert",
        "given": "Steffen"
      },
      {
        "family": "Schweinberger",
        "given": "Matthias"
      },
      {
        "family": "G√≥mez-P√©rez",
        "given": "Clara E."
      },
      {
        "family": "Kuhn",
        "given": "Jochen"
      },
      {
        "family": "K√ºchemann",
        "given": "Stefan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          14
        ]
      ]
    },
    "abstract": "Multiple external representations (MERs) and personalized feedback support physics learning, yet evidence on how personalized feedback can effectively integrate MERs remains limited. This question is particularly timely given the emergence of multimodal large language models. We conducted a 16-24 week observational study in high school physics (N=661) using a computer-based platform that provided verification and optional elaborated feedback in verbal, graphical and mathematical forms. Linear mixed-effects models and strategy-cluster analyses (ANCOVA-adjusted comparisons) tested associations between feedback use and post-test performance and moderation by representational competence. Elaborated multirepresentational feedback showed a small but consistent positive association with post-test scores independent of prior knowledge and confidence. Learners adopted distinct representation-selection strategies; among students with lower representational competence, using a diverse set of representations related to higher learning, whereas this advantage diminished as competence increased. These findings motivate adaptive feedback designs and inform intelligent tutoring systems capable of tailoring feedback elaboration and representational format to learner profiles, advancing personalized instruction in physics education.",
    "URL": "http://arxiv.org/abs/2601.09470v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.09953",
    "type": "report",
    "title": "Take Out Your Calculators: Estimating the Real Difficulty of Question Items with LLM Student Simulations",
    "author": [
      {
        "family": "Acquaye",
        "given": "Christabel"
      },
      {
        "family": "Huang",
        "given": "Yi Ting"
      },
      {
        "family": "Carpuat",
        "given": "Marine"
      },
      {
        "family": "Rudinger",
        "given": "Rachel"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          15
        ]
      ]
    },
    "abstract": "Standardized math assessments require expensive human pilot studies to establish the difficulty of test items. We investigate the predictive value of open-source large language models (LLMs) for evaluating the difficulty of multiple-choice math questions for real-world students. We show that, while LLMs are poor direct judges of problem difficulty, simulation-based approaches with LLMs yield promising results under the right conditions. Under the proposed approach, we simulate a \"classroom\" of 4th, 8th, or 12th grade students by prompting the LLM to role-play students of varying proficiency levels. We use the outcomes of these simulations to fit Item Response Theory (IRT) models, comparing learned difficulty parameters for items to their real-world difficulties, as determined by item-level statistics furnished by the National Assessment of Educational Progress (NAEP). We observe correlations as high as 0.75, 0.76, and 0.82 for grades 4, 8, and 12, respectively. In our simulations, we experiment with different \"classroom sizes,\" showing tradeoffs between computation size and accuracy. We find that role-plays with named students improves predictions (compared to student ids), and stratifying names across gender and race further improves predictions. Our results show that LLMs with relatively weaker mathematical abilities (Gemma) actually yield better real-world difficulty predictions than mathematically stronger models (Llama and Qwen), further underscoring the suitability of open-source models for the task.",
    "URL": "http://arxiv.org/abs/2601.09953v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.10093",
    "type": "report",
    "title": "Mark My Works Autograder for Programming Courses",
    "author": [
      {
        "family": "Qiu",
        "given": "Yiding"
      },
      {
        "family": "Azimi",
        "given": "Seyed Mahdi"
      },
      {
        "family": "Lensky",
        "given": "Artem"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          15
        ]
      ]
    },
    "abstract": "Large programming courses struggle to provide timely, detailed feedback on student code. We developed Mark My Works, a local autograding system that combines traditional unit testing with LLM-generated explanations. The system uses role-based prompts to analyze submissions, critique code quality, and generate pedagogical feedback while maintaining transparency in its reasoning process.\n  We piloted the system in a 191-student engineering course, comparing AI-generated assessments with human grading on 79 submissions. While AI scores showed no linear correlation with human scores (r = -0.177, p = 0.124), both systems exhibited similar left-skewed distributions, suggesting they recognize comparable quality hierarchies despite different scoring philosophies. The AI system demonstrated more conservative scoring (mean: 59.95 vs 80.53 human) but generated significantly more detailed technical feedback.",
    "URL": "http://arxiv.org/abs/2601.10093v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.10691",
    "type": "report",
    "title": "The Conversational Exam: A Scalable Assessment Design for the AI Era",
    "author": [
      {
        "family": "Barba",
        "given": "Lorena A."
      },
      {
        "family": "Stegner",
        "given": "Laura"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          15
        ]
      ]
    },
    "abstract": "Traditional assessment methods collapse when students use generative AI to complete work without genuine engagement, creating an illusion of competence where they believe they're learning but aren't. This paper presents the conversational exam -- a scalable oral examination format that restores assessment validity by having students code live while explaining their reasoning. Drawing on human-computer interaction principles, we examined 58 students in small groups across just two days, demonstrating that oral exams can scale to typical class sizes. The format combines authentic practice (students work with documentation and supervised AI access) with inherent validity (real-time performance cannot be faked). We provide detailed implementation guidance to help instructors adapt this approach, offering a practical path forward when many educators feel paralyzed between banning AI entirely or accepting that valid assessment is impossible.",
    "URL": "http://arxiv.org/abs/2601.10691v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.10852",
    "type": "report",
    "title": "Gamifying Cyber Governance: A Virtual Escape Room to Transform Cybersecurity Policy Education",
    "author": [
      {
        "family": "Hasan",
        "given": "Khondokar Fida"
      },
      {
        "family": "Hughes",
        "given": "William"
      },
      {
        "family": "Rahman",
        "given": "Adrita"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          15
        ]
      ]
    },
    "abstract": "Serious games are gaining popularity as effective teaching and learning tools, providing engaging, interactive, and practical experiences for students. Gamified learning experiences, such as virtual escape rooms, have emerged as powerful tools in bridging theory and practice, fostering deeper understanding and engagement among students. This paper presents the design, implementation, and evaluation of a virtual escape room tailored specifically for cybersecurity governance and policy education. Developed as a 3D immersive environment, the escape room simulates a virtual company scenario to facilitate risk-informed cyber policy development. It consists of three interactive zones, each offering distinct sets of scenario-based problems that target specific educational objectives. Through these zones, students analyze cybersecurity risks, match security frameworks, and draft appropriate policies, thereby developing critical thinking, decision-making skills, and practical cybersecurity competencies. The primary contribution of this work lies in its innovative integration of game-based learning and immersive technology to create robust, interactive learning materials that are also resilient to generative AI interventions, thereby maintaining academic integrity. Additionally, the escape room provides students with exposure to real-world cybersecurity scenarios in a virtual office environment that meets industry expectations. Results from a student survey indicated strong positive feedback, highlighting significant improvements in students engagement, practical understanding, and enthusiasm toward cybersecurity governance and policy concepts, underscoring the effectiveness and potential of gamification in cybersecurity education.",
    "URL": "http://arxiv.org/abs/2601.10852v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.10983",
    "type": "report",
    "title": "Evaluating 21st-Century Competencies in Postsecondary Curricula with Large Language Models: Performance Benchmarking and Reasoning-Based Prompting Strategies",
    "author": [
      {
        "family": "Xu",
        "given": "Zhen"
      },
      {
        "family": "Guan",
        "given": "Xin"
      },
      {
        "family": "Shi",
        "given": "Chenxi"
      },
      {
        "family": "Chen",
        "given": "Qinhao"
      },
      {
        "family": "Yu",
        "given": "Renzhe"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          16
        ]
      ]
    },
    "abstract": "The growing emphasis on 21st-century competencies in postsecondary education, intensified by the transformative impact of generative AI, underscores the need to evaluate how these competencies are embedded in curricula and how effectively academic programs align with evolving workforce and societal demands. Curricular Analytics, particularly recent generative AI-powered approaches, offer a promising data-driven pathway. However, analyzing 21st-century competencies requires pedagogical reasoning beyond surface-level information retrieval, and the capabilities of large language models in this context remain underexplored. In this study, we extend prior curricular analytics research by examining a broader range of curriculum documents, competency frameworks, and models. Using 7,600 manually annotated curriculum-competency alignment scores, we assess the informativeness of different curriculum sources, benchmark general-purpose LLMs for curriculum-to-competency mapping, and analyze error patterns. We further introduce a reasoning-based prompting strategy, Curricular CoT, to strengthen LLMs' pedagogical reasoning. Our results show that detailed instructional activity descriptions are the most informative type of curriculum document for competency analytics. Open-weight LLMs achieve accuracy comparable to proprietary models on coarse-grained tasks, demonstrating their scalability and cost-effectiveness for institutional use. However, no model reaches human-level precision in fine-grained pedagogical reasoning. Our proposed Curricular CoT yields modest improvements by reducing bias in instructional keyword inference and improving the detection of nuanced pedagogical evidence in long text. Together, these findings highlight the untapped potential of institutional curriculum documents and provide an empirical foundation for advancing AI-driven curricular analytics.",
    "URL": "http://arxiv.org/abs/2601.10983v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.10986",
    "type": "report",
    "title": "ZPD Detector: Data Selection via Capability-Difficulty Alignment for Large Language Models",
    "author": [
      {
        "family": "Yang",
        "given": "Bo"
      },
      {
        "family": "Chen",
        "given": "Yunkui"
      },
      {
        "family": "Feng",
        "given": "Lanfei"
      },
      {
        "family": "Zhang",
        "given": "Yu"
      },
      {
        "family": "Li",
        "given": "Shijian"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          16
        ]
      ]
    },
    "abstract": "As the cost of training large language models continues to increase and high-quality training data become increasingly scarce, selecting high-value samples or synthesizing effective training data under limited data budgets has emerged as a critical research problem. Most existing data selection methods rely on static criteria, such as difficulty, uncertainty, or heuristics, and fail to model the evolving relationship between the model and the data. Inspired by the educational theory of the Zone of Proximal Development (ZPD), we propose ZPD Detector, a data selection framework that adopts a bidirectional perspective between models and data by explicitly modeling the alignment between sample difficulty and the model's current capability. ZPD Detector integrates difficulty calibration, model capability estimation based on Item Response Theory (IRT), and a capability-difficulty matching score to dynamically identify the most informative samples at each learning stage, improving data utilization efficiency; moreover, this dynamic matching strategy provides new insights into training strategy design. All code and data will be released after our work be accepted to support reproducible researc",
    "URL": "http://arxiv.org/abs/2601.10986v1",
    "publisher": "arXiv"
  },
  {
    "id": "arxiv:2601.11060",
    "type": "report",
    "title": "Children's Expectations, Engagement, and Evaluation of an LLM-enabled Spherical Visualization Platform in the Classroom",
    "author": [
      {
        "family": "F√§lton",
        "given": "Emelie"
      },
      {
        "family": "Str√∂mstedt",
        "given": "Isabelle"
      },
      {
        "family": "Brossier",
        "given": "Mathis"
      },
      {
        "family": "G√∂ransson",
        "given": "Andreas"
      },
      {
        "family": "Sch√∂nborn",
        "given": "Konrad"
      },
      {
        "family": "Loutfi",
        "given": "Amy"
      },
      {
        "family": "Sunden",
        "given": "Erik"
      },
      {
        "family": "Jawad",
        "given": "Mujtaba Fadhil"
      },
      {
        "family": "Suleiman",
        "given": "Yadgar"
      },
      {
        "family": "Bj√∂rklund",
        "given": "Johanna"
      },
      {
        "family": "Romero",
        "given": "Mario"
      },
      {
        "family": "Ynnerman",
        "given": "Anders"
      },
      {
        "family": "Besan√ßon",
        "given": "Lonni"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2026,
          1,
          16
        ]
      ]
    },
    "abstract": "We present our first stage results from deploying an LLM-augmented visualization software in a classroom setting to engage primary school children with earth-related datasets. Motivated by the growing interest in conversational AI as a means to support inquiry-based learning, we investigate children's expectations, engagement, and evaluation of a spoken LLM interface with a shared, immersive visualization system in a formal educational context. Our system integrates a speech-capable large language model with an interactive spherical display. It enables children to ask natural-language questions and receive coordinated verbal explanations and visual responses through the LLM-augmented visualization updating in real time based on spoken queries. We report on a classroom study with Swedish children aged 9-10, combining structured observation and small-group discussions to capture expectations prior to interaction, interaction patterns during facilitated sessions, and children's reflections on their encounter afterward. Our results provide empirical insights into children's initial encounters with an LLM-enabled visualization platform within a classroom setting and their expectations, interactions, and evaluations of the system. These findings inform the technology's potential for educational use and highlight important directions for future research.",
    "URL": "http://arxiv.org/abs/2601.11060v1",
    "publisher": "arXiv"
  }
]