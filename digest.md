# LLM Education Literature Digest

*Generated: 2026-02-02 07:19:08 UTC*

**32 new items**

## 1. Dynamic Framework for Collaborative Learning: Leveraging Advanced LLM with Adaptive Feedback Mechanisms

**Authors:** Hassam Tahir, Faizan Faisal, Fady Alnajjar, Muhammad Imran Taj, Lucia Gordon et al. (8 authors)

**Source:** arxiv • 2026-01-29

**Summary:** This paper presents a framework for integrating LLM into collaborative learning platforms to enhance student engagement, critical thinking, and inclusivity. The framework employs advanced LLMs as dynamic moderators to facilitate real-time discussions and adapt to learners' evolving needs, ensuring d...

**Links:**
- Landing page: http://arxiv.org/abs/2601.21344v1
- PDF: https://arxiv.org/pdf/2601.21344v1
- DOI: https://doi.org/10.1109/ABC64332.2025.11118419

*Relevance score: 18.0*

---

## 2. TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability in Large Language Models

**Authors:** Zheng Li, Siyao Song, Jingyuan Ma, Rui Li, Ying Zeng et al. (7 authors)

**Source:** arxiv • 2026-01-29

**Summary:** Large language models (LLMs) show promise as teaching assistants, yet their teaching capability remains insufficiently evaluated. Existing benchmarks mainly focus on problem-solving or problem-level guidance, leaving knowledge-centered teaching underexplored. We propose a syllabus-grounded evaluatio...

**Links:**
- Landing page: http://arxiv.org/abs/2601.21375v1
- PDF: https://arxiv.org/pdf/2601.21375v1

*Relevance score: 17.0*

---

## 3. From Atoms to Chains: Divergence-Guided Reasoning Curriculum for Unlabeled LLM Domain Adaptation

**Authors:** Yongqi Wang, Xiaofeng Ji, Jie Wang, Qingbin Li, Xiao Xiong et al. (9 authors)

**Source:** arxiv • 2026-01-27

**Summary:** Adapting Large Language Models (LLMs) to specialized domains without human-annotated data is a crucial yet formidable challenge. Widely adopted knowledge distillation methods often devolve into coarse-grained mimicry, where the student model inefficiently targets its own weaknesses and risks inherit...

**Links:**
- Landing page: http://arxiv.org/abs/2601.19588v1
- PDF: https://arxiv.org/pdf/2601.19588v1

*Relevance score: 17.0*

---

## 4. Learning to Live with AI: How Students Develop AI Literacy Through Naturalistic ChatGPT Interaction

**Authors:** Tawfiq Ammari, Meilun Chen, S M Mehedi Zaman, Kiran Garimella

**Source:** arxiv • 2026-01-28

**Summary:** How do students develop AI literacy through everyday practice rather than formal instruction? While normative AI literacy frameworks proliferate, empirical understanding of how students actually learn to work with generative AI remains limited. This study analyzes 10,536 ChatGPT messages from 36 und...

**Links:**
- Landing page: http://arxiv.org/abs/2601.20749v1
- PDF: https://arxiv.org/pdf/2601.20749v1

*Relevance score: 16.0*

---

## 5. Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning

**Authors:** Shuhui Qu

**Source:** arxiv • 2026-01-27

**Summary:** Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \textbf{Self-Querying Bidirectiona...

**Links:**
- Landing page: http://arxiv.org/abs/2601.20014v1
- PDF: https://arxiv.org/pdf/2601.20014v1

*Relevance score: 15.0*

---

## 6. LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?

**Authors:** Zhuang Yu, Lei Shen, Jing Zhao, Shiliang Sun

**Source:** arxiv • 2026-01-27

**Summary:** Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lectur...

**Links:**
- Landing page: http://arxiv.org/abs/2601.20705v1
- PDF: https://arxiv.org/pdf/2601.20705v1

*Relevance score: 15.0*

---

## 7. Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics

**Authors:** Mohamed Elgaar, Hadi Amiri

**Source:** arxiv • 2026-01-29

**Summary:** Curriculum learning changes the order of pre-training data, but it remains unclear whether it changes the learning trajectory or mainly reorders exposure over a fixed trajectory. We train Pythia models (14M-410M parameters) for 300B tokens under three linguistically motivated curricula-Age-of-Acquis...

**Links:**
- Landing page: http://arxiv.org/abs/2601.21698v1
- PDF: https://arxiv.org/pdf/2601.21698v1

*Relevance score: 14.0*

---

## 8. GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback

**Authors:** James Sungarda, Hongkai Liu, Zilong Zhou, Tien-Hsuan Wu, Johnson Chun-Sing Cheung et al. (6 authors)

**Source:** arxiv • 2026-01-26

**Summary:** Field education is the signature pedagogy of social work, yet providing timely and objective feedback during training is constrained by the availability of instructors and counseling clients. In this paper, we present SWITCH, the Social Work Interactive Training Chatbot. SWITCH integrates realistic ...

**Links:**
- Landing page: http://arxiv.org/abs/2601.18517v1
- PDF: https://arxiv.org/pdf/2601.18517v1

*Relevance score: 14.0*

---

## 9. DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding

**Authors:** Jiaming Zhou, Xuxin Cheng, Shiwan Zhao, Yuhang Jia, Cao Liu et al. (8 authors)

**Source:** arxiv • 2026-01-30

**Summary:** Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language model...

**Links:**
- Landing page: http://arxiv.org/abs/2601.23161v1
- PDF: https://arxiv.org/pdf/2601.23161v1

*Relevance score: 11.0*

---

## 10. Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery

**Authors:** Xinyi Ke, Kai Li, Junliang Xing, Yifan Zhang, Jian Cheng

**Source:** arxiv • 2026-01-30

**Summary:** Large language models (LLMs) have enabled rapid progress in automatic heuristic discovery (AHD), yet most existing methods are predominantly limited by static evaluation against fixed instance distributions, leading to potential overfitting and poor generalization under distributional shifts. We pro...

**Links:**
- Landing page: http://arxiv.org/abs/2601.22896v1
- PDF: https://arxiv.org/pdf/2601.22896v1

*Relevance score: 11.0*

---

## 11. TTCS: Test-Time Curriculum Synthesis for Self-Evolving

**Authors:** Chengyi Yang, Zhishang Xiang, Yunbo Tang, Zongpei Teng, Chengsong Huang et al. (8 authors)

**Source:** arxiv • 2026-01-30

**Summary:** Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield hi...

**Links:**
- Landing page: http://arxiv.org/abs/2601.22628v1
- PDF: https://arxiv.org/pdf/2601.22628v1

*Relevance score: 11.0*

---

## 12. When LLM meets Fuzzy-TOPSIS for Personnel Selection through Automated Profile Analysis

**Authors:** Shahria Hoque, Ahmed Akib Jawad Karim, Md. Golam Rabiul Alam, Nirjhar Gope

**Source:** arxiv • 2026-01-30

**Summary:** In this highly competitive employment environment, the selection of suitable personnel is essential for organizational success. This study presents an automated personnel selection system that utilizes sophisticated natural language processing (NLP) methods to assess and rank software engineering ap...

**Links:**
- Landing page: http://arxiv.org/abs/2601.22433v1
- PDF: https://arxiv.org/pdf/2601.22433v1
- DOI: https://doi.org/10.1109/ACCESS.2026.3658575

*Relevance score: 11.0*

---

## 13. A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition

**Authors:** Hoang Khang Phan, Quang Vinh Dang, Noriyo Colley, Christina Garcia, Nhat Tan Le

**Source:** arxiv • 2026-01-29

**Summary:** Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and fee...

**Links:**
- Landing page: http://arxiv.org/abs/2601.21802v1
- PDF: https://arxiv.org/pdf/2601.21802v1

*Relevance score: 11.0*

---

## 14. TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning

**Authors:** Huiyuan Lai, Malvina Nissim

**Source:** arxiv • 2026-01-29

**Summary:** Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with re...

**Links:**
- Landing page: http://arxiv.org/abs/2601.21711v1
- PDF: https://arxiv.org/pdf/2601.21711v1

*Relevance score: 11.0*

---

## 15. CORE: Collaborative Reasoning via Cross Teaching

**Authors:** Kshitij Mishra, Mirat Aubakirov, Martin Takac, Nils Lukas, Salem Lahlou

**Source:** arxiv • 2026-01-29

**Summary:** Large language models exhibit complementary reasoning errors: on the same instance, one model may succeed with a particular decomposition while another fails. We propose Collaborative Reasoning (CORE), a training-time collaboration framework that converts peer success into a learning signal via a cr...

**Links:**
- Landing page: http://arxiv.org/abs/2601.21600v1
- PDF: https://arxiv.org/pdf/2601.21600v1

*Relevance score: 11.0*

---

## 16. Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs

**Authors:** Haochen Zhang, Animesh Sinha, Felix Juefei-Xu, Haoyu Ma, Kunpeng Li et al. (11 authors)

**Source:** arxiv • 2026-01-28

**Summary:** Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn be...

**Links:**
- Landing page: http://arxiv.org/abs/2601.20911v1
- PDF: https://arxiv.org/pdf/2601.20911v1

*Relevance score: 11.0*

---

## 17. On the Effectiveness of LLM-Specific Fine-Tuning for Detecting AI-Generated Text

**Authors:** Michał Gromadzki, Anna Wróblewska, Agnieszka Kaliska

**Source:** arxiv • 2026-01-27

**Summary:** The rapid progress of large language models has enabled the generation of text that closely resembles human writing, creating challenges for authenticity verification in education, publishing, and digital security. Detecting AI-generated text has therefore become a crucial technical and ethical issu...

**Links:**
- Landing page: http://arxiv.org/abs/2601.20006v1
- PDF: https://arxiv.org/pdf/2601.20006v1

*Relevance score: 11.0*

---

## 18. CaseMaster: Designing and Evaluating a Probe for Oral Case Presentation Training with LLM Assistance

**Authors:** Yang Ouyang, Yuansong Xu, Chang Jiang, Yifan Jin, Haoran Jiang et al. (6 authors)

**Source:** arxiv • 2026-01-27

**Summary:** Preparing an oral case presentation (OCP) is a crucial skill for medical students, requiring clear communication of patient information, clinical findings, and treatment plans. However, inconsistent student participation and limited guidance can make this task challenging. While Large Language Model...

**Links:**
- Landing page: http://arxiv.org/abs/2601.19332v1
- PDF: https://arxiv.org/pdf/2601.19332v1

*Relevance score: 11.0*

---

## 19. Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning

**Authors:** Kishan Panaganti, Zhenwen Liang, Wenhao Yu, Haitao Mi, Dong Yu

**Source:** arxiv • 2026-01-27

**Summary:** Recent progress in Large Language Model (LLM) reasoning is increasingly driven by the refinement of post-training loss functions and alignment strategies. However, standard Reinforcement Learning (RL) paradigms like Group Relative Policy Optimization (GRPO) remain constrained by static uniformity: u...

**Links:**
- Landing page: http://arxiv.org/abs/2601.19280v1
- PDF: https://arxiv.org/pdf/2601.19280v1

*Relevance score: 11.0*

---

## 20. Proactive Hardening of LLM Defenses with HASTE

**Authors:** Henry Chen, Victor Aranda, Samarth Keshari, Ryan Heartfield, Nicole Nichols

**Source:** arxiv • 2026-01-27

**Summary:** Prompt-based attack techniques are one of the primary challenges in securely deploying and protecting LLM-based AI systems. LLM inputs are an unbounded, unstructured space. Consequently, effectively defending against these attacks requires proactive hardening strategies capable of continuously gener...

**Links:**
- Landing page: http://arxiv.org/abs/2601.19051v1
- PDF: https://arxiv.org/pdf/2601.19051v1

*Relevance score: 11.0*

---

## 21. Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability

**Authors:** Shobhita Sundaram, John Quan, Ariel Kwiatkowski, Kartik Ahuja, Yann Ollivier et al. (6 authors)

**Source:** arxiv • 2026-01-26

**Summary:** Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to gener...

**Links:**
- Landing page: http://arxiv.org/abs/2601.18778v1
- PDF: https://arxiv.org/pdf/2601.18778v1

*Relevance score: 11.0*

---

## 22. Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models

**Authors:** Siyan Zhao, Zhihui Xie, Mengchen Liu, Jing Huang, Guan Pang et al. (7 authors)

**Source:** arxiv • 2026-01-26

**Summary:** Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addres...

**Links:**
- Landing page: http://arxiv.org/abs/2601.18734v1
- PDF: https://arxiv.org/pdf/2601.18734v1

*Relevance score: 11.0*

---

## 23. LLAMA LIMA: A Living Meta-Analysis on the Effects of Generative AI on Learning Mathematics

**Authors:** Anselm Strohmaier, Samira Bödefeld, Frank Reinhold

**Source:** arxiv • 2026-01-26

**Summary:** The capabilities of generative AI in mathematics education are rapidly evolving, posing significant challenges for research to keep pace. Research syntheses remain scarce and risk being outdated by the time of publication. To address this issue, we present a Living Meta-Analysis (LIMA) on the effect...

**Links:**
- Landing page: http://arxiv.org/abs/2601.18685v1
- PDF: https://arxiv.org/pdf/2601.18685v1

*Relevance score: 11.0*

---

## 24. LeanTutor: Towards a Verified AI Mathematical Proof Tutor

**Authors:** Manooshree Patel, Rayna Bhattacharyya, Thomas Lu, Arnav Mehta, Niels Voss et al. (7 authors)

**Source:** arxiv • 2026-01-24

**Summary:** This paper considers the development of an AI-based provably-correct mathematical proof tutor. While Large Language Models (LLMs) allow seamless communication in natural language, they are error prone. Theorem provers such as Lean allow for provable-correctness, but these are hard for students to le...

**Links:**
- Landing page: http://arxiv.org/abs/2601.17473v1
- PDF: https://arxiv.org/pdf/2601.17473v1

*Relevance score: 11.0*

---

## 25. Multi-Agent Learning Path Planning via LLMs

**Authors:** Haoxin Xu, Changyong Qi, Tong Liu, Bohao Zhang, Anna He et al. (8 authors)

**Source:** arxiv • 2026-01-24

**Summary:** The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address t...

**Links:**
- Landing page: http://arxiv.org/abs/2601.17346v1
- PDF: https://arxiv.org/pdf/2601.17346v1

*Relevance score: 11.0*

---

## 26. Ensuring Computer Science Learning in the AI Era: Open Generative AI Policies and Assignment-Driven Written Quizzes

**Authors:** Chan-Jin Chung

**Source:** arxiv • 2026-01-16

**Summary:** The widespread availability of generative artificial intelligence (GenAI) has created a pressing challenge in computer science (CS) education: how to incorporate powerful AI tools into programming coursework without undermining student learning through cognitive offloading. This paper presents an as...

**Links:**
- Landing page: http://arxiv.org/abs/2601.17024v1
- PDF: https://arxiv.org/pdf/2601.17024v1

*Relevance score: 11.0*

---

## 27. Co-Designing Digital Humans for Online Learning: A Framework for Human-AI Pedagogical Integration

**Authors:** Xiaokang Lei, Ching Christie Pang, Yuyang Jiang, Xin Tong, Pan Hui

**Source:** arxiv • 2026-01-24

**Summary:** Artificial intelligence (AI) and large language models (LLMs) are reshaping education, with virtual avatars emerging as digital teachers capable of enhancing engagement, sustaining attention, and addressing instructor shortages. Aligned with the Sustainable Development Goals (SDGs) for equitable qua...

**Links:**
- Landing page: http://arxiv.org/abs/2601.17434v1
- PDF: https://arxiv.org/pdf/2601.17434v1

*Relevance score: 10.0*

---

## 28. The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation

**Authors:** Devanshu Sahoo, Manish Prasad, Vasudev Majhi, Arjun Neekhra, Yash Sinha et al. (8 authors)

**Source:** arxiv • 2026-01-29

**Summary:** The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, m...

**Links:**
- Landing page: http://arxiv.org/abs/2601.21360v1
- PDF: https://arxiv.org/pdf/2601.21360v1

*Relevance score: 9.0*

---

## 29. Can We Improve Educational Diagram Generation with In-Context Examples? Not if a Hallucination Spoils the Bunch

**Authors:** Evanfiya Logacheva, Arto Hellas, Tsvetomila Mihaylova, Juha Sorva, Ava Heinonen et al. (6 authors)

**Source:** arxiv • 2026-01-28

**Summary:** Generative artificial intelligence (AI) has found a widespread use in computing education; at the same time, quality of generated materials raises concerns among educators and students. This study addresses this issue by introducing a novel method for diagram code generation with in-context examples...

**Links:**
- Landing page: http://arxiv.org/abs/2601.20476v1
- PDF: https://arxiv.org/pdf/2601.20476v1

*Relevance score: 9.0*

---

## 30. Editrail: Understanding AI Usage by Visualizing Student-AI Interaction in Code

**Authors:** Ashley Ge Zhang, Yan-Ru Jhou, Yinuo Yang, Shamita Rao, Maryam Arab et al. (7 authors)

**Source:** arxiv • 2026-01-27

**Summary:** Programming instructors have diverse philosophies about integrating generative AI into their classes. Some encourage students to use AI, while others restrict or forbid it. Regardless of their approach, all instructors benefit from understanding how their students actually use AI while writing code....

**Links:**
- Landing page: http://arxiv.org/abs/2601.20085v1
- PDF: https://arxiv.org/pdf/2601.20085v1

*Relevance score: 9.0*

---

## 31. Designing AI Peers for Collaborative Mathematical Problem Solving with Middle School Students: A Participatory Design Study

**Authors:** Wenhan Lyu, Yimeng Wang, Murong Yue, Yifan Sun, Jennifer Suh et al. (8 authors)

**Source:** arxiv • 2026-01-25

**Summary:** Collaborative problem solving (CPS) is a fundamental practice in middle-school mathematics education; however, student groups frequently stall or struggle without ongoing teacher support. Recent work has explored how Generative AI tools can be designed to support one-on-one tutoring, but little is k...

**Links:**
- Landing page: http://arxiv.org/abs/2601.17962v2
- PDF: https://arxiv.org/pdf/2601.17962v2
- DOI: https://doi.org/10.1145/3772318.3791138

*Relevance score: 9.0*

---

## 32. MEIDNet: Multimodal generative AI framework for inverse materials design

**Authors:** Anand Babu, Rogério Almeida Gouvêa, Pierre Vandergheynst, Gian-Marco Rignanese

**Source:** arxiv • 2026-01-29

**Summary:** In this work, we present Multimodal Equivariant Inverse Design Network (MEIDNet), a framework that jointly learns structural information and materials properties through contrastive learning, while encoding structures via an equivariant graph neural network (EGNN). By combining generative inverse de...

**Links:**
- Landing page: http://arxiv.org/abs/2601.22009v1
- PDF: https://arxiv.org/pdf/2601.22009v1

*Relevance score: 8.0*

---
