# LLM Education Literature Digest

*Generated: 2026-01-12 06:56:02 UTC*

**27 new items**

## 1. What Students Ask, How a Generative AI Assistant Responds: Exploring Higher Education Students' Dialogues on Learning Analytics Feedback

**Authors:** Yildiz Uzun, Andrea Gauthier, Mutlu Cukurova

**Source:** arxiv • 2026-01-08

**Summary:** Learning analytics dashboards (LADs) aim to support students' regulation of learning by translating complex data into feedback. Yet students, especially those with lower self-regulated learning (SRL) competence, often struggle to engage with and interpret analytics feedback. Conversational generativ...

**Links:**
- Landing page: http://arxiv.org/abs/2601.04919v1
- PDF: https://arxiv.org/pdf/2601.04919v1

*Relevance score: 18.0*

---

## 2. FeedEval: Pedagogically Aligned Evaluation of LLM-Generated Essay Feedback

**Authors:** Seongyeub Chu, Jongwoo Kim, Munyong Yi

**Source:** arxiv • 2026-01-08

**Summary:** Going beyond the prediction of numerical scores, recent research in automated essay scoring has increasingly emphasized the generation of high-quality feedback that provides justification and actionable guidance. To mitigate the high cost of expert annotation, prior work has commonly relied on LLM-g...

**Links:**
- Landing page: http://arxiv.org/abs/2601.04574v1
- PDF: https://arxiv.org/pdf/2601.04574v1

*Relevance score: 17.0*

---

## 3. EDCO: Dynamic Curriculum Orchestration for Domain-specific Large Language Model Fine-tuning

**Authors:** Jing-Cheng Pang, Liu Sun, Chang Zhou, Xian Tang, Haichuan Ma et al. (13 authors)

**Source:** arxiv • 2026-01-07

**Summary:** Domain-specific large language models (LLMs), typically developed by fine-tuning a pre-trained general-purpose LLM on specialized datasets, represent a significant advancement in applied AI. A common strategy in LLM fine-tuning is curriculum learning, which pre-orders training samples based on metri...

**Links:**
- Landing page: http://arxiv.org/abs/2601.03725v1
- PDF: https://arxiv.org/pdf/2601.03725v1

*Relevance score: 17.0*

---

## 4. Towards Valid Student Simulation with Large Language Models

**Authors:** Zhihao Yuan, Yunze Xiao, Ming Li, Weihao Xuan, Richard Tong et al. (7 authors)

**Source:** arxiv • 2026-01-09

**Summary:** This paper presents a conceptual and methodological framework for large language model (LLM) based student simulation in educational settings. The authors identify a core failure mode, termed the "competence paradox" in which broadly capable LLMs are asked to emulate partially knowledgeable learners...

**Links:**
- Landing page: http://arxiv.org/abs/2601.05473v1
- PDF: https://arxiv.org/pdf/2601.05473v1

*Relevance score: 15.0*

---

## 5. CodeEval: A pedagogical approach for targeted evaluation of code-trained Large Language Models

**Authors:** Danny Brahman, Mohammad Mahoor

**Source:** arxiv • 2026-01-06

**Summary:** Large Language Models (LLMs) are predominantly assessed based on their common sense reasoning, language comprehension, and logical reasoning abilities. While models trained in specialized domains like mathematics or coding have demonstrated remarkable advancements in logical reasoning, there remains...

**Links:**
- Landing page: http://arxiv.org/abs/2601.03432v1
- PDF: https://arxiv.org/pdf/2601.03432v1

*Relevance score: 15.0*

---

## 6. EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners

**Authors:** Shenqi Lu, Liangwei Zhang

**Source:** arxiv • 2026-01-03

**Summary:** In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an ...

**Links:**
- Landing page: http://arxiv.org/abs/2601.01196v1
- PDF: https://arxiv.org/pdf/2601.01196v1

*Relevance score: 15.0*

---

## 7. Feedback Indices to Evaluate LLM Responses to Rebuttals for Multiple Choice Type Questions

**Authors:** Justin C. Dunlap, Anne-Simone Parent, Ralf Widenhorn

**Source:** arxiv • 2026-01-02

**Summary:** We present a systematic framework of indices designed to characterize Large Language Model (LLM) responses when challenged with rebuttals during a chat. Assessing how LLMs respond to user dissent is crucial for understanding their reliability and behavior patterns, yet the complexity of human-LLM in...

**Links:**
- Landing page: http://arxiv.org/abs/2601.03285v1
- PDF: https://arxiv.org/pdf/2601.03285v1

*Relevance score: 15.0*

---

## 8. CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning

**Authors:** Alexandra Dragomir, Florin Brad, Radu Tudor Ionescu

**Source:** arxiv • 2026-01-09

**Summary:** Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given du...

**Links:**
- Landing page: http://arxiv.org/abs/2601.05858v1
- PDF: https://arxiv.org/pdf/2601.05858v1

*Relevance score: 11.0*

---

## 9. Readability-Robust Code Summarization via Meta Curriculum Learning

**Authors:** Wenhao Zeng, Yitian Chai, Hao Zhou, Fandong Meng, Jie Zhou et al. (6 authors)

**Source:** arxiv • 2026-01-09

**Summary:** Code summarization has emerged as a fundamental technique in the field of program comprehension. While code language models have shown significant advancements, the current models and benchmarks are confined to high-readability code, which contains sufficient semantic cues such as function and varia...

**Links:**
- Landing page: http://arxiv.org/abs/2601.05485v1
- PDF: https://arxiv.org/pdf/2601.05485v1

*Relevance score: 11.0*

---

## 10. Large Language Models Are Bad Dice Players: LLMs Struggle to Generate Random Numbers from Statistical Distributions

**Authors:** Minda Zhao, Yilun Du, Mengyu Wang

**Source:** arxiv • 2026-01-08

**Summary:** As large language models (LLMs) transition from chat interfaces to integral components of stochastic pipelines across domains like educational assessment and synthetic data construction, the ability to faithfully sample from specified probability distributions has become a functional requirement rat...

**Links:**
- Landing page: http://arxiv.org/abs/2601.05414v1
- PDF: https://arxiv.org/pdf/2601.05414v1

*Relevance score: 11.0*

---

## 11. SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning

**Authors:** Yanchang Liang, Xiaowei Zhao

**Source:** arxiv • 2026-01-08

**Summary:** Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, ...

**Links:**
- Landing page: http://arxiv.org/abs/2601.05187v1
- PDF: https://arxiv.org/pdf/2601.05187v1

*Relevance score: 11.0*

---

## 12. CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs

**Authors:** Arthur Nijdam, Harri Kähkönen, Valtteri Niemi, Paul Stankovski Wagner, Sara Ramezanian

**Source:** arxiv • 2026-01-08

**Summary:** The cybersecurity landscape is constantly evolving, driven by increased digitalization and new cybersecurity threats. Cybersecurity programs often fail to equip graduates with skills demanded by the workforce, particularly concerning recent developments in cybersecurity, as curriculum design is cost...

**Links:**
- Landing page: http://arxiv.org/abs/2601.04940v1
- PDF: https://arxiv.org/pdf/2601.04940v1

*Relevance score: 11.0*

---

## 13. LLMs-Integrated Automatic Hate Speech Recognition Using Controllable Text Generation Models

**Authors:** Ryutaro Oshima, Yuya Hosoda, Youji Iiguni

**Source:** arxiv • 2026-01-08

**Summary:** This paper proposes an automatic speech recognition (ASR) model for hate speech using large language models (LLMs). The proposed method integrates the encoder of the ASR model with the decoder of the LLMs, enabling simultaneous transcription and censorship tasks to prevent the exposure of harmful co...

**Links:**
- Landing page: http://arxiv.org/abs/2601.04654v1
- PDF: https://arxiv.org/pdf/2601.04654v1

*Relevance score: 11.0*

---

## 14. Simulated Students in Tutoring Dialogues: Substance or Illusion?

**Authors:** Alexander Scarlatos, Jaewook Lee, Simon Woodhead, Andrew Lan

**Source:** arxiv • 2026-01-07

**Summary:** Advances in large language models (LLMs) enable many new innovations in education. However, evaluating the effectiveness of new technology requires real students, which is time-consuming and hard to scale up. Therefore, many recent works on LLM-powered tutoring solutions have used simulated students...

**Links:**
- Landing page: http://arxiv.org/abs/2601.04025v1
- PDF: https://arxiv.org/pdf/2601.04025v1

*Relevance score: 11.0*

---

## 15. Women Worry, Men Adopt: How Gendered Perceptions Shape the Use of Generative AI

**Authors:** Fabian Stephany, Jedrzej Duszynski

**Source:** arxiv • 2026-01-07

**Summary:** Generative artificial intelligence (GenAI) is diffusing rapidly, yet its adoption is strikingly unequal. Using nationally representative UK survey data from 2023 to 2024, we show that women adopt GenAI substantially less often than men because they perceive its societal risks differently. We constru...

**Links:**
- Landing page: http://arxiv.org/abs/2601.03880v1
- PDF: https://arxiv.org/pdf/2601.03880v1

*Relevance score: 11.0*

---

## 16. What Does Loss Optimization Actually Teach, If Anything? Knowledge Dynamics in Continual Pre-training of LLMs

**Authors:** Seyed Mahed Mousavi, Simone Alghisi, Giuseppe Riccardi

**Source:** arxiv • 2026-01-07

**Summary:** Continual Pre-Training (CPT) is widely used for acquiring and updating factual knowledge in LLMs. This practice treats loss as a proxy for knowledge learning, while offering no grounding into how it changes during training. We study CPT as a knowledge learning process rather than a solely optimizati...

**Links:**
- Landing page: http://arxiv.org/abs/2601.03858v1
- PDF: https://arxiv.org/pdf/2601.03858v1

*Relevance score: 11.0*

---

## 17. LLM-MC-Affect: LLM-Based Monte Carlo Modeling of Affective Trajectories and Latent Ambiguity for Interpersonal Dynamic Insight

**Authors:** Yu-Zheng Lin, Bono Po-Jen Shih, John Paul Martin Encinas, Elizabeth Victoria Abraham Achom, Karan Himanshu Patel et al. (10 authors)

**Source:** arxiv • 2026-01-07

**Summary:** Emotional coordination is a core property of human interaction that shapes how relational meaning is constructed in real time. While text-based affect inference has become increasingly feasible, prior approaches often treat sentiment as a deterministic point estimate for individual speakers, failing...

**Links:**
- Landing page: http://arxiv.org/abs/2601.03645v1
- PDF: https://arxiv.org/pdf/2601.03645v1

*Relevance score: 11.0*

---

## 18. Automated Feedback Generation for Undergraduate Mathematics: Development and Evaluation of an AI Teaching Assistant

**Authors:** Aron Gohr, Marie-Amelie Lawn, Kevin Gao, Inigo Serjeant, Stephen Heslip

**Source:** arxiv • 2026-01-06

**Summary:** Intelligent tutoring systems have long enabled automated immediate feedback on student work when it is presented in a tightly structured format and when problems are very constrained, but reliably assessing free-form mathematical reasoning remains challenging.
  We present a system that processes fr...

**Links:**
- Landing page: http://arxiv.org/abs/2601.03458v1
- PDF: https://arxiv.org/pdf/2601.03458v1

*Relevance score: 11.0*

---

## 19. Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning

**Authors:** Xinglang Zhang, Yunyao Zhang, ZeLiang Chen, Junqing Yu, Wei Yang et al. (6 authors)

**Source:** arxiv • 2026-01-06

**Summary:** Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning unde...

**Links:**
- Landing page: http://arxiv.org/abs/2601.02902v1
- PDF: https://arxiv.org/pdf/2601.02902v1

*Relevance score: 11.0*

---

## 20. AI-exposed jobs deteriorated before ChatGPT

**Authors:** Morgan R. Frank, Alireza Javadian Sabet, Lisa Simon, Sarah H. Bana, Renzhe Yu

**Source:** arxiv • 2026-01-05

**Summary:** Public debate links worsening job prospects for AI-exposed occupations to the release of ChatGPT in late 2022. Using monthly U.S. unemployment insurance records, we measure occupation- and location-specific unemployment risk and find that risk rose in AI-exposed occupations beginning in early 2022, ...

**Links:**
- Landing page: http://arxiv.org/abs/2601.02554v1
- PDF: https://arxiv.org/pdf/2601.02554v1

*Relevance score: 11.0*

---

## 21. LACONIC: Dense-Level Effectiveness for Scalable Sparse Retrieval via a Two-Phase Training Curriculum

**Authors:** Zhichao Xu, Shengyao Zhuang, Crystina Zhang, Xueguang Ma, Yijun Tian et al. (8 authors)

**Source:** arxiv • 2026-01-04

**Summary:** While dense retrieval models have become the standard for state-of-the-art information retrieval, their deployment is often constrained by high memory requirements and reliance on GPU accelerators for vector similarity search. Learned sparse retrieval offers a compelling alternative by enabling effi...

**Links:**
- Landing page: http://arxiv.org/abs/2601.01684v1
- PDF: https://arxiv.org/pdf/2601.01684v1

*Relevance score: 11.0*

---

## 22. PCEval: A Benchmark for Evaluating Physical Computing Capabilities of Large Language Models

**Authors:** Inpyo Song, Eunji Jeon, Jangwon Lee

**Source:** arxiv • 2025-12-31

**Summary:** Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including software development, education, and technical assistance. Among these, software development is one of the key areas where LLMs are increasingly adopted. However, when hardware constraints are co...

**Links:**
- Landing page: http://arxiv.org/abs/2601.02404v1
- PDF: https://arxiv.org/pdf/2601.02404v1

*Relevance score: 11.0*

---

## 23. Can Consumer Chatbots Reason? A Student-Led Field Experiment Embedded in an "AI-for-All" Undergraduate Course

**Authors:** Amarda Shehu, Adonyas Ababu, Asma Akbary, Griffin Allen, Aroush Baig et al. (37 authors)

**Source:** arxiv • 2025-12-28

**Summary:** Claims about whether large language model (LLM) chatbots "reason" are typically debated using curated benchmarks and laboratory-style evaluation protocols. This paper offers a complementary perspective: a student-led field experiment embedded as a midterm project in UNIV 182 (AI4All) at George Mason...

**Links:**
- Landing page: http://arxiv.org/abs/2601.04225v1
- PDF: https://arxiv.org/pdf/2601.04225v1

*Relevance score: 11.0*

---

## 24. AgentTutor: Empowering Personalized Learning with Multi-Turn Interactive Teaching in Intelligent Education Systems

**Authors:** Yuxin Liu, Zeqing Song, Jiong Lou, Chentao Wu, Jie Li

**Source:** arxiv • 2025-12-24

**Summary:** The rapid advancement of large-scale language models (LLMs) has shown their potential to transform intelligent education systems (IESs) through automated teaching and learning support applications. However, current IESs often rely on single-turn static question-answering, which fails to assess learn...

**Links:**
- Landing page: http://arxiv.org/abs/2601.04219v1
- PDF: https://arxiv.org/pdf/2601.04219v1

*Relevance score: 11.0*

---

## 25. PhyEduVideo: A Benchmark for Evaluating Text-to-Video Models for Physics Education

**Authors:** Megha Mariam K. M, Aditya Arun, Zakaria Laskar, C. V. Jawahar

**Source:** arxiv • 2026-01-02

**Summary:** Generative AI models, particularly Text-to-Video (T2V) systems, offer a promising avenue for transforming science education by automating the creation of engaging and intuitive visual explanations. In this work, we take a first step toward evaluating their potential in physics education by introduci...

**Links:**
- Landing page: http://arxiv.org/abs/2601.00943v1
- PDF: https://arxiv.org/pdf/2601.00943v1

*Relevance score: 10.0*

---

## 26. Pilot Study on Student Public Opinion Regarding GAI

**Authors:** William Franz Lamberti, Sunbin Kim, Samantha Rose Lawrence

**Source:** arxiv • 2026-01-07

**Summary:** The emergence of generative AI (GAI) has sparked diverse opinions regarding its appropriate use across various domains, including education. This pilot study investigates university students' perceptions of GAI in higher education classrooms, aiming to lay the groundwork for understanding these atti...

**Links:**
- Landing page: http://arxiv.org/abs/2601.04336v1
- PDF: https://arxiv.org/pdf/2601.04336v1

*Relevance score: 9.0*

---

## 27. KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models

**Authors:** Zixian Liu, Sihao Liu, Yuqi Zhao

**Source:** arxiv • 2026-01-04

**Summary:** With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform task...

**Links:**
- Landing page: http://arxiv.org/abs/2601.01366v1
- PDF: https://arxiv.org/pdf/2601.01366v1

*Relevance score: 9.0*

---
