# LLM Education Literature Digest

*Generated: 2026-02-09 07:22:25 UTC*

**47 new items**

## 1. Knowledge Synthesis Graph: An LLM-Based Approach for Modeling Student Collaborative Discourse

**Authors:** Bo Shui, Xinran Zhu

**Source:** arxiv • 2026-02-05

**Summary:** Asynchronous, text-based discourse-such as students' posts in discussion forums-is widely used to support collaborative learning. However, the distributed and evolving nature of such discourse often makes it difficult to see how ideas connect, develop, and build on one another over time. As a result...

**Links:**
- Landing page: http://arxiv.org/abs/2602.06194v1
- PDF: https://arxiv.org/pdf/2602.06194v1

*Relevance score: 18.0*

---

## 2. Not All Students Engage Alike: Multi-Institution Patterns in GenAI Tutor Use

**Authors:** Youjie Chen, Xixi Shi, Xinyu Liu, Shuaiguo Wang, Tracy Xiao Liu et al. (6 authors)

**Source:** arxiv • 2026-01-31

**Summary:** The emergence of generative artificial intelligence (GenAI) has created unprecedented opportunities to provide individualized learning support in classrooms as automated tutoring systems at scale. However, concerns have been raised that students may engage with these tools in ways that do not suppor...

**Links:**
- Landing page: http://arxiv.org/abs/2602.00447v1
- PDF: https://arxiv.org/pdf/2602.00447v1

*Relevance score: 18.0*

---

## 3. T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation

**Authors:** Suhan Guo, Bingxu Wang, Shaodan Zhang, Furao Shen

**Source:** arxiv • 2026-02-02

**Summary:** Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effective...

**Links:**
- Landing page: http://arxiv.org/abs/2602.01937v1
- PDF: https://arxiv.org/pdf/2602.01937v1

*Relevance score: 17.0*

---

## 4. From Code-Centric to Concept-Centric: Teaching NLP with LLM-Assisted "Vibe Coding"

**Authors:** Hend Al-Khalifa

**Source:** arxiv • 2026-02-02

**Summary:** The rapid advancement of Large Language Models (LLMs) presents both challenges and opportunities for Natural Language Processing (NLP) education. This paper introduces ``Vibe Coding,'' a pedagogical approach that leverages LLMs as coding assistants while maintaining focus on conceptual understanding...

**Links:**
- Landing page: http://arxiv.org/abs/2602.01919v1
- PDF: https://arxiv.org/pdf/2602.01919v1

*Relevance score: 17.0*

---

## 5. Evidence-Decision-Feedback: Theory-Driven Adaptive Scaffolding for LLM Agents

**Authors:** Clayton Cohn, Siyuan Guo, Surya Rayala, Hanchen David Wang, Naveeduddin Mohammed et al. (14 authors)

**Source:** arxiv • 2026-02-01

**Summary:** Multi-agent LLM architectures offer opportunities for pedagogical agents to help students construct domain knowledge and develop critical-thinking skills, yet many operate on a "one-size-fits-all" basis, limiting their ability to provide personalized support. To address this, we introduce Evidence-D...

**Links:**
- Landing page: http://arxiv.org/abs/2602.01415v1
- PDF: https://arxiv.org/pdf/2602.01415v1

*Relevance score: 17.0*

---

## 6. Learning Context Matters: Measuring and Diagnosing Personalization Gaps in LLM-Based Instructional Design

**Authors:** Johaun Hatchett, Debshila Basu Mallick, Brittany C. Bradford, Richard G. Baraniuk

**Source:** arxiv • 2026-02-04

**Summary:** The adoption of generative AI in education has accelerated dramatically in recent years, with Large Language Models (LLMs) increasingly integrated into learning environments in the hope of providing personalized support that enhances learner engagement and knowledge retention. However, truly persona...

**Links:**
- Landing page: http://arxiv.org/abs/2602.04972v1
- PDF: https://arxiv.org/pdf/2602.04972v1

*Relevance score: 16.0*

---

## 7. CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models

**Authors:** Rui Jia, Ruiyi Lan, Fengrui Liu, Zhongxiang Dai, Bo Jiang et al. (10 authors)

**Source:** arxiv • 2026-02-05

**Summary:** Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and...

**Links:**
- Landing page: http://arxiv.org/abs/2602.05633v1
- PDF: https://arxiv.org/pdf/2602.05633v1

*Relevance score: 15.0*

---

## 8. Relying on LLMs: Student Practices and Instructor Norms are Changing in Computer Science Education

**Authors:** Xinrui Lin, Heyan Huang, Shumin Shi, John Vines

**Source:** arxiv • 2026-02-05

**Summary:** Prior research has raised concerns about students' over-reliance on large language models (LLMs) in higher education. This paper examines how Computer Science students and instructors engage with LLMs across five scenarios: "Writing", "Quiz", "Programming", "Project-based learning", and "Information...

**Links:**
- Landing page: http://arxiv.org/abs/2602.05506v1
- PDF: https://arxiv.org/pdf/2602.05506v1

*Relevance score: 15.0*

---

## 9. Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education

**Authors:** Adithya Kulkarni, Mohna Chakraborty, Jay Bagga

**Source:** arxiv • 2026-02-04

**Summary:** Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. Thi...

**Links:**
- Landing page: http://arxiv.org/abs/2602.05059v1
- PDF: https://arxiv.org/pdf/2602.05059v1

*Relevance score: 15.0*

---

## 10. ReasonCACHE: Teaching LLMs To Reason Without Weight Updates

**Authors:** Sharut Gupta, Phillip Isola, Stefanie Jegelka, David Lopez-Paz, Kartik Ahuja et al. (7 authors)

**Source:** arxiv • 2026-02-02

**Summary:** Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However,...

**Links:**
- Landing page: http://arxiv.org/abs/2602.02366v1
- PDF: https://arxiv.org/pdf/2602.02366v1

*Relevance score: 15.0*

---

## 11. Generative AI in Signal Processing Education: An Audio Foundation Model Based Approach

**Authors:** Muhammad Salman Khan, Ahmad Ullah, Siddique Latif, Junaid Qadir

**Source:** arxiv • 2026-02-01

**Summary:** Audio Foundation Models (AFMs), a specialized category of Generative AI (GenAI), have the potential to transform signal processing (SP) education by integrating core applications such as speech and audio enhancement, denoising, source separation, feature extraction, automatic classification, and rea...

**Links:**
- Landing page: http://arxiv.org/abs/2602.01249v1
- PDF: https://arxiv.org/pdf/2602.01249v1

*Relevance score: 15.0*

---

## 12. PedagoSense: A Pedology Grounded LLM System for Pedagogical Strategy Detection and Contextual Response Generation in Learning Dialogues

**Authors:** Shahem Sultan, Shahem Fadi, Yousef Melhim, Ibrahim Alsarraj, Besher Hassan

**Source:** arxiv • 2026-02-01

**Summary:** This paper addresses the challenge of improving interaction quality in dialogue based learning by detecting and recommending effective pedagogical strategies in tutor student conversations. We introduce PedagoSense, a pedology grounded system that combines a two stage strategy classifier with large ...

**Links:**
- Landing page: http://arxiv.org/abs/2602.01169v1
- PDF: https://arxiv.org/pdf/2602.01169v1

*Relevance score: 15.0*

---

## 13. Large Language Models as Students Who Think Aloud: Overly Coherent, Verbose, and Confident

**Authors:** Conrad Borchers, Jill-Jênn Vie, Roger Azevedo

**Source:** arxiv • 2026-02-01

**Summary:** Large language models (LLMs) are increasingly embedded in AI-based tutoring systems. Can they faithfully model novice reasoning and metacognitive judgments? Existing evaluations emphasize problem-solving accuracy, overlooking the fragmented and imperfect reasoning that characterizes human learning. ...

**Links:**
- Landing page: http://arxiv.org/abs/2602.01015v1
- PDF: https://arxiv.org/pdf/2602.01015v1

*Relevance score: 15.0*

---

## 14. Lessons Learned from Integrating Generative AI into an Introductory Undergraduate Astronomy Course at Harvard

**Authors:** Christopher W. Stubbs, Dongpeng Huang, Jungyoon Koh, Madeleine Woods, Andrés A. Plazas Malagón

**Source:** arxiv • 2026-02-04

**Summary:** We describe our efforts to fully integrate generative artificial intelligence (GAI) into an introductory undergraduate astronomy course. Ordered by student perception of utility, GAI was used in instructional Python notebooks, in a subset of assignments, for student presentation preparations, and as...

**Links:**
- Landing page: http://arxiv.org/abs/2602.04389v1
- PDF: https://arxiv.org/pdf/2602.04389v1

*Relevance score: 14.0*

---

## 15. Which course? Discourse! Teaching Discourse and Generation in the Era of LLMs

**Authors:** Junyi Jessy Li, Yang Janet Liu, Kanishka Misra, Valentina Pyatkin, William Sheffield

**Source:** arxiv • 2026-02-02

**Summary:** The field of NLP has undergone vast, continuous transformations over the past few years, sparking debates going beyond discipline boundaries. This begs important questions in education: how do we design courses that bridge sub-disciplines in this shifting landscape? This paper explores this question...

**Links:**
- Landing page: http://arxiv.org/abs/2602.02878v1
- PDF: https://arxiv.org/pdf/2602.02878v1

*Relevance score: 14.0*

---

## 16. Calibrating Generative AI to Produce Realistic Essays for Data Augmentation

**Authors:** Edward W. Wolfe, Justin O. Barber

**Source:** arxiv • 2026-02-06

**Summary:** Data augmentation can mitigate limited training data in machine-learning automated scoring engines for constructed response items. This study seeks to determine how well three approaches to large language model prompting produce essays that preserve the writing quality of the original essays and pro...

**Links:**
- Landing page: http://arxiv.org/abs/2602.06772v1
- PDF: https://arxiv.org/pdf/2602.06772v1

*Relevance score: 12.0*

---

## 17. Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank

**Authors:** Joshua Mitton, Prarthana Bhattacharyya, Digory Smith, Thomas Christie, Ralph Abboud et al. (6 authors)

**Source:** arxiv • 2026-02-02

**Summary:** Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconcepti...

**Links:**
- Landing page: http://arxiv.org/abs/2602.02414v1
- PDF: https://arxiv.org/pdf/2602.02414v1

*Relevance score: 12.0*

---

## 18. ClassAid: A Real-time Instructor-AI-Student Orchestration System for Classroom Programming Activities

**Authors:** Gefei Zhang, Guodao Sun, Meng Xia, Ronghua Liang

**Source:** arxiv • 2026-02-06

**Summary:** Generative AI is reshaping education, but it also raises concerns about instability and overreliance. In programming classrooms, we aim to leverage its feedback capabilities while reinforcing the educator's role in guiding student-AI interactions. We developed ClassAid, a real-time orchestration sys...

**Links:**
- Landing page: http://arxiv.org/abs/2602.06734v1
- PDF: https://arxiv.org/pdf/2602.06734v1

*Relevance score: 11.0*

---

## 19. Estimating Exam Item Difficulty with LLMs: A Benchmark on Brazil's ENEM Corpus

**Authors:** Thiago Brant, Julien Kühn, Jun Pang

**Source:** arxiv • 2026-02-06

**Summary:** As Large Language Models (LLMs) are increasingly deployed to generate educational content, a critical safety question arises: can these models reliably estimate the difficulty of the questions they produce? Using Brazil's high-stakes ENEM exam as a testbed, we benchmark ten proprietary and open-weig...

**Links:**
- Landing page: http://arxiv.org/abs/2602.06631v1
- PDF: https://arxiv.org/pdf/2602.06631v1

*Relevance score: 11.0*

---

## 20. FairJudge: An Adaptive, Debiased, and Consistent LLM-as-a-Judge

**Authors:** Bo Yang, Lanfei Feng, Yunkui Chen, Yu Zhang, Xiao Xu et al. (6 authors)

**Source:** arxiv • 2026-02-06

**Summary:** Existing LLM-as-a-Judge systems suffer from three fundamental limitations: limited adaptivity to task- and domain-specific evaluation criteria, systematic biases driven by non-semantic cues such as position, length, format, and model provenance, and evaluation inconsistency that leads to contradicto...

**Links:**
- Landing page: http://arxiv.org/abs/2602.06625v1
- PDF: https://arxiv.org/pdf/2602.06625v1

*Relevance score: 11.0*

---

## 21. Prompting Destiny: Negotiating Socialization and Growth in an LLM-Mediated Speculative Gameworld

**Authors:** Mandi Yang, Zhiqi Gao, Yibo Meng, Dongyijie Primo Pan

**Source:** arxiv • 2026-02-05

**Summary:** We present an LLM-mediated role-playing game that supports reflection on socialization, moral responsibility, and educational role positioning. Grounded in socialization theory, the game follows a four-season structure in which players guide a child prince through morally charged situations and comp...

**Links:**
- Landing page: http://arxiv.org/abs/2602.05864v1
- PDF: https://arxiv.org/pdf/2602.05864v1

*Relevance score: 11.0*

---

## 22. Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks

**Authors:** Chaimae Abouzahir, Congbo Ma, Nizar Habash, Farah E. Shamout

**Source:** arxiv • 2026-02-05

**Summary:** In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse commu...

**Links:**
- Landing page: http://arxiv.org/abs/2602.05374v1
- PDF: https://arxiv.org/pdf/2602.05374v1

*Relevance score: 11.0*

---

## 23. Evaluating the Presence of Sex Bias in Clinical Reasoning by Large Language Models

**Authors:** Isabel Tsintsiper, Sheng Wong, Beth Albert, Shaun P Brennecke, Gabriel Davis Jones

**Source:** arxiv • 2026-02-04

**Summary:** Large language models (LLMs) are increasingly embedded in healthcare workflows for documentation, education, and clinical decision support. However, these systems are trained on large text corpora that encode existing biases, including sex disparities in diagnosis and treatment, raising concerns tha...

**Links:**
- Landing page: http://arxiv.org/abs/2602.04392v1
- PDF: https://arxiv.org/pdf/2602.04392v1

*Relevance score: 11.0*

---

## 24. MiniRec: Data-Efficient Reinforcement Learning for LLM-based Recommendation

**Authors:** Lin Wang, Yang Zhang, Jingfan Chen, Xiaoyan Zhao, Fengbin Zhu et al. (7 authors)

**Source:** arxiv • 2026-02-04

**Summary:** The integration of reinforcement learning (RL) into large language models (LLMs) has opened new opportunities for recommender systems by eliciting reasoning and improving user preference modeling. However, RL-based LLM recommendation faces significant efficiency challenges, making full-data training...

**Links:**
- Landing page: http://arxiv.org/abs/2602.04278v1
- PDF: https://arxiv.org/pdf/2602.04278v1

*Relevance score: 11.0*

---

## 25. Automatic Classification of Pedagogical Materials against CS Curriculum Guidelines

**Authors:** Erik Saule, Kalpathi Subramanian, Razvan Bunescu

**Source:** arxiv • 2026-02-03

**Summary:** Professional societies often publish curriculum guidelines to help programs align their content to international standards. In Computer Science, the primary standard is published by ACM and IEEE and provide detailed guidelines for what should be and could be included in a Computer Science program.
 ...

**Links:**
- Landing page: http://arxiv.org/abs/2602.03962v1
- PDF: https://arxiv.org/pdf/2602.03962v1

*Relevance score: 11.0*

---

## 26. Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation

**Authors:** Aditya Basarkar, Benyamin Tabarsi, Tiffany Barnes, Dongkuan, Xu

**Source:** arxiv • 2026-02-03

**Summary:** Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems h...

**Links:**
- Landing page: http://arxiv.org/abs/2602.03950v1
- PDF: https://arxiv.org/pdf/2602.03950v1

*Relevance score: 11.0*

---

## 27. Can Large Language Models Generalize Procedures Across Representations?

**Authors:** Fangru Lin, Valentin Hofmann, Xingchen Wan, Weixing Wang, Zifeng Ding et al. (7 authors)

**Source:** arxiv • 2026-02-03

**Summary:** Large language models (LLMs) are trained and tested extensively on symbolic representations such as code and graphs, yet real-world user tasks are often specified in natural language. To what extent can LLMs generalize across these representations? Here, we approach this question by studying isomorp...

**Links:**
- Landing page: http://arxiv.org/abs/2602.03542v1
- PDF: https://arxiv.org/pdf/2602.03542v1

*Relevance score: 11.0*

---

## 28. Knowledge Model Prompting Increases LLM Performance on Planning Tasks

**Authors:** Erik Goh, John Kos, Ashok Goel

**Source:** arxiv • 2026-02-03

**Summary:** Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into que...

**Links:**
- Landing page: http://arxiv.org/abs/2602.03900v1
- PDF: https://arxiv.org/pdf/2602.03900v1

*Relevance score: 11.0*

---

## 29. Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning

**Authors:** Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Wenlei Shi et al. (8 authors)

**Source:** arxiv • 2026-02-03

**Summary:** Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to se...

**Links:**
- Landing page: http://arxiv.org/abs/2602.03249v1
- PDF: https://arxiv.org/pdf/2602.03249v1

*Relevance score: 11.0*

---

## 30. EventFlash: Towards Efficient MLLMs for Event-Based Vision

**Authors:** Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang et al. (7 authors)

**Source:** arxiv • 2026-02-03

**Summary:** Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of eve...

**Links:**
- Landing page: http://arxiv.org/abs/2602.03230v1
- PDF: https://arxiv.org/pdf/2602.03230v1

*Relevance score: 11.0*

---

## 31. AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback

**Authors:** Zhitao Gao, Jie Ma, Xuhong Li, Pengyu Li, Ning Qu et al. (8 authors)

**Source:** arxiv • 2026-02-03

**Summary:** Large Language Models (LLMs) have achieved significant success in complex reasoning but remain bottlenecked by reliance on expert-annotated data and external verifiers. While existing self-evolution paradigms aim to bypass these constraints, they often fail to identify the optimal learning zone and ...

**Links:**
- Landing page: http://arxiv.org/abs/2602.03084v2
- PDF: https://arxiv.org/pdf/2602.03084v2

*Relevance score: 11.0*

---

## 32. Reshaping Perception Through Technology: From Ancient Script to Large Language Models

**Authors:** Parham Pourdavood, Michael Jacob

**Source:** arxiv • 2026-02-02

**Summary:** As large language models reshape how we create and access information, questions arise about how to frame their role in human creative and cognitive life. We argue that AI is best understood not as artificial intelligence but as a new medium -- one that, like writing before it, reshapes perception a...

**Links:**
- Landing page: http://arxiv.org/abs/2602.02794v2
- PDF: https://arxiv.org/pdf/2602.02794v2

*Relevance score: 11.0*

---

## 33. S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs

**Authors:** Yanrui Du, Sendong Zhao, Yibo Gao, Danyang Zhao, Qika Lin et al. (12 authors)

**Source:** arxiv • 2026-02-02

**Summary:** Large language models (LLMs) equipped with chain-of-thought (CoT) achieve strong performance and offer a window into LLM behavior. However, recent evidence suggests that improvements in CoT capabilities often come with redundant reasoning processes, motivating a key question: Can LLMs acquire a fast...

**Links:**
- Landing page: http://arxiv.org/abs/2602.01982v1
- PDF: https://arxiv.org/pdf/2602.01982v1

*Relevance score: 11.0*

---

## 34. Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling

**Authors:** Fabrice Boissier, Monica Sen, Irina Rychkova

**Source:** arxiv • 2026-02-02

**Summary:** Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Anal...

**Links:**
- Landing page: http://arxiv.org/abs/2602.01933v1
- PDF: https://arxiv.org/pdf/2602.01933v1

*Relevance score: 11.0*

---

## 35. PretrainRL: Alleviating Factuality Hallucination of Large Language Models at the Beginning

**Authors:** Langming Liu, Kangtao Lv, Haibin Chen, Weidong Zhang, Yejing Wang et al. (11 authors)

**Source:** arxiv • 2026-02-02

**Summary:** Large language models (LLMs), despite their powerful capabilities, suffer from factual hallucinations where they generate verifiable falsehoods. We identify a root of this issue: the imbalanced data distribution in the pretraining corpus, which leads to a state of "low-probability truth" and "high-p...

**Links:**
- Landing page: http://arxiv.org/abs/2602.01875v1
- PDF: https://arxiv.org/pdf/2602.01875v1

*Relevance score: 11.0*

---

## 36. $\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality

**Authors:** Pengyu Li, Lingling Zhang, Zhitao Gao, Yanrui Wu, Yuxuan Dong et al. (8 authors)

**Source:** arxiv • 2026-02-02

**Summary:** While Large Language Models (LLMs) have achieved remarkable capabilities, they unintentionally memorize sensitive data, posing critical privacy and security risks. Machine unlearning is pivotal for mitigating these risks, yet existing paradigms face a fundamental dilemma: aggressive unlearning often...

**Links:**
- Landing page: http://arxiv.org/abs/2602.01703v1
- PDF: https://arxiv.org/pdf/2602.01703v1

*Relevance score: 11.0*

---

## 37. Argument Rarity-based Originality Assessment for AI-Assisted Writing

**Authors:** Keito Inoshita, Michiaki Omura, Tsukasa Yamanaka, Go Maeda, Kentaro Tsuji

**Source:** arxiv • 2026-02-02

**Summary:** As Large Language Models (LLMs) have become capable of effortlessly generating high-quality text, traditional quality-focused writing assessment is losing its significance. If the essential goal of education is to foster critical thinking and original perspectives, assessment must also shift its par...

**Links:**
- Landing page: http://arxiv.org/abs/2602.01560v1
- PDF: https://arxiv.org/pdf/2602.01560v1

*Relevance score: 11.0*

---

## 38. GradingAttack: Attacking Large Language Models Towards Short Answer Grading Ability

**Authors:** Xueyi Li, Zhuoneng Zhou, Zitao Liu, Yongdong Wu, Weiqi Luo

**Source:** arxiv • 2026-02-01

**Summary:** Large language models (LLMs) have demonstrated remarkable potential for automatic short answer grading (ASAG), significantly boosting student assessment efficiency and scalability in educational scenarios. However, their vulnerability to adversarial manipulation raises critical concerns about automa...

**Links:**
- Landing page: http://arxiv.org/abs/2602.00979v1
- PDF: https://arxiv.org/pdf/2602.00979v1

*Relevance score: 11.0*

---

## 39. WordCraft: Scaffolding the Keyword Method for L2 Vocabulary Learning with Multimodal LLMs

**Authors:** Yuheng Shao, Junjie Xiong, Chaoran Wu, Xiyuan Wang, Ziyu Zhou et al. (8 authors)

**Source:** arxiv • 2026-01-31

**Summary:** Applying the keyword method for vocabulary memorization remains a significant challenge for L1 Chinese-L2 English learners. They frequently struggle to generate phonologically appropriate keywords, construct coherent associations, and create vivid mental imagery to aid long-term retention. Existing ...

**Links:**
- Landing page: http://arxiv.org/abs/2602.00762v1
- PDF: https://arxiv.org/pdf/2602.00762v1

*Relevance score: 11.0*

---

## 40. "OpenBloom": A Question-Based LLM Tool to Support Stigma Reduction in Reproductive Well-Being

**Authors:** Ashley Hua, Adya Daruka, Yang Hong, Sharifa Sultana

**Source:** arxiv • 2026-01-30

**Summary:** Reproductive well-being education remains widely stigmatized across diverse cultural contexts, constraining how individuals access and interpret reproductive health knowledge. We designed and evaluated OpenBloom, a stigma-sensitive, AI-mediated system that uses LLMs to transform reproductive health ...

**Links:**
- Landing page: http://arxiv.org/abs/2602.00243v1
- PDF: https://arxiv.org/pdf/2602.00243v1

*Relevance score: 11.0*

---

## 41. BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks

**Authors:** Nishant Balepur, Bhavya Rajasekaran, Jane Oh, Michael Xie, Atrey Desai et al. (10 authors)

**Source:** arxiv • 2026-02-05

**Summary:** Multiple-choice question answering (MCQA) is standard in NLP, but benchmarks lack rigorous quality control. We present BenchMarker, an education-inspired toolkit using LLM judges to flag three common MCQ flaws: 1) contamination - items appearing exactly online; 2) shortcuts - cues in the choices tha...

**Links:**
- Landing page: http://arxiv.org/abs/2602.06221v1
- PDF: https://arxiv.org/pdf/2602.06221v1

*Relevance score: 9.0*

---

## 42. "It Talks Like a Patient, But Feels Different": Co-Designing AI Standardized Patients with Medical Learners

**Authors:** Zhiqi Gao, Guo Zhu, Huarui Luo, Dongyijie Primo Pan, Haoming Tang et al. (9 authors)

**Source:** arxiv • 2026-02-05

**Summary:** Standardized patients (SPs) play a central role in clinical communication training but are costly, difficult to scale, and inconsistent. Large language model (LLM) based AI standardized patients (AI-SPs) promise flexible, on-demand practice, yet learners often report that they talk like a patient bu...

**Links:**
- Landing page: http://arxiv.org/abs/2602.05856v1
- PDF: https://arxiv.org/pdf/2602.05856v1

*Relevance score: 9.0*

---

## 43. Exploring Emerging Norms of AI Disclosure in Programming Education

**Authors:** Runlong Ye, Oliver Huang, Jessica He, Michael Liut

**Source:** arxiv • 2026-02-03

**Summary:** Generative AI blurs the lines of authorship in computing education, creating uncertainty around how students should attribute AI assistance. To examine these emerging norms, we conducted a factorial vignette study with 94 computer science students across 102 unique scenarios, systematically manipula...

**Links:**
- Landing page: http://arxiv.org/abs/2602.04023v1
- PDF: https://arxiv.org/pdf/2602.04023v1

*Relevance score: 9.0*

---

## 44. Footprints of the Walking of Numbers: A Dynamic Visualization Task for Understanding Decimal Numbers in Secondary Education

**Authors:** Felix De la Cruz Serrano

**Source:** arxiv • 2026-02-03

**Summary:** The study of decimal numbers in secondary education is often approached from algorithmic perspectives, which limits students' understanding of their structure. This paper presents the task Footprints of the Walking of Numbers, a dynamic visualization proposal aimed at supporting the understanding of...

**Links:**
- Landing page: http://arxiv.org/abs/2602.03679v1
- PDF: https://arxiv.org/pdf/2602.03679v1

*Relevance score: 9.0*

---

## 45. STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models

**Authors:** Jiliang Ni, Jiachen Pu, Zhongyi Yang, Jingfeng Luo, Conggang Hu

**Source:** arxiv • 2026-02-03

**Summary:** The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training...

**Links:**
- Landing page: http://arxiv.org/abs/2602.03022v1
- PDF: https://arxiv.org/pdf/2602.03022v1

*Relevance score: 9.0*

---

## 46. PS$^2$: Parameterized Control for Fine-Grained Student Proficiency Simulation

**Authors:** Ruochen Liu, Zhiyuan Wen, Hao Yan, Jun Yin, Senzhang Wang et al. (6 authors)

**Source:** arxiv • 2026-01-31

**Summary:** Understanding how students with different proficiency levels respond to educational materials is a critical issue within the field of AI for Education. However, acquiring sufficient real student response data for a robust evaluation is often hindered by cost, ethics, and security constraints. Conseq...

**Links:**
- Landing page: http://arxiv.org/abs/2602.00850v1
- PDF: https://arxiv.org/pdf/2602.00850v1

*Relevance score: 9.0*

---

## 47. Video-OPD: Efficient Post-Training of Multimodal Large Language Models for Temporal Video Grounding via On-Policy Distillation

**Authors:** Jiaze Li, Hao Yin, Haoran Xu, Boshen Xu, Wenhui Tan et al. (9 authors)

**Source:** arxiv • 2026-02-03

**Summary:** Reinforcement learning has emerged as a principled post-training paradigm for Temporal Video Grounding (TVG) due to its on-policy optimization, yet existing GRPO-based methods remain fundamentally constrained by sparse reward signals and substantial computational overhead. We propose Video-OPD, an e...

**Links:**
- Landing page: http://arxiv.org/abs/2602.02994v1
- PDF: https://arxiv.org/pdf/2602.02994v1

*Relevance score: 8.0*

---
