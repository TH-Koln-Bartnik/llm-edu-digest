# LLM Education Literature Digest

*Generated: 2025-12-23 06:56:16 UTC*

**48 new items**

## 1. Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation

**Authors:** Ziyang Song, Zelin Zang, Zuyao Chen, Xusheng Liang, Dong Yi et al. (8 authors)

**Source:** arxiv • 2025-12-22

**Summary:** Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent ans...

**Links:**
- Landing page: http://arxiv.org/abs/2512.19512v1
- PDF: https://arxiv.org/pdf/2512.19512v1

*Relevance score: 17.0*

---

## 2. VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop

**Authors:** JiaWei Zhu, ZiHeng Liu

**Source:** arxiv • 2025-12-22

**Summary:** Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausi...

**Links:**
- Landing page: http://arxiv.org/abs/2512.19349v1
- PDF: https://arxiv.org/pdf/2512.19349v1

*Relevance score: 17.0*

---

## 3. Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?

**Authors:** Amar Lakel

**Source:** arxiv • 2025-12-22

**Summary:** This paper proposes an epistemological shift in the analysis of large generative models, replacing the category ''Large Language Models'' (LLM) with that of ''Large Discourse Models'' (LDM), and then with that of Artificial Discursive Agent (ADA). The theoretical framework is based on an ontological...

**Links:**
- Landing page: http://arxiv.org/abs/2512.19117v1
- PDF: https://arxiv.org/pdf/2512.19117v1

*Relevance score: 17.0*

---

## 4. IntelliCode: A Multi-Agent LLM Tutoring System with Centralized Learner Modeling

**Authors:** Jones David, Shreya Ghosh

**Source:** arxiv • 2025-12-21

**Summary:** LLM-based tutors are typically single-turn assistants that lack persistent representations of learner knowledge, making it difficult to provide principled, transparent, and long-term pedagogical support. We introduce IntelliCode, a multi-agent LLM tutoring system built around a centralized, versione...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18669v1
- PDF: https://arxiv.org/pdf/2512.18669v1

*Relevance score: 17.0*

---

## 5. SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback

**Authors:** Jianglin Lu, Yuanwei Wu, Ziyi Zhao, Hongcheng Wang, Felix Jimenez et al. (7 authors)

**Source:** arxiv • 2025-12-21

**Summary:** Complex image restoration aims to recover high-quality images from inputs affected by multiple degradations such as blur, noise, rain, and compression artifacts. Recent restoration agents, powered by vision-language models and large language models, offer promising restoration capabilities but suffe...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18599v1
- PDF: https://arxiv.org/pdf/2512.18599v1

*Relevance score: 17.0*

---

## 6. Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement

**Authors:** Saman Forouzandeh, Wei Peng, Parham Moradi, Xinghuo Yu, Mahdi Jalili

**Source:** arxiv • 2025-12-22

**Summary:** We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects a...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18950v1
- PDF: https://arxiv.org/pdf/2512.18950v1

*Relevance score: 15.0*

---

## 7. When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models

**Authors:** Michael S. Zhang, Rishi A. Ruia, Arnav Kewalram, Saathvik Dharmapuram, Utkarsh Sharma et al. (6 authors)

**Source:** arxiv • 2025-12-22

**Summary:** Catastrophic forgetting poses a fundamental challenge in continual learning, particularly when models are quantized for deployment efficiency. We systematically investigate the interplay between quantization precision (FP16, INT8, INT4) and replay buffer strategies in large language models, revealin...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18934v1
- PDF: https://arxiv.org/pdf/2512.18934v1

*Relevance score: 15.0*

---

## 8. Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction

**Authors:** Ming Li, Han Chen, Yunze Xiao, Jian Chen, Hong Jiao et al. (6 authors)

**Source:** arxiv • 2025-12-21

**Summary:** Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of ...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18880v1
- PDF: https://arxiv.org/pdf/2512.18880v1

*Relevance score: 15.0*

---

## 9. ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning

**Authors:** Weijie Zhou, Xuangtang Xiong, Ye Tian, Lijun Yue, Xinyu Wu et al. (11 authors)

**Source:** arxiv • 2025-12-21

**Summary:** Multimodal Large Language Models (MLLMs) have empowered embodied agents with remarkable capabilities in planning and reasoning. However, when facing ambiguous natural language instructions (e.g., "fetch the tool" in a cluttered room), current agents often fail to balance the high cost of physical ex...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18571v1
- PDF: https://arxiv.org/pdf/2512.18571v1

*Relevance score: 15.0*

---

## 10. GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping

**Authors:** Yikang Yue, Yishu Yin, Xuehai Qian

**Source:** arxiv • 2025-12-19

**Summary:** SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a ...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17570v1
- PDF: https://arxiv.org/pdf/2512.17570v1

*Relevance score: 15.0*

---

## 11. Exploring the features used for summary evaluation by Human and GPT

**Authors:** Zahra Sadeghi, Evangelos Milios, Frank Rudzicz

**Source:** arxiv • 2025-12-22

**Summary:** Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the o...

**Links:**
- Landing page: http://arxiv.org/abs/2512.19620v1
- PDF: https://arxiv.org/pdf/2512.19620v1

*Relevance score: 11.0*

---

## 12. Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios

**Authors:** Jiawen Wang, Jingjing Wang Tianyang Chen, Min Zhang, Guodong Zhou

**Source:** arxiv • 2025-12-22

**Summary:** In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging sc...

**Links:**
- Landing page: http://arxiv.org/abs/2512.19551v1
- PDF: https://arxiv.org/pdf/2512.19551v1

*Relevance score: 11.0*

---

## 13. CodeSimpleQA: Scaling Factuality in Code Large Language Models

**Authors:** Jian Yang, Wei Zhang, Yizhi Li, Shawn Guo, Haowen Wang et al. (11 authors)

**Source:** arxiv • 2025-12-22

**Summary:** Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, ...

**Links:**
- Landing page: http://arxiv.org/abs/2512.19424v1
- PDF: https://arxiv.org/pdf/2512.19424v1

*Relevance score: 11.0*

---

## 14. Brain-Grounded Axes for Reading and Steering LLM States

**Authors:** Sandro Andric

**Source:** arxiv • 2025-12-22

**Summary:** Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dat...

**Links:**
- Landing page: http://arxiv.org/abs/2512.19399v1
- PDF: https://arxiv.org/pdf/2512.19399v1

*Relevance score: 11.0*

---

## 15. AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards

**Authors:** Zihan Lin, Xiaohan Wang, Hexiong Yang, Jiajun Chai, Jie Cao et al. (8 authors)

**Source:** arxiv • 2025-12-22

**Summary:** While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and o...

**Links:**
- Landing page: http://arxiv.org/abs/2512.19126v1
- PDF: https://arxiv.org/pdf/2512.19126v1

*Relevance score: 11.0*

---

## 16. BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation

**Authors:** Mahir Labib Dihan, Sadif Ahmed, Md Nafiu Rahman

**Source:** arxiv • 2025-12-22

**Summary:** Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaF...

**Links:**
- Landing page: http://arxiv.org/abs/2512.19122v1
- PDF: https://arxiv.org/pdf/2512.19122v1

*Relevance score: 11.0*

---

## 17. The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation

**Authors:** Hengrui Jia, Taoran Li, Jonas Guan, Varun Chandrasekaran

**Source:** arxiv • 2025-12-22

**Summary:** Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_...

**Links:**
- Landing page: http://arxiv.org/abs/2512.19025v1
- PDF: https://arxiv.org/pdf/2512.19025v1

*Relevance score: 11.0*

---

## 18. CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis

**Authors:** Kaidi Liang, Ke Li, Xianbiao Hu, Ruwen Qin

**Source:** arxiv • 2025-12-21

**Summary:** Automating crash video analysis is essential to leverage the growing availability of driving video data for traffic safety research and accountability attribution in autonomous driving. Crash video analysis is a challenging multitask problem due to the complex spatiotemporal dynamics of crash events...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18878v1
- PDF: https://arxiv.org/pdf/2512.18878v1

*Relevance score: 11.0*

---

## 19. From Word to World: Can Large Language Models be Implicit Text-based World Models?

**Authors:** Yixia Li, Hongru Wang, Jiahao Qiu, Zhenfei Yin, Dongdong Zhang et al. (11 authors)

**Source:** arxiv • 2025-12-21

**Summary:** Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whethe...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18832v1
- PDF: https://arxiv.org/pdf/2512.18832v1

*Relevance score: 11.0*

---

## 20. HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare

**Authors:** Aditya Siddhant

**Source:** arxiv • 2025-12-21

**Summary:** Behavioral healthcare risk assessment remains a challenging problem due to the highly multimodal nature of patient data and the temporal dynamics of mood and affective disorders. While large language models (LLMs) have demonstrated strong reasoning capabilities, their effectiveness in structured cli...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18829v1
- PDF: https://arxiv.org/pdf/2512.18829v1

*Relevance score: 11.0*

---

## 21. LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction

**Authors:** Jensen Zhang, Ningyuan Liu, Yijia Fan, Zihao Huang, Qinglin Zeng et al. (8 authors)

**Source:** arxiv • 2025-12-21

**Summary:** Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally e...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18623v1
- PDF: https://arxiv.org/pdf/2512.18623v1

*Relevance score: 11.0*

---

## 22. A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback

**Authors:** Thanh Dat Hoang, Thanh Trung Huynh, Matthias Weidlich, Thanh Tam Nguyen, Tong Chen et al. (7 authors)

**Source:** arxiv • 2025-12-21

**Summary:** Text2SQL, the task of generating SQL queries from natural language text, is a critical challenge in data engineering. Recently, Large Language Models (LLMs) have demonstrated superior performance for this task due to their advanced comprehension and generation capabilities. However, privacy and cost...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18622v1
- PDF: https://arxiv.org/pdf/2512.18622v1

*Relevance score: 11.0*

---

## 23. Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V

**Authors:** John Chen, Sihan Cheng, Can Gurkan, Ryan Lay, Moez Salahuddin

**Source:** arxiv • 2025-12-21

**Summary:** Large Language Models' capacity to reason in natural language makes them uniquely promising for 4X and grand strategy games, enabling more natural human-AI gameplay interactions such as collaboration and negotiation. However, these games present unique challenges due to their complexity and long-hor...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18564v1
- PDF: https://arxiv.org/pdf/2512.18564v1

*Relevance score: 11.0*

---

## 24. Large Language Models as Discounted Bayesian Filters

**Authors:** Jensen Zhang, Jing Yang, Keze Wang

**Source:** arxiv • 2025-12-20

**Summary:** Large Language Models (LLMs) demonstrate strong few-shot generalization through in-context learning, yet their reasoning in dynamic and stochastic environments remains opaque. Prior studies mainly focus on static tasks and overlook the online adaptation required when beliefs must be continuously upd...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18489v1
- PDF: https://arxiv.org/pdf/2512.18489v1

*Relevance score: 11.0*

---

## 25. Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling

**Authors:** Christopher Román Jaimes

**Source:** arxiv • 2025-12-20

**Summary:** Natural Language Inference (NLI) models frequently rely on spurious correlations rather than semantic reasoning. Existing mitigation strategies often incur high annotation costs or trigger catastrophic forgetting during fine-tuning. We propose an automated, scalable pipeline to address these limitat...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18462v1
- PDF: https://arxiv.org/pdf/2512.18462v1

*Relevance score: 11.0*

---

## 26. Phoneme-based speech recognition driven by large language models and sampling marginalization

**Authors:** Te Ma, Nanjie Li, Hao Huang, Zhijian Ou

**Source:** arxiv • 2025-12-20

**Summary:** Recently, the Large Language Model-based Phoneme-to-Grapheme (LLM-P2G) method has shown excellent performance in speech recognition tasks and has become a feasible direction to replace the traditional WFST decoding method. This framework takes into account both recognition accuracy and system scalab...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18371v1
- PDF: https://arxiv.org/pdf/2512.18371v1

*Relevance score: 11.0*

---

## 27. Linear Personality Probing and Steering in LLMs: A Big Five Study

**Authors:** Michel Frising, Daniel Balcells

**Source:** arxiv • 2025-12-19

**Summary:** Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. While this means that personality frameworks would be highly valuable tools to characterize and control LLMs' behavior, current approaches remain either costly (post-training) or brit...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17639v1
- PDF: https://arxiv.org/pdf/2512.17639v1

*Relevance score: 11.0*

---

## 28. Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing

**Authors:** Lingxiao Zhao, Haoran Zhou, Yuezhi Che, Dazhao Cheng

**Source:** arxiv • 2025-12-19

**Summary:** Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especiall...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17574v1
- PDF: https://arxiv.org/pdf/2512.17574v1

*Relevance score: 11.0*

---

## 29. Towards Explainable Conversational AI for Early Diagnosis with Large Language Models

**Authors:** Maliha Tabassum, M Shamim Kaiser

**Source:** arxiv • 2025-12-19

**Summary:** Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or tra...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17559v1
- PDF: https://arxiv.org/pdf/2512.17559v1

*Relevance score: 11.0*

---

## 30. GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation

**Authors:** Rang Li, Lei Li, Shuhuai Ren, Hao Tian, Shuhao Gu et al. (13 authors)

**Source:** arxiv • 2025-12-19

**Summary:** Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground lan...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17495v1
- PDF: https://arxiv.org/pdf/2512.17495v1

*Relevance score: 11.0*

---

## 31. UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models

**Authors:** Jiajun Wu, Jian Yang, Wei Zhang, Lin Jing, Yuqing Ma et al. (9 authors)

**Source:** arxiv • 2025-12-19

**Summary:** Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and diffi...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17385v1
- PDF: https://arxiv.org/pdf/2512.17385v1

*Relevance score: 11.0*

---

## 32. AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens

**Authors:** Tung-Ling Li, Yuhao Wu, Hongliang Liu

**Source:** arxiv • 2025-12-19

**Summary:** Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short ...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17375v1
- PDF: https://arxiv.org/pdf/2512.17375v1

*Relevance score: 11.0*

---

## 33. Bridging Natural Language and Formal Specification--Automated Translation of Software Requirements to LTL via Hierarchical Semantics Decomposition Using LLMs

**Authors:** Zhi Ma, Cheng Wen, Zhexin Su, Xiao Liang, Cong Tian et al. (7 authors)

**Source:** arxiv • 2025-12-19

**Summary:** Automating the translation of natural language (NL) software requirements into formal specifications remains a critical challenge in scaling formal verification practices to industrial settings, particularly in safety-critical domains. Existing approaches, both rule-based and learning-based, face si...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17334v1
- PDF: https://arxiv.org/pdf/2512.17334v1

*Relevance score: 11.0*

---

## 34. Neuro-Symbolic Control with Large Language Models for Language-Guided Spatial Tasks

**Authors:** Momina Liaqat Ali, Muhammad Abid

**Source:** arxiv • 2025-12-19

**Summary:** Although large language models (LLMs) have recently become effective tools for language-conditioned control in embodied systems, instability, slow convergence, and hallucinated actions continue to limit their direct application to continuous control. A modular neuro-symbolic control framework that c...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17321v1
- PDF: https://arxiv.org/pdf/2512.17321v1

*Relevance score: 11.0*

---

## 35. Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation

**Authors:** Daksh Jain, Aarya Jain, Ashutosh Desai, Avyakt Verma, Ishan Bhanuka et al. (7 authors)

**Source:** arxiv • 2025-12-19

**Summary:** Strategic decision-making in Pokémon battles presents a unique testbed for evaluating large language models. Pokémon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Model...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17308v1
- PDF: https://arxiv.org/pdf/2512.17308v1

*Relevance score: 11.0*

---

## 36. Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning

**Authors:** Siqi Yang, Zilve Gao, Haibo Qiu, Fanfan Liu, Peng Shi et al. (8 authors)

**Source:** arxiv • 2025-12-19

**Summary:** Multimodal Large Language Models (MLLMs) demonstrate significant potential but remain brittle in complex, long-chain visual reasoning tasks. A critical failure mode is "visual forgetting", where models progressively lose visual grounding as reasoning extends, a phenomenon aptly described as "think l...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17227v1
- PDF: https://arxiv.org/pdf/2512.17227v1

*Relevance score: 11.0*

---

## 37. TCDE: Topic-Centric Dual Expansion of Queries and Documents with Large Language Models for Information Retrieval

**Authors:** Yu Yang, Feng Tian, Ping Chen

**Source:** arxiv • 2025-12-19

**Summary:** Query Expansion (QE) enriches queries and Document Expansion (DE) enriches documents, and these two techniques are often applied separately. However, such separate application may lead to semantic misalignment between the expanded queries (or documents) and their relevant documents (or queries). To ...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17164v1
- PDF: https://arxiv.org/pdf/2512.17164v1

*Relevance score: 11.0*

---

## 38. Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs

**Authors:** Aaron Defazio, Konstantin Mishchenko, Parameswaran Raman, Hao-Jun Michael Shi, Lin Xiao

**Source:** arxiv • 2025-12-18

**Summary:** We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic ap...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17131v1
- PDF: https://arxiv.org/pdf/2512.17131v1

*Relevance score: 10.0*

---

## 39. Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement

**Authors:** Hongsheng Xing, Qiuxin Si

**Source:** arxiv • 2025-12-22

**Summary:** Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extr...

**Links:**
- Landing page: http://arxiv.org/abs/2512.19530v1
- PDF: https://arxiv.org/pdf/2512.19530v1

*Relevance score: 9.0*

---

## 40. From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions

**Authors:** Jiaren Peng, Hongda Sun, Xuan Tian, Cheng Huang, Zeqing Li et al. (6 authors)

**Source:** arxiv • 2025-12-22

**Summary:** The automation of Cyber Threat Intelligence (CTI) relies heavily on Named Entity Recognition (NER) to extract critical entities from unstructured text. Currently, Large Language Models (LLMs) primarily address this task through retrieval-based In-Context Learning (ICL). This paper analyzes this main...

**Links:**
- Landing page: http://arxiv.org/abs/2512.19414v1
- PDF: https://arxiv.org/pdf/2512.19414v1

*Relevance score: 9.0*

---

## 41. OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions

**Authors:** Wendong Bu, Kaihang Pan, Yuze Lin, Jiacheng Li, Kai Shen et al. (9 authors)

**Source:** arxiv • 2025-12-22

**Summary:** Large language models (LLMs) have unified diverse linguistic tasks within a single framework, yet such unification remains unexplored in human motion generation. Existing methods are confined to isolated tasks, limiting flexibility for free-form and omni-objective generation. To address this, we pro...

**Links:**
- Landing page: http://arxiv.org/abs/2512.19159v1
- PDF: https://arxiv.org/pdf/2512.19159v1

*Relevance score: 9.0*

---

## 42. HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction

**Authors:** Haoyu Jiang, Boan Qu, Junjie Zhu, Fanjie Zeng, Xiaojie Lin et al. (6 authors)

**Source:** arxiv • 2025-12-22

**Summary:** The rapid growth of artificial intelligence is exponentially escalating computational demand, inflating data center energy use and carbon emissions, and spurring rapid deployment of green data centers to relieve resource and environmental stress. Achieving sub-minute orchestration of renewables, sto...

**Links:**
- Landing page: http://arxiv.org/abs/2512.19114v1
- PDF: https://arxiv.org/pdf/2512.19114v1

*Relevance score: 9.0*

---

## 43. Toward Training Superintelligent Software Agents through Self-Play SWE-RL

**Authors:** Yuxiang Wei, Zhiqing Sun, Emily McMilin, Jonas Gehring, David Zhang et al. (9 authors)

**Source:** arxiv • 2025-12-21

**Summary:** While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge ...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18552v1
- PDF: https://arxiv.org/pdf/2512.18552v1

*Relevance score: 9.0*

---

## 44. Secret mixtures of experts inside your LLM

**Authors:** Enric Boix-Adsera

**Source:** arxiv • 2025-12-20

**Summary:** Despite being one of the earliest neural network layers, the Multilayer Perceptron (MLP) is arguably one of the least understood parts of the transformer architecture due to its dense computation and lack of easy visualization. This paper seeks to understand the MLP layers in dense LLM models by hyp...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18452v1
- PDF: https://arxiv.org/pdf/2512.18452v1

*Relevance score: 9.0*

---

## 45. An Agentic AI Framework for Training General Practitioner Student Skills

**Authors:** Victor De Marez, Jens Van Nooten, Luna De Bruyne, Walter Daelemans

**Source:** arxiv • 2025-12-20

**Summary:** Advancements in large language models offer strong potential for enhancing virtual simulated patients (VSPs) in medical education by providing scalable alternatives to resource-intensive traditional methods. However, current VSPs often struggle with medical accuracy, consistent roleplaying, scenario...

**Links:**
- Landing page: http://arxiv.org/abs/2512.18440v1
- PDF: https://arxiv.org/pdf/2512.18440v1

*Relevance score: 9.0*

---

## 46. AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning

**Authors:** Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel et al. (14 authors)

**Source:** arxiv • 2025-12-19

**Summary:** Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17853v1
- PDF: https://arxiv.org/pdf/2512.17853v1

*Relevance score: 9.0*

---

## 47. Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection

**Authors:** Menna Elgabry, Ali Hamdi

**Source:** arxiv • 2025-12-19

**Summary:** This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet's Jury Theorem (CJT). Unlike conventional ensembles that often rely on homogeneous architectures, our approach combines architecturally diverse small transformer-...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17630v1
- PDF: https://arxiv.org/pdf/2512.17630v1

*Relevance score: 9.0*

---

## 48. Learning What to Write: Write-Gated KV for Efficient Long-Context Inference

**Authors:** Yen-Chieh Huang, Pi-Cheng Hsiu, Rui Fang, Ming-Syan Chen

**Source:** arxiv • 2025-12-19

**Summary:** Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache managem...

**Links:**
- Landing page: http://arxiv.org/abs/2512.17452v2
- PDF: https://arxiv.org/pdf/2512.17452v2

*Relevance score: 9.0*

---
